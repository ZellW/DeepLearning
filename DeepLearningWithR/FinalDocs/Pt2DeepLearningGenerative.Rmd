---
title: "Deep Learning with R"
output:
  html_document:
    highlight: pygments
    theme: spacelab
    toc: yes
---

<style type="text/css">
p{ /* Normal  */
   font-size: 14px;
   line-height: 18px;}
body{ /* Normal  */
   font-size: 14px;}
td {  /* Table  */
   font-size: 12px;}
h1 { /* Header 1 */
 font-size: 26px;
 color: #4294ce;}
h2 { /* Header 2 */
 font-size: 22px;}
h3 { /* Header 3 */
 font-size: 18px;}
code.r{ /* Code block */
  font-size: 12px;}
pre { /* Code block */
  font-size: 12px}
#table-of-contents h2 {
background-color: #4294ce;}
#table-of-contents{
background: #688FAD;}
#nav-top span.glyphicon{
color: #4294ce;}
#postamble{
background: #4294ce;
border-top: ;}
</style>

```{r setup, include=FALSE}
knitr::opts_knit$set(
     root.dir = 'C:/Users/czwea/Documents/GitHub/DeepLearning/Deep_Learning_with_R')
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
#remotes::install_github("rstudio/gt")

if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "keras", "gt", "here", "stringr", prompt = TRUE)
setwd("~/GitHub/DeepLearning/Deep_Learning_with_R")
```

# Notes from *Deep Learning with R*

## Web Links

[Keras RStudio](https://keras.rstudio.com/)
[Book Code](https://github.com/jjallaire/deep-learning-with-r-notebooks)
[Live Book](https://www.manning.com/books/deep-learning-with-r)

> Install Anaconda with Python 6, update conda, then `install_keras`

- Anaconda2-4.3.1-Windows-x86_64
      - installed defaults including path and default Python
- conda update conda in Anaconda prompt (used admin rights by right clicking)
- `install_keras`

# Part 1: Fundamentals of Deep Learning

Anatomy of a neural network:

Training a neural network revolves around the following objects:

1. Layers, which are combined into a network (or model) 
2. The input data and corresponding targets
3. The loss function, which defines the feedback signal used for learning 
4. The optimizer which determines how learning proceeds 

1. The fundamental data structure in neural networks is the layer. A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer weights, one or several tensors learned with stochastic gradient descent, which together contain the network knowledge. 

2. Different layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape (samples, features), is often processed by densely connected layers, also called fully connected or dense layers (the `layer_dense` function in Keras). Sequence data, stored in 3D tensors of shape (samples, timesteps, features), is typically processed by recurrent layers such as `layer_lstm.` Image data, stored in 4D tensors, is usually processed by 2D convolution layers (`layer_conv_2d`). 

The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. 

---

The topology of a network defines a hypothesis space. Machine learning is defined as *searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal.* By choosing a network topology, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. What you will then be searching for is a good set of values for the weight tensors involved in these tensor operations. 

---

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/DLwR_p_11.JPG")
```

3. **Loss Function** (Objective Function):  The quantity that will be minimized during training.  It represents a measure of success for the task.

- A neural network that has multiple outputs may have multiple loss functions (one per output).  Gradient descent process must be based on a single scalar loss value.  For multiloss networks, all losses are combined and averaged into a single scalar quantity.
     - Choosing the right loss function is extremely important
          - Two-Class Classifications use *binary crossentropy*
          - many-class classification uses *categorical crossentropy*
          - Regression uses mean squared error
          - Sequence learning tasks use *connectionist temporal classification* (CTC)

> Connectionist temporal classification is a type of neural network output and associated scoring function, for training recurrent neural networks such as LSTM networks to tackle sequence problems where the timing is variable. It can be used for tasks like on-line handwriting recognition or recognizing phonemes in speech audio. CTC refers to the outputs and scoring, and is independent of the underlying neural network structure. 

4. **Optimizer**: Determines how the network will be updated based on the loss function.  It implements a specific variant of stochastic gradient descent (SGD).  [stochastic simply means random]

## Developing with Keras

The typical Keras workflow looks like:

1. Define your training data: input tensors and target tensors.
2. Define a network of layers (or model) that maps your inputs to your targets. 
3. Configure the learning process by choosing a loss function, an optimizer and some metrics to monitor.
4. Iterate on your training data by calling the fit() method of your model. 

**Vectorization**:  All inputs and targets in a neural network must be tensors of floating-point data (or, in specific cases, tensors of integers). Whatever data you need to process - sound, images, text - you must first turn into tensors, a step called _data vectorization_. For instance, in the two following text-classification examples, start from text represented as lists of integers (standing for sequences of words), and  used one-hot encoding to turn them into a tensor of floating-point data. In the examples of classifying digits and predicting house prices, the data already comes in vectorized form. 

**Value normalization**: In the digit-classification example below, start from image data encoded as integers in the 0 - 255 range, encoding grayscale values. Before you feed this data into the network, divide by 255 so you would end up with floating-point values in the 0-1 range. Similarly, when predicting house prices, you start from features that took a variety of ranges some features had small floating-point values, and others had fairly large integer values. Before feeding this data into the network, you had to normalize each feature independently so that it had a standard deviation of 1 and a mean of 0. 

In general, it is not safe to feed into a neural network data that takes relatively large values (for example, multidigit integers, which are much larger than the initial values taken by the weights of a network) or data that is heterogeneous (for example, data where one feature is in the range 0-1 and another is in the range 100 - 200). Doing so can trigger large gradient updates that will prevent the network from converging. 

To make learning easier for your network, your data should have the following characteristics:

- Take small values. Typically, most values should be in the 0 - 1 range. 
- Be homogeneousThat is, all features should take values in roughly the same range.

Additionally, the following stricter normalization practice is common and can help, although it is not always necessary: 

-Normalize each feature independently to have a mean of 0.
-Normalize each feature independently to have a standard deviation of 1.

This is easy to do with R using the scale() function: 

```{r eval=FALSE}
x <- scale(x)                         1
```
Assuming x is a 2D matrix of shape (samples, features)

> Typically, you normalize features in both training and test data. In this case, you want to compute the mean and standard deviation on the training data only and then apply them to both the training and test data.

```{}
1 mean <- apply(train_data, 2, mean) 
2 std <- apply(train_data, 2, sd)
3 train_data <- scale(train_data, center = mean, scale = std) 
4 test_data <- scale(test_data, center = mean, scale = std)
```

1. Calculates the mean and standard deviation on the training data
2. Scales the training and test data using the mean and standard deviation from the training data

The `caret` and `recipes` packages both include many more high-level functions for data preprocessing and normalization. 

**Feature Engineering**:  Modern deep learning removes the need for most feature engineering because neural networks are capable of automatically extracting useful features from raw data. Does this mean you do not have to worry about feature engineering as long as you are using deep neural networks? No, for two reasons: 

- Good features still allow you to solve problems more elegantly while using fewer resources. For instance, it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network. 
- Good features let you solve a problem with far less data. The ability of deep-learning models to learn features on their own relies on having lots of training data available; if you have only a few samples, then the information value in their features becomes critical. 

**Overfitting**:  In all three examples below, predicting movie reviews, topic classification, and house-price regressions, the performance of the model on the held-out validation data always peaked after a few epochs and then began to degrade: the model quickly started to overfit to the training data. Overfitting happens in every machine-learning problem. Learning how to deal with overfitting is essential to mastering machine learning. 

The fundamental issue in machine learning is the tension between optimization and generalization. Optimization refers to the process of adjusting a model to get the best performance possible on the training data (the learning in machine learning), whereas generalization refers to how well the trained model performs on data it has never seen before. The goal of the game is to get good generalization, of course, but you do not control generalization; you can only adjust the model based on its training data. 

At the beginning of training, optimization and generalization are correlated: the lower the loss on training data, the lower the loss on test data. While this is happening, your model is said to be underfit: there is still progress to be made; the network has not yet modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops improving, and validation metrics stall and then begin to degrade: the model is starting to overfit. It is beginning to learn patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data. 

To prevent a model from learning misleading or irrelevant patterns found in the training data, the best solution is to get more training data. A model trained on more data will naturally generalize better. When that is not possible, the next-best solution is to modulate the quantity of information that your model is allowed to store or to add constraints on what information it is allowed to store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well. 

The process of fighting overfitting this way is called `regularization.` Review some of the most common regularization techniques and apply them in practice to improve the movie-classification model. 

1. Reducing the network size 

The simplest way to prevent overfitting is to reduce the size of the model: the number of learnable parameters in the model (which is determined by the number of layers and the number of units per layer). In deep learning, _the number of learnable parameters in a model is often referred to as the models capacity_. Intuitively, a model with more parameters has more memorization capacity and therefore can easily learn a perfect dictionary - like mapping between training samples and their target a mapping without any generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digit in the MNIST training set: we need only 10 binary parameters for each of the 50,000 digits. But such a model would be useless for classifying new digit samples. Always keep this in mind: deep-learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting. 

On the other hand, if the network has limited memorization resources, it will not be able to learn this mapping as easily; thus, in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets precisely the type of representations we are interested in. At the same time, keep in mind that you should use models that have enough parameters that they do not underfit: your model should not be starved for memorization resources. There is a compromise to be found between too much capacity and not enough capacity. 

Unfortunately, there is no magical formula to determine the right number of layers or the right size for each layer. You must evaluate an array of different architectures (on your validation set, not on your test set) in order to find the correct model size for your data. _the general workflow to find an appropriate model size is to start with relatively few layers and parameters, and increase the size of the layers or add new layers until you see diminishing returns with regard to validation loss_. 

Try this on the movie-review classification network. The original network is shown next.

```{r eval=FALSE}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Replace it with this smaller network.

```{r eval=FALSE}
model <- keras_model_sequential() %>%
  layer_dense(units = 4, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 4, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

The figure below shows a comparison of the validation losses of the original network and the smaller network (remember, a lower validation loss signals a better model). As you can see, the smaller network starts overfitting later than the reference network, and its performance degrades more slowly once it begins to overfit. 

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/network_size1.JPG")
```

Add to this benchmark a network that has much more capacity far more than the problem warrants.

```{r eval=FALSE}
model <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Figure below shows how the bigger network fares compared to the reference network. The bigger network starts overfitting almost immediately, after just one epoch, and it overfits much more severely. Its validation loss is also noisier. 

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/network_size2.JPG")
```

Meanwhile, the figure below shows the training losses for the two networks. As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the more quickly it can model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss). 

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/network_size3.JPG")
```

2. Adding weight regularization 

You may be familiar with the principle of Occams razor: _given two explanations for something, the explanation most likely to be correct is the simplest ones the one that makes fewer assumptions_. This idea also applies to the models learned by neural networks: given some training data and a network architecture, multiple sets of weight values (multiple models) could explain the data. Simpler models are less likely to overfit than complex ones. 

A simple model in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters, as you saw in the previous section). Thus, a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more regular. This is called weight regularization, and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors: 

- L1 regularization:The cost added is proportional to the __absolute value__ of the weight coefficients (the L1 norm of the weights). 
- L2 regularization:The cost added is proportional to the __square of the value__ of the weight coefficients (the L2 norm of the weights). L2 regularization is also called weight decay in the context of neural networks. Do not let the different name confuse you: weight decay is mathematically the same as L2 regularization. 

In Keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Add L2 weight regularization to the movie-review classification network. 

```{r eval=FALSE}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001), activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001), activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

`regularizer_l2(0.001)` means every coefficient in the weight matrix of the layer will add `0.001 * weight_coefficient_value` to the total loss of the network. Note that because this *penalty is only added at training time*, the loss for this network will be much higher at training time than at test time. 

The next figure shows the impact of the L2 regularization penalty. As you can see, the model with L2 regularization has become much more resistant to overfitting than the reference model, even though both models have the same number of parameters. 

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/network_size4.JPG")
```
 
As an alternative to L2 regularization, you can use one of the following Keras weight regularizers. 

```{r eval=FALSE}
regularizer_l1(0.001)
regularizer_l1_l2(l1 = 0.001, l2 = 0.001)
```

3. Adding dropout 

Dropout is one of the most effective and most commonly used regularization techniques for neural networks. Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training. Say a given layer would normally return a vector `[0.2, 0.5, 1.3, 0.8, 1.1]` for a given input sample during training. After applying dropout, this vector will have a few zero entries distributed at random: for example, `[0, 0.5, 1.3, 0, 1.1]`. The _dropout rate is the fraction of the features that are zeroed out_; it is usually set between 0.2 and 0.5. At test time, no units are dropped out; instead, the layers output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time. 

Consider a matrix containing the output of a layer, `layer_output`, of shape (`batch_size, features`). At training time, we zero out at random a fraction of the values in the matrix: 

```{r eval=FALSE}
layer_output <- layer_output * sample(0:1, length(layer_output), replace = TRUE)
```

At test time, we scale down the output by the dropout rate. Here, we scale by 0.5 (because we previously dropped half the units): 

```{r eval=FALSE}
layer_output <- layer_output * 0.5
```

Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is often the way it is implemented in practice (see below): 

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/network_size5.JPG")
```

```{r eval=FALSE}
layer_output <- layer_output * sample(0:1, length(layer_output), replace = TRUE) # at training time
layer_output <- layer_output / 0.5  #scaling up rather scaling down in this case
```

This technique may seem strange and arbitrary. Why would this help reduce overfitting? The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that are not significant, which the network will start memorizing if no noise is present. 

In Keras, you  introduce dropout in a network via `layer_dropout`, which is applied to the output of the layer immediately before it: 

```{r eval=FALSE}
layer_dropout(rate = 0.5)
```

Add two dropout layers in the IMDB network to see how well they do at reducing overfitting.

```{r eval=FALSE}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

The figure shows a plot of the results. Again, this is a clear improvement over the reference network. 

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/network_size6.JPG")
```
 
To recap, these are the most common ways to prevent overfitting in neural networks:

- Get more training data
- Reduce the capacity of the network
- Add weight regularization
- Add dropout

## Binary Classification Example

Two-class classification, or binary classification, may be the most widely applied kind of machine learning problem. In this example, we will learn to classify movie reviews into _positive_ reviews and _negative_ reviews, just based on the text content of the reviews.

### The IMDB dataset

The IMDB dataset consists of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.

Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.

The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded):

```{r, results='hide'}
imdb <- dataset_imdb(num_words = 10000)

#c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb

train_data <- imdb$train$x
train_labels <- imdb$train$y
test_data <- imdb$test$x
test_labels <- imdb$test$y
```

> multi-assignment operator (%<-%) from the `zeallot` package to unpack the list into a set of distinct variables

The argument `num_words = 10000` means that we keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. 

The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for "negative" and 1 stands for "positive":

```{r}
str(train_data[[1]])
```

```{r}
head(train_labels, 10)
```

### Preparing the data

You cannot feed lists of integers into a neural network. You have to turn your lists into tensors. This can be done using One-hot-encoding your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence `[3, 5]` into a 10,000-dimensional vector that would be all zeros except for indices 3 and 5, which would be ones. Then you could use as the first layer in your network a dense layer, capable of handling floating-point vector data.

> Typical one-hot encoding will not work because there are a difference number of values in each nested branch

> Similarly, `as.data.frame` and `as.matrix` fail for the same reason.

Therefore a prepopulated matrix must be created and then filled by the loop.

```{r vectorize_sequences}
vectorize_sequences <- function(sequences, dimension = 10000) {
  # Create an all-zero matrix of shape (len(sequences), dimension)
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    # Sets specific indices of results[i] to 1s
    results[i, sequences[[i]]] <- 1
  results}

# Our vectorized training data
x_train <- vectorize_sequences(train_data)
# Our vectorized test data
x_test <- vectorize_sequences(test_data)
```

Here is what our samples look like now:

```{r}
str(x_train[1,])
```

We should also vectorize our labels:

```{r}
# Our vectorized labels
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

Now our data is ready to be fed into a neural network.

### Building the network

The input data is vectors and the labels are scalars (1s and 0s): this is the easiest setup you will  encounter. _A type of network that performs well on such a problem is a simple stack of fully connected ("dense") layers with `relu` activations_: `layer_dense(units = 16, activation = "relu")`.

The argument being passed to each dense layer (16) is the number of hidden units of the layer. A _hidden unit_ is a dimension in the representation space of the layer. You may remember that each such dense layer with a `relu` activation implements the following chain of tensor operations:

`output = relu(dot(W, input) + b)`

Having 16 hidden units means that the weight matrix `W` will have shape `(input_dimension, 16)`, i.e. the dot product with `W` will project the input data onto a 16-dimensional representation space (and then we would add the bias vector `b` and apply the `relu` operation). You can intuitively understand the dimensionality of your representation space as _how much freedom you are allowing the network to have when learning internal representations_. Having more hidden units (a higher-dimensional representation space) allows your network to learn more complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).

> The dot product or scalar product is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number.

There are two key architecture decisions to be made about such stack of dense layers:

* How many layers to use
* How many "hidden units" to chose for each layer

For the time being trust the following architecture choice: 

- two intermediate layers with 16 hidden units each
- a third layer which will output the scalar prediction regarding the sentiment of the current review. 

The intermediate layers will use `relu` as their "activation function" and the final layer will use a `sigmoid` activation  to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target "1", i.e. how likely the review is to be positive). A `relu` (rectified linear unit) is a function meant to zero-out negative values, while a sigmoid "squashes" arbitrary values into the `[0, 1]` interval, thus outputting something that can be interpreted as a probability.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/relu.png")
```

Here is what our network looks like:

```{r, out.width = "200px", echo=FALSE}
knitr::include_graphics("./images/Keras_Sequential.png")
```

And here is the Keras implementation:

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

Lastly, pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the `binary_crossentropy` loss. It is not the only viable choice: you could use, for instance, `mean_squared_error`. But _crossentropy is usually the best choice when you are dealing with models that output probabilities_. `Crossentropy` is a quantity from the field of Information Theory that measures the _distance_ between probability distributions, or in our case, between the ground-truth distribution and our predictions.

---

Without an activation function like `relu` (also called a non-linearity), the dense layer would consist of 2 linear operations - a dot product and an addition:
$$output = dot(W, input) + b$$
So the layer could only learn linear transformations of the input data: the *hypothesis space* of the layer would be the set of all possible linear transformations of the input data into a 16-dimension space.  This is too restrictive and would not benefit from multiple payers or representations because a deep stack of linear layers would still implement a linear operation.  A non-linearity or activation function is needed to access a much richer *hypothesis space* that would benefit from deep representations.

---

Configure the model with the `rmsprop` optimizer and the `binary_crossentropy` loss function. Note that we will also monitor accuracy during training.

```{r}
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
```

You are passing your optimizer, loss function, and metrics as strings, which is possible because `rmsprop`, `binary_crossentropy`, and `accuracy` are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer or pass a custom loss function or metric function. The former can be done by passing an optimizer instance as the `optimizer` argument: 

```{r}
model %>% compile(optimizer = optimizer_rmsprop(lr=0.001), loss = "binary_crossentropy", metrics = c("accuracy")) 
```

The latter can be done by passing function objects as the `loss` or `metrics` arguments:

```{r}
model %>% compile(optimizer = optimizer_rmsprop(lr = 0.001), loss = loss_binary_crossentropy,
                  metrics = metric_binary_accuracy) 
```

### Validating Approach

In order to monitor during training the accuracy of the model on data that it has never seen before, create a "validation set" by setting apart 10,000 samples from the original training data:

```{r}
val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```

Now train the model for 20 epochs (20 iterations over all samples in the `x_train` and `y_train` tensors), in mini-batches of 512 samples. At this same time  monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the validation data as the `validation_data` argument:

```{r, echo=TRUE, results='hide'}
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))

history <- model %>% fit(partial_x_train, partial_y_train, epochs = 20, batch_size = 512,
                         validation_data = list(x_val, y_val))
```

Note that the call to `fit()` returns a `history` object:

```{r}
str(history)
```

The `history` object includes various parameters used to fit the model (`history$params`) as well as data for each of the metrics being monitored (`history$metrics`).

The `history` object has a `plot()` method that enables us to visualize the training and validation metrics by epoch:

```{r}
plot(history)
```

The accuracy is plotted on the top panel and the loss on the bottom panel. Note results may vary slightly due to a different random initialization of your network.

The dots are the training loss and accuracy, while the solid lines are the validation loss and accuracy. Note that your own results may vary slightly due to a different random initialization of your network.

The training loss decreases with every epoch and the training accuracy increases with every epoch. That's what you would expect when running a gradient-descent optimization - the quantity you're trying to minimize should be less with every iteration. But that isn't the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn't necessarily a model that will do better on data it has never seen before. In precise terms, what you're seeing is _overfitting_: after the second epoch, you're over-optimizing on the training data, and you end up learning representations that are specific to the training data and don't generalize to data outside of the training set.

---
`plot` used above uses `ggplot2`.  To create a custom plot, use `as.data.frame` to produce a data frame with factors for each metric as well as validation.

```{r}
history_df <- as.data.frame(history)
str(history_df)
```

In this case, to prevent overfitting, you could stop training after three epochs. 

Train a new network from scratch for four epochs and then evaluate it on the test data.

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(optimizer = "rmsprop",loss = "binary_crossentropy", metrics = c("accuracy"))

model %>% fit(x_train, y_train, epochs = 4, batch_size = 512)
results <- model %>% evaluate(x_test, y_test)
```

```{r}
results
```

Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%.

### Predictions on new data

After having trained a network, use it in a practical setting. You can generate the likelihood of reviews being positive by using the `predict` method:

```{r}
model %>% predict(x_test[1:10,])
```

The network is very confident for some samples but less confident for others. 

### Further experiments

* Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.
* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...
* Try to use the `mse` loss function instead of `binary_crossentropy`.
* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.

These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be improved!

### Conclusions

Here is what you should take away from this example:

* You usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it -- as tensors -- into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options, too.
* Stacks of dense layers with `relu` activations can solve a wide range of problems (including sentiment classification), and you'll likely use them frequently.
* In a binary classification problem (two output classes), your network should end with a dense layer with one unit and a `sigmoid` activation. That is, the output of your network should be a scalar between 0 and 1, encoding a probability.
* With such a scalar sigmoid output on a binary classification problem, the loss function you should use is `binary_crossentropy`.
* The `rmsprop` optimizer is generally a good enough choice, whatever your problem. That's one less thing for you to worry about.
* As they get better on their training data, neural networks eventually start _overfitting_ and end up obtaining increasingly worse results on data they've never seen before. Be sure to always monitor performance on data that is outside of the training set.

## Multiclass Classification

```{r echo=FALSE}
rm(list = ls())
```

Build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since there are many classes, this problem is an instance of *multi-class classification* and since each data point should be classified into only one category, the problem is more specifically an instance of *single-label, multi-class classification*. If each data point could have belonged to multiple categories then it would be a *multi-label, multi-class classification* problem.

### The Reuters dataset

The _Reuters dataset_ is a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.

Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. 

```{r}
reuters <- dataset_reuters(num_words = 10000)
summary(reuters)
```
```{r}
summary(reuters$train)
```
```{r}
summary(reuters$train$x[1:10])
```

```{r}
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% reuters
```

The argument `num_words = 10000` restricts the data to the 10,000 most frequently occurring words found in the data.

We have 8,982 training examples and 2,246 test examples:

```{r}
length(train_data)
```

```{r}
length(test_data)
```

Each example is a list of integers (word indices):

```{r}
train_data[[1]]
```

```{r}
train_labels[[1]]
```

### Preparing Data

We can vectorize the data with the exact same code as before:

```{r}
vectorize_sequences <- function(sequences, dimension = 10000) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results}

x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
```

To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a "one-hot" encoding. One-hot encoding is a widely used format for categorical data, also called "categorical encoding". In this case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:

```{r}
to_one_hot <- function(labels, dimension = 46) {
  results <- matrix(0, nrow = length(labels), ncol = dimension)
  for (i in 1:length(labels))
    results[i, labels[[i]] + 1] <- 1
  results}

one_hot_train_labels <- to_one_hot(train_labels)
one_hot_test_labels <- to_one_hot(test_labels)
```

> Note that there is a _built-in way to do this in Keras_ using `to_categorical`:

```{r}
one_hot_train_labels <- to_categorical(train_labels)
one_hot_test_labels <- to_categorical(test_labels)
```

Below is what was used in the binary classification. 
```{r eval=FALSE}
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

---

__A different way to handle the labels and the loss__

Another way to encode the labels would be to preserve their integer values. The only thing this approach would change is the choice of the loss function. The loss function currently used, `categorical_crossentropy`, expects the labels to follow a categorical encoding. With integer labels, you should use s`parse_categorical_crossentropy`: 

```{r eval=FALSE}
model %>% compile(optimizer = "rmsprop", loss = "sparse_categorical_crossentropy", metrics = c("accuracy"))
```

---

### Building Network

This classification problem looks very similar to the previous one: in both cases, classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the dimensionality of the output space is much larger. 

In a stack of dense layers like what we were using, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each layer can potentially become an "*information bottleneck*. In the previous example, a 16-dimensional intermediate layers,were used but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information.

For this reason use larger layers - 64 units:

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")
```

There are two other things to note about this architecture:

* End the network with a dense layer of size 46. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.

* The last layer uses a `softmax` activation. It means the network will output a _probability distribution_ over the 46 different output classes: for every input sample, the network will produce a 46-dimensional output vector, where `output[[i]]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.

The best loss function to use is `categorical_crossentropy`. It measures the distance between two probability distributions: By minimizing the distance between these two distributions, train the network to output something as close as possible to the true labels.

```{r}
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = c("accuracy"))
```

> Recall `rmsprop` optimizer is generally a good enough choice, whatever your problem!

### Validating Approach

Set aside 1,000 samples in our training data to use as a validation set:

```{r}
val_indices <- 1:1000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- one_hot_train_labels[val_indices,]
partial_y_train = one_hot_train_labels[-val_indices,]
```

Train the network for 20 epochs:

```{r echo=TRUE, results='hide'}
history <- model %>% fit(partial_x_train, partial_y_train, epochs = 20, batch_size = 512, 
                         validation_data = list(x_val, y_val))
```

Display the loss and accuracy curves:

```{r}
plot(history)
```

The network begins to overfit after a 9 epochs. Train a new network from scratch for nine epochs and then evaluate it on the test set.

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")
  
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = c("accuracy"))

history <- model %>% fit(partial_x_train, partial_y_train, epochs = 9, batch_size = 512, validation_data = list(x_val, y_val))

results <- model %>% evaluate(x_test, one_hot_test_labels)
```

```{r}
results
```

The approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier would be 50%, but in this case it is closer to 19%, so the results seem pretty good, at least when compared to a random baseline:

```{r}
test_labels_copy <- test_labels
test_labels_copy <- sample(test_labels_copy)
length(which(test_labels == test_labels_copy)) / length(test_labels)
```

> The code directly above is immensely useful - never thought of it before and it is so easy!

### Predictions

Verify that the `predict` method of the model instance returns a probability distribution over all 46 topics. Generate topic predictions for all of the test data:

```{r}
predictions <- model %>% predict(x_test)
head(predictions, 1)
```

Each entry in `predictions` is a vector of length 46:

```{r}
dim(predictions)
```

The coefficients in this vector sum to 1:

```{r}
sum(predictions[1,])
```

The largest entry is the predicted class, i.e. the class with the highest probability - `r max(predictions[1,])`:

```{r}
which.max(predictions[1,])
```

### Labels and the Loss

Another way to encode the labels would be to preserve their integer values. The only thing this approach would change is the choice of the loss function. The previous loss, `categorical_crossentropy`, expects the labels to follow a categorical encoding. With integer labels,  should use `sparse_categorical_crossentropy`:

```{r}
model %>% compile(optimizer = "rmsprop", loss = "sparse_categorical_crossentropy", metrics = c("accuracy"))
```

This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface.

### Large Intermediate Layers

Since the final outputs were 46-dimensional, avoid intermediate layers with much less than 46 hidden units. See what happens when a different dimension is used to for an information bottleneck by having intermediate layers significantly less than 46-dimensional, e.g. 4-dimensional.

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 4, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")
  
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = c("accuracy"))

model %>% fit(partial_x_train, partial_y_train, epochs = 20, batch_size = 128, validation_data = list(x_val, y_val))
```

The network peaks at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that the model is trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all of it.

### Further Experiments

* Try using larger or smaller layers: 32 units, 128 units...
* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers.

### Wrapping-up

Here's what you should take away from this example:

* If you are trying to classify data points between N classes, your network should end with a dense layer of size N.
* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a probability distribution over the N output classes.
* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the probability distributions output by the network, and the true distribution of the targets.
* There are two ways to handle labels in multi-class classification:
    * Encoding the labels via "categorical encoding" (also known as "one-hot encoding") and using `categorical_crossentropy` as your loss function.
    * Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.
* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having intermediate layers that are too small.

## Regression Example

```{r echo=FALSE}
rm(list = ls())
```

A common type of machine learning problem is "regression", which consists of predicting a continuous value instead of a discrete label. For instance, predicting the temperature tomorrow, given meteorological data, or predicting the time that a software project will take to complete, given its specifications.

### Boston Housing Price Data

Predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the suburb at the time, such as the crime rate, the local property tax rate, etc.

The dataset we will be using has another interesting difference from our two previous examples: it has very few data points, only 506 in total, split between 404 training samples and 102 test samples, and each "feature" in the input data (e.g. the crime rate is a feature) has a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12, others between 0 and 100...

Let's take a look at the data:

```{r}
library(keras)
dataset <- dataset_boston_housing()
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset
```

```{r}
str(train_data)
```

```{r}
str(test_data)
```

As you can see, we have 404 training samples and 102 test samples. The data comprises 13 features. The 13 features in the input data are as 
follow:

1. Per capita crime rate.
2. Proportion of residential land zoned for lots over 25,000 square feet.
3. Proportion of non-retail business acres per town.
4. Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
5. Nitric oxides concentration (parts per 10 million).
6. Average number of rooms per dwelling.
7. Proportion of owner-occupied units built prior to 1940.
8. Weighted distances to five Boston employment centers.
9. Index of accessibility to radial highways.
10. Full-value property-tax rate per $10,000.
11. Pupil-teacher ratio by town.
12. 1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.
13. % lower status of the population.

The targets are the median values of owner-occupied homes, in thousands of dollars:

```{r}
str(train_targets)
```

The prices are typically between \$10,000 and \$50,000. If that sounds cheap, remember this was the mid-1970s and these prices are not inflation-adjusted.

### Preparing the data

It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. _A widespread best practice to deal with such data is to do feature-wise normalization_: for each feature in the input data, you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done using  `scale()`.

```{r}
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)
```

> I like the simplicity of the basic code above

> Note that the quantities used for normalizing the test data have been computed using the training data. **Never use any quantity computed on the test data, even for something as simple as data normalization**.

### Building the Network

Because so few samples are available, we will be using a very small network with two hidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting.

```{r}
# Need to instantiate the same model multiple times, use a function to construct it.
build_model <- function() {
  model <- keras_model_sequential() %>% 
    layer_dense(units = 64, activation = "relu", input_shape = dim(train_data)[[2]]) %>% 
    layer_dense(units = 64, activation = "relu") %>% 
    layer_dense(units = 1) 
    
  model %>% compile(optimizer = "rmsprop", loss = "mse", metrics = c("mae"))}
```

> IMPORTANT BELOW

The network ends with a single unit and no activation (i.e. it will be linear layer). This is a typical setup for scalar regression. _Applying an activation function would constrain the range that the output can take_.  If a `sigmoid` activation function were applied, to the last layer, the network could only learn to predict values between 0 and 1. Here, because the last layer is purely linear, the network is free to learn to predict values in any range.

Compiling the network with the `mse` loss function -- Mean Squared Error, the square of the difference between the predictions and the targets, a widely used loss function for regression problems.

Are also monitoring a new metric during training: `mae`. This stands for Mean Absolute Error. It is simply the absolute value of the difference between the predictions and the targets. For instance, a MAE of 0.5 on this problem would mean that our predictions are off by \$500 on average.

### Validating our approach using K-fold validation

To evaluate the network while adjusting its parameters (such as the number of epochs used for training), simply splitting the data into a training set and a validation set is not appropriate. Because there are few data points, the validation set would end up being very small (e.g. about 100 examples). A consequence is that the validation scores may change a lot depending on _which_ data points are selected to use for validation and training, i.e. the validation scores may have a high _variance_ with regard to the validation split. This would prevent reliably evaluating our model.

The best practice in such situations is to use K-fold cross-validation. It consists of splitting the available data into K partitions (typically K=4 or 5), then instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining partition. The validation score for the model used would then be the average of the K validation scores obtained.

In terms of code, this is straightforward:

```{r, echo=TRUE, results='hide'}
k <- 4
indices <- sample(1:nrow(train_data))
folds <- cut(1:length(indices), breaks = k, labels = FALSE) 

num_epochs <- 100
all_scores <- c()
for (i in 1:k) {
  cat("processing fold #", i, "\n")
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE) 
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Prepare the training data: data from all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Build the Keras model (already compiled)
  model <- build_model()
  
  # Train the model (in silent mode, verbose=0)
  model %>% fit(partial_train_data, partial_train_targets,
                epochs = num_epochs, batch_size = 1, verbose = 0)
                
  # Evaluate the model on the validation data
  results <- model %>% evaluate(val_data, val_targets, verbose = 0)
  all_scores <- c(all_scores, results$mean_absolute_error)
}  
```

---
Recall `which` parameters`

**Usage**

which(x, arr.ind = FALSE, useNames = TRUE)

**Arguments**

- x:  a logical vector or array. NAs are allowed and omitted (treated as if FALSE).
- arr.ind	:  logical; should **arr**ay **ind**ices be returned when x is an array?
- ind: integer-valued index vector, as resulting from which(x).
- .dim:  dim(.) integer vector
- .dimnames:  optional list of character dimnames(.). If useNames is true, to be used for constructing dimnames for arrayInd() (and hence, which(*, arr.ind=TRUE)). If names(.dimnames) is not empty, these are used as column names. .dimnames[[1]] is used as row names.
- useNames:  logical indicating if the value of arrayInd() should have (non-null) dimnames at all.

**Value**

If `arr.ind == FALSE` (the default), an integer vector with length equal to sum(x), i.e., to the number of TRUEs in x; Basically, the result is `(1:length(x))[x]`.

If `arr.ind == TRUE` and x is an array (has a dim attribute), the result is `arrayInd(which(x)`, `dim(x)`, `dimnames(x))`, namely a matrix whose rows each are the indices of one element of x.

```{r}
all_scores
```

```{r}
mean(all_scores)
```

As you can notice, the different runs do indeed show rather different validation scores, from `r round(min(all_scores), 1)` to `r round(max(all_scores), 1)`. Their average (`r mean(all_scores)`) is a much more reliable metric than any single of these scores -- that's the entire point of K-fold cross-validation. In this case, we are off by \$`r round(mean(all_scores*1000),0)` on average, which is still significant considering that the prices range from \$10,000 to \$50,000. 

Try training the network for a bit longer: 500 epochs. To keep a record of how well the model did at each epoch, modify the training loop to save the per-epoch validation score log:

```{r}
# Some memory clean-up - a Keras function to avoid clutter from old models / layers.
k_clear_session()
```

```{r, echo=TRUE, results='hide'}
# These lines are repeated so the code block below is self-sufficient
k <- 4
indices <- sample(1:nrow(train_data))
folds <- cut(1:length(indices), breaks = k, labels = FALSE)

num_epochs <- 500
all_mae_histories <- NULL
for (i in 1:k) {
  cat("processing fold #", i, "\n")
  
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE)
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Prepare the training data: data from all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Build the Keras model (already compiled)
  model <- build_model()
  
  # Train the model (in silent mode, verbose=0)
  history <- model %>% fit(
    partial_train_data, partial_train_targets,
    validation_data = list(val_data, val_targets),
    epochs = num_epochs, batch_size = 1, verbose = 0
  )
  mae_history <- history$metrics$val_mean_absolute_error
  all_mae_histories <- rbind(all_mae_histories, mae_history)
}
```

We can then compute the average of the per-epoch MAE scores for all folds:

```{r}
average_mae_history <- data.frame(epoch = seq(1:ncol(all_mae_histories)),
  validation_mae = apply(all_mae_histories, 2, mean))
```

Let's plot this:

```{r}
library(ggplot2)
ggplot(average_mae_history, aes(x = epoch, y = validation_mae)) + geom_line()
```

It is hard to see the plot due to scaling issues and relatively high variance. Use `geom_smooth()` to try to get a clearer picture:

```{r warning=FALSE}
ggplot(average_mae_history, aes(x = epoch, y = validation_mae)) + geom_smooth()
```

According to this plot, it appears the validation MAE stops improving significantly after 70 epochs. Past that point, overfitting is a problem.

Train a final "production" model on all of the training data, with the best parameters, then look at its performance on the test data:

```{r, echo=FALSE, results='hide'}
# Get a fresh, compiled model.
model <- build_model()

# Train it on the entirety of the data.
model %>% fit(train_data, train_targets, epochs = 80, batch_size = 16, verbose = 0)

result <- model %>% evaluate(test_data, test_targets)
```

```{r}
result
```

Still off by about \$2,680.

### Wrapping up

* Regression is done using different loss functions from classification; `Mean Squared Error (MSE)` is a commonly used loss function for regression.
* Evaluation metrics to be used for regression differ from those used for classification; naturally the concept of "accuracy" does not apply for regression. A common regression metric is `Mean Absolute Error (MAE)`.
* When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.
* When there is little data available, using K-Fold validation is a great way to reliably evaluate a model.
* When little training data is available, it is preferable to use a small network with very few hidden layers (typically only one or two) in order to avoid severe overfitting.

## Over/Under Fitting

```{r echo=FALSE}
rm(list = ls())
```

Above, the performance of the models on the held-out validation data would always peak after a few epochs and would then start degrading, i.e. our model would quickly start to _overfit_ to the training data. Overfitting happens in every single machine learning problem. Learning how to deal with overfitting is essential to mastering machine learning.

The fundamental issue in machine learning is the tension between optimization and generalization. "Optimization" refers to the process of adjusting a model to get the best performance possible on the training data (the "learning" in "machine learning"), while "generalization" refers to how well the trained model would perform on data it has never seen before. The goal of the game is to get good generalization, of course, but you do not control generalization; you can only adjust the model based on its training data.

At the beginning of training, optimization and generalization are correlated: the lower your loss on training data, the lower your loss on test data. While this is happening, your model is said to be _under-fit_: there is still progress to be made; the network hasn't yet modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops improving, validation metrics stall then start degrading: the model is then starting to over-fit, i.e. is it starting to learn patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data.

To prevent a model from learning misleading or irrelevant patterns found in the training data, _the best solution is of course to get more training data_. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution is to modulate the quantity of information that your model is allowed to store, or to add constraints on what information it is allowed to store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.

The processing of fighting overfitting in this way is called _regularization_. Review some of the most common regularization techniques and  apply them to improve our movie classification model.

Prepare by re-running some of the IMDB Movie Review code above.

```{r}
library(keras)

imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb

vectorize_sequences <- function(sequences, dimension = 10000) {
  # Create an all-zero matrix of shape (len(sequences), dimension)
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    # Sets specific indices of results[i] to 1s
    results[i, sequences[[i]]] <- 1
  results}

# Our vectorized training data
x_train <- vectorize_sequences(train_data)
# Our vectorized test data
x_test <- vectorize_sequences(test_data)

# Our vectorized labels
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

### Fighting overfitting

#### Reducing the network size

The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model's *capacity*. Intuitively, a model with more parameters will have more "memorization capacity" and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digits in the MNIST training set: would only need 10 binary parameters for each of the 50,000 digits. Such a model would be useless for classifying new digit samples. Always keep this in mind: **deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting**.

On the other hand, if the network has limited memorization resources, it will not be able to learn this mapping as easily, and thus, in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets - precisely the type of representations that are interesting. At the same time, using models that have enough parameters that they won't be underfitting: your model shouldn't be starved for memorization resources. There is a compromise to be found between "too much capacity" and "not enough capacity".

Unfortunately, there is no magical formula to determine what the right number of layers is, or what the right size for each layer is. Must  evaluate an array of different architectures (on your validation set, not on your test set) in order to find the right model size for your data. The general workflow to find an appropriate model size is to start with relatively few layers and parameters and start increasing the size of the layers or adding new layers until you see diminishing returns with regard to the validation loss.

Try this on the movie review classification network. The original network was:

```{r}
original_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

original_model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
```

Try to replace it with this smaller network:

```{r}
smaller_model <- keras_model_sequential() %>% 
  layer_dense(units = 4, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 4, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

smaller_model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
```

Train both the original and smaller network:

```{r, echo=FALSE, results='hide'}
original_hist <- original_model %>% fit(x_train, y_train, epochs = 20, batch_size = 512, validation_data = list(x_test, y_test))
```

```{r, echo=FALSE, results='hide'}
smaller_model_hist <- smaller_model %>% fit(x_train, y_train, epochs = 20, batch_size = 512, validation_data = list(x_test, y_test))
```

To compare the losses we'll write an R function that takes a named list of loss series and plots it:

```{r}
library(ggplot2)
library(tidyr)
plot_training_losses <- function(losses) {
  loss_names <- names(losses)
  losses <- as.data.frame(losses)
  losses$epoch <- seq_len(nrow(losses))
  losses %>% gather(model, loss, loss_names[[1]], loss_names[[2]]) %>% 
    ggplot(aes(x = epoch, y = loss, colour = model)) + geom_point()}
```

Compare the validation losses of the original network and the smaller network:

```{r}
plot_training_losses(losses = list(
  original_model = original_hist$metrics$val_loss,
  smaller_model = smaller_model_hist$metrics$val_loss))
```

The smaller network starts overfitting later than the reference one and its performance degrades much more slowly once it starts overfitting.

Add to this benchmark a network that has much more capacity, far more than the problem would warrant:

```{r}
bigger_model <- keras_model_sequential() %>% 
  layer_dense(units = 512, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

bigger_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c('acc'))
```

```{r, echo=FALSE, results='hide'}
bigger_model_hist <- bigger_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_test, y_test))
```

Here's how the bigger network fares compared to the reference one:

```{r}
plot_training_losses(losses = list(
  original_model = original_hist$metrics$val_loss,
  bigger_model = bigger_model_hist$metrics$val_loss))
```

The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also more noisy.

Meanwhile, here are the training losses for our two networks:

```{r}
plot_training_losses(losses = list(
  original_model = original_hist$metrics$loss,
  bigger_model = bigger_model_hist$metrics$loss))
```

As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss).

#### Adding weight regularization

You may be familiar with _Occam's Razor_ principle: given two explanations for something, the explanation most likely to be correct is the "simplest" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple _models_) that could explain the data, and simpler models are less likely to overfit than complex ones.

A "simple model" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to only take small values, which makes the distribution of weight values more "regular". This is called *weight regularization* and it is done by adding to the loss function of the network a _cost_ associated with having large weights. This cost comes in two flavors:

- L1 regularization, where the cost added is proportional to the _absolute value of the weights coefficients_ (i.e. to what is called the "L1 norm" of the weights).
- L2 regularization, where the cost added is proportional to the _square of the value of the weights coefficients_ (i.e. to what is called the "L2 norm" of the weights). L2 regularization is also called _weight decay_ in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.

In Keras, weight regularization is added by passing _weight regularizer instances_ to layers as keyword arguments. Let's add L2 weight regularization to our movie review classification network:

```{r}
l2_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001),
              activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001),
              activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

l2_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc"))
```

`regularizer_l2(0.001)` means every coefficient in the weight matrix of the layer will add `0.001 * weight_coefficient_value` to the total loss of the network. Note that because this penalty is _only added at training time_, the loss for this network will be much higher at training than at test time. 

Here's the impact of our L2 regularization penalty:

```{r, echo=FALSE, results='hide'}
l2_model_hist <- l2_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_test, y_test))
```

```{r}
plot_training_losses(losses = list(
  original_model = original_hist$metrics$val_loss,
  l2_model = l2_model_hist$metrics$val_loss))
```

As you can see, the model with L2 regularization has become much more resistant to overfitting than the reference model, even though both models have the same number of parameters.

As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:

```{r, echo=TRUE, eval=FALSE}
# L1 regularization
regularizer_l1(0.001)

# L1 and L2 regularization at the same time
regularizer_l1_l2(l1 = 0.001, l2 = 0.001)
```

#### Adding dropout

```{r, eval=FALSE}
# At training time: we drop out 50% of the units in the output
layer_output <- layer_output * sample(0:1, length(layer_output), replace = TRUE)  
```

At test time, we would be scaling the output down by the dropout rate. Here we scale by 0.5 (because we were previous dropping half the units):

```{r, eval=FALSE}
# At test time:
layer_output <- layer_output * 0.5 
```

Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time which is often the way it is implemented in practice:

```{r, eval=FALSE}
# At training time:
layer_output <- layer_output * sample(0:1, length(layer_output),
                                      replace = TRUE) 

# Note that we are scaling *up* rather scaling *down* in this case
layer_output <- layer_output / 0.5
```

This technique may seem strange and arbitrary. Why would this help reduce overfitting? Geoff Hinton has said that he was inspired, among other things, by a fraud prevention mechanism used by banks -- in his own words: _"I went to my bank. The tellers kept changing and I asked one of them why. He said he did not know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting"_.

The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that are not significant (what Hinton refers to as "conspiracies"), which the network would start memorizing if no noise was present. 

In Keras, you can introduce dropout in a network via `layer_dropout()`, which is applied to the output of layer right before it:

```{r, echo=TRUE, eval=FALSE}
layer_dropout(rate = 0.5)
```

Let's add two dropout layers in our IMDB network to see how well they do at reducing overfitting:

```{r}
dpt_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

dpt_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc"))
```

```{r, echo=TRUE, results='hide'}
dpt_model_hist <- dpt_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_test, y_test))
```

Let's plot the results:

```{r}
plot_training_losses(losses = list(
  original_model = original_hist$metrics$val_loss,
  dpt_model = dpt_model_hist$metrics$val_loss))
```

Improvement over the reference network.

* Getting more training data.
* Reducing the capacity of the network.
* Adding weight regularization.
* Adding dropout.

## Universal ML Workflow

### Problem Definition

1. Defining the problem and assembling a dataset 
2. Choosing a measure of success
3. Deciding on an evaluation protocol
4. Preparing your data
5. Developing a model that does better than a baseline
6. Scaling up: developing a model that overfits
7. Regularizing your model and tuning your hyperparameters

### Data availability is often the the problem.

What type of problem?  Binary Classification? Multiclass Classification? Scaler Regression? Vector Regression? Multilabel Classification?

> Vector Regression A task where the target is a set of continuous values: for example, a continuous vector. If you are doing regression against multiple values (such as the coordinates of a bounding box in an image), then you are doing vector regression

Something else, like clustering, generation, or reinforcement learning? Identifying the problem type will guide your choice of model architecture, loss function, etc.

You cannot move to the next stage until you know what your inputs and outputs are, and what data you will use. Be aware of the hypotheses you make at this stage:

- You hypothesize that your outputs can be predicted given your inputs.
- You hypothesize that your available data is sufficiently informative to learn the relationship between inputs and outputs.

Until you have a working model, these are merely hypotheses, waiting to be validated or invalidated. Not all problems can be solved; just because you assembled examples of inputs X and targets Y does not mean X contains enough information to predict Y. For instance, if you are trying to predict the movements of a stock on the stock market given its recent price history, you are unlikely to succeed, because price history does not contain much predictive information.

One class of unsolvable problems you should be aware of is nonstationary problems. Suppose you are trying to build a recommendation engine for clothing, you are training it on one month of data (August), and you want to start generating recommendations in the winter. One big issue is that the kinds of clothes people buy change from season to season: clothes buying is a nonstationary phenomenon over the scale of a few months. What you are trying to model changes over time. In this case, the right move is to constantly retrain your model on data from the recent past, or gather data at a timescale where the problem is stationary. For a cyclical problem like clothes buying, a few years worth of data will suffice to capture seasonal variations but remember to make the time of the year an input of your model!

Keep in mind that machine learning can only be used to memorize patterns that are present in your training data. You can only recognize what you have seen before. Using machine learning trained on past data to predict the future is making the assumption that the future will behave like the past. That often is not the case. 

### Choosing a measure of success

To control something, you need to be able to observe it. To achieve success, you must define what you mean by success accuracy? Precision and recall? Customer-retention rate? Your metric for success will guide the choice of a loss function: what your model will optimize. It should directly align with your higher-level goals, such as the success of your business.

For _balanced-classification problems_, where every class is equally likely, _accuracy_ and _area under the receiver operating characteristic curve (ROC AUC)_ are common metrics. For _class-imbalanced problems_, you can use _precision_ and _recall_. For _ranking_ problems or _multilabel classification,_ you can use _mean average precision_. 

### Deciding on an evaluation protocol

Once you know what you are aiming for, you must establish how you will measure your current progress. We have previously reviewed three common evaluation protocols:

- Maintaining a hold-out validation sets. The way to go when you have plenty of data
- Doing K-fold cross-validation. The right choice when you have too few samples for hold-out validation to be reliable
- Doing iterated K-fold validation. For performing highly accurate model evaluation when little data is available

Just pick one of these. In most cases, the first will work well enough. 

### Preparing your data

Once you know what you are training on, what you are optimizing for, and how to evaluate your approach, you are almost ready to begin training models. But first, you should format your data in a way that can be fed into a machine-learning models here, assume a deep neural network:

- Data should be formatted as tensors.
- The values taken by these tensors should usually be scaled to small values: for example, in the [-1, 1] range or [0, 1] range.
- If different features take values in different ranges (heterogeneous data), then the data should be normalized.
- You may want to do some feature engineering, especially for small-data problems.

Once your tensors of input data and target data are ready, you can begin to train models. 

### Developing a model that does better than a baseline

Your goal at this stage is to achieve *statistical power*: that is, to develop a small model that is capable of beating a dumb baseline. In the MNIST digit-classification example, anything that achieves an accuracy greater than 0.1 can be said to have statistical power; in the IMDB example, it is anything with an accuracy greater than 0.5.

 Note that it is not always possible to achieve statistical power. If you cannot beat a random baseline after trying multiple reasonable architectures, it may be that the answer to the question you are asking is not  present in the input data. Remember that you make two hypotheses:

1. You hypothesize that your outputs can be predicted given your inputs.
2. You hypothesize that the available data is sufficiently informative to learn the relationships between inputs and outputs.

It may well be that these hypotheses are false, in which case you must go back to the drawing board.

Assuming things go well, you need to make three key choices to build your first working model:

1. _Last-layer activation_ - This establishes useful constraints on the networks output. For instance, the IMDB classification example used `sigmoid` in the last layer; the regression example did not use any last-layer activation.
2. _Loss function_ - This should match the type of problem you are trying to solve. For instance, the IMDB example used `binary_crossentropy`, the regression example used `mse`, and so on.
3. _Optimization configuration_ - What optimizer will you use? What will its learning rate be? In most cases, it is safe to go with `rmsprop` and its default learning rate.

Regarding the choice of a loss function, note that it is not always possible to directly optimize for the metric that measures success on a problem. Sometimes there is no easy way to turn a metric into a loss function; loss functions, after all, need to be computable given only a mini-batch of data (ideally, a loss function should be computable for as little as a single data point) and must be differentiable (otherwise, you cannot use backpropagation to train your network). For instance, the widely used classification metric `ROC AUC` cannot be directly optimized. Hence, in classification tasks, it is common to optimize for a proxy metric of `ROC AUC`, such as `crossentropy.` In general, you can hope that the lower the `crossentropy` gets, the higher the `ROC AUC` will be.

The table below can help you choose a last-layer activation and a loss function for a few common problem types.

```{r}
#Note below when creating tibble must use  = not <-
gt_tbl <- dplyr::tibble(
     problem = c("Binary classification", "Multiclass, single-label classification",
                  "Multiclass, multilabel classification", "Regression to arbitrary values",
                  "Regression to values between 0 and 1"),
     last = c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
     loss = c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
               "MSE", "MSE or binary_crossentropy")) %>% 
     gt() %>% cols_label(
          problem = md("**Problem**"), 
          last = md("**Last Layer Activation**"), 
          loss = md("**Loss Function**")) %>% 
     tab_header(md("**Selecting Last Layer Activation and Loss**")) %>% 
     tab_options(heading.background.color = "#3d5657")
gt_tbl
```

### Scaling up: developing a model that overfits

Once you have obtained a model that has statistical power, the question becomes, i_s it sufficiently powerful_? Does it have enough layers and parameters to properly model the problem at hand? For instance, a network with a single hidden layer with two units would have statistical power on MNIST but would not be sufficient to solve the problem well. Remember that the universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting, between under-capacity and overcapacity. To figure out where this border lies, first you must cross it.

To figure out how big a model you will need, you must develop a model that overfits. This is fairly easy:

1. Add layers.
2. Make the layers bigger.
3. Train for more epochs.

Always monitor the training loss and validation loss, as well as the training and validation values for any metrics you care about. __When you see that the models performance on the validation data begins to degrade, you have achieved overfitting__.

The next stage is to start regularizing and tuning the model, to get as close as possible to the ideal model that neither under fits nor overfits. 

### Regularizing the model & tuning  hyperparameters

This step will take the most time: you will repeatedly modify your model, train it, evaluate on your validation data, modify it again and repeat, until the model is as good as it can get.

These are some things you should try:

- Add dropout.
- Try different architectures: add or remove layers.
- Add L1 and/or L2 regularization.
- Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.
- Optionally, iterate on feature engineering: add new features, or remove features that do not seem to be informative.

Be mindful of the following: _every time you use feedback from your validation process to tune your model, you leak information about the validation process into the model_. Repeated just a few times, this is innocuous; but done systematically over many iterations, it will eventually cause your model to overfit to the validation process (even though no model is directly trained on any of the validation data). This makes the evaluation process less reliable.

Once you have developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set. If it turns out that the performance on the test set is significantly worse than the performance measured on the validation data, this may mean either that your validation procedure was not reliable after all, or that you began overfitting to the validation data while tuning the parameters of the model. In this case, _you may want to switch to a more reliable evaluation protocol (such as iterated K-fold validation)_. 

### Summary

- Define the problem at hand and the data on which you will train. Collect this data, or annotate it with labels if need be.
- Choose how you will measure success on your problem. Which metrics will you monitor on your validation data?
- Determine your evaluation protocol: Hold-out validation? K-fold validation? Which portion of the data should you use for validation?
- Develop a first model that does better than a basic baseline: a model with statistical power.
- Develop a model that overfits.
- Regularize your model and tune its hyperparameters, based on performance on the validation data. A lot of machine-learning research tends to focus only on this step but keep the big picture in mind.

# Part 2:  Deep Learning in Practice

```{r echo=FALSE}
rm(list = ls())
```

## Convnets

Convnets is a type of deep-learning model almost universally used in computer vision applications.

Get started by examining a simple convnet.  The following lines of code show you what a basic convnet looks like. it is a stack of `layer_conv_2d` and `layer_max_pooling_2d` layers. 
```{r}
model <- keras_model_sequential() %>% 
     layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", 
                   input_shape = c(28, 28, 1)) %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
     layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>%
     layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")
```

Importantly, a convnet takes as input tensors of shape (image_height, image_width, image_channels) (not including the batch dimension). In this case,  configure the convnet to process inputs of size (28, 28, 1), which is the format of MNIST images. We will do this by passing the argument `input_shape = c(28, 28, 1)` to the first layer.

Display the architecture of the convnet so far:

```{r echo=FALSE}
model
```

The output of every `layer_conv_2d` and `layer_max_pooling_2d` is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of channels is controlled by the first argument passed to `layer_conv_2d` (32 or 64).

The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network: a stack of dense layers. These classifiers process vectors, which are 1D, whereas the current output is a 3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few dense layers on top. Do 10-way classification using a final layer with 10 outputs and a softmax activation.

```{r}
model <- model %>%
     layer_flatten() %>% 
     layer_dense(units = 64, activation = "relu") %>% 
     layer_dense(units = 10, activation = "softmax")
```

```{r echo=FALSE}
model
```
The (3, 3, 64) outputs are flattened into vectors of shape (576 = 3 x 3 x 64) before going through two dense layers.

Train the convnet on the MNIST digits. 

```{r eval=FALSE}
mnist <- dataset_mnist() 
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist

train_images <- array_reshape(train_images, c(60000, 28, 28, 1)) 
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28, 28, 1)) 
test_images <- test_images / 255
train_labels <- to_categorical(train_labels) 
test_labels <- to_categorical(test_labels)

model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", 
                                    metrics = c("accuracy"))

model %>% fit(train_images, train_labels, epochs = 5, batch_size=64)

results <- model %>% evaluate(test_images, test_labels)
results

save.image("convnet1.RData")
```

```{r convnet1, echo=FALSE}
load("convnet1.RData")
results
```

### Convolution Operation

The fundamental difference between a densely connected layer and a convolution layer is  _dense layers learn global patterns in their input feature space (for example, for an MNIST digit, patterns involving all pixels), whereas convolution layers learn local patterns_: in the case of images, patterns found in small 2D windows of the inputs. In the previous example, these windows were all 3 x 3.

This key characteristic gives convnets two interesting properties:

1. _The patterns they learn are translation invariant_. After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn the pattern anew if it appeared at a new location. This makes convnets data efficient when processing images (because _the visual world is fundamentally translation invariant_): they need fewer training samples to learn representations that have generalization power.
2. _They can learn spatial hierarchies of patterns_. A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (_because the visual world is fundamentally spatially hierarchical_).

Convolutions operate over 3D tensors, called _feature maps_, with two spatial axes (height and width) as well as a depth axis (also called the channels axis). For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. For a black-and-white picture, like the MNIST digits, the depth is 1 (levels of gray). The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an output feature map. This output _feature map is still a 3D tensor: it has a width and a height. Its depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB input_; rather, they stand for filters. Filters encode specific aspects of the input data: at a high level.

In the MNIST example, the first convolution layer takes a feature map of size (28, 28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its input. Each of these 32 output channels contains a 26 x 26 grid of values, which is a response map of the filter over the input, indicating the response of that filter pattern at different locations in the input. That is what the term feature map means: every dimension in the depth axis is a feature (or filter), and the 2D tensor output`[:, :, n]` is the 2D spatial map of the response of this filter over the input.

```{r, out.width = "500px", echo=FALSE}
knitr::include_graphics("./images/convnet1.jpg")
```

Convolutions are defined by two key parameters:

1. _Size of the patches extracted from the inputs_ - These are typically 3 x 3 or 5 x 5. In the example, they were 3 x 3, which is a common choice.
2. _Depth of the output feature map_ - The number of filters computed by the convolution. The example started with a depth of 32 and ended with a depth of 64.

In Keras, these parameters are the first arguments passed to the layer: `layer_conv_2d`

`(output_depth, c(window_height, window_width))`.

A convolution works by sliding these windows of size 3 x 3 or 5 x 5 over the 3D input feature map, stopping at every possible location, and extracting the 3D patch of surrounding features (shape (window_height, window_width, input_depth)). Each such 3D patch is then transformed (via a tensor product with the same learned weight matrix, called the convolution kernel) into a 1D vector of shape (output_depth). All of these vectors are then spatially reassembled into a 3D output map of shape (height, width, output_depth). Every spatial location in the output feature map corresponds to the same location in the input feature map (for example, the lower-right corner of the output contains information about the lower-right corner of the input). For instance, with 3 x 3 windows, the vector `output[i, j, ]` comes from the 3D patch `input[i-1:i+1, j-1:j+1, ]`.

Note that the output width and height may differ from the input width and height. They may differ for two reasons:

1. _Border effects_s_, which can be countered by padding the input feature map
2. The use of strides

#### Understanding Border Effects/Padding

Consider a 5 x 5 feature map (25 tiles total). There are only 9 tiles around which you can center a 3 x 3 window, forming a 3 x 3 grid. Hence, the output feature map will be 3 x 3. It shrinks a little: by exactly two tiles alongside each dimension, in this case. You can see this border effect in action in the earlier example: you start with 28 x 28 inputs, which become 26 x 26 after the first convolution layer.

If you want to get an output feature map with the same spatial dimensions as the input, you can use padding. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map to make it possible to fit center convolution windows around every input tile. For a 3 x 3 window, you add one column on the right, one column on the left, one row at the top, and one row at the bottom. For a 5 x 5 window, you add two rows.

In `layer_conv_2d` layers, padding is configurable via the `padding` argument, which takes two values: `"valid"`, which means no padding (only valid window locations will be used); and `"same"`, which means pad in such a way as to have an output with the same width and height as the input. The padding argument defaults to `"valid"`. 

#### Understanding Convolution Strides

The other factor that can influence output size is the notion of strides. The description of convolution so far has assumed that the center tiles of the convolution windows are all contiguous. But the distance between two successive windows is a parameter of the convolution, called its _stride_, which defaults to 1. it is possible to have strided convolutions: convolutions with a stride higher than 1. 

Using stride 2 means the width and height of the feature map are downsampled by a factor of 2 (in addition to any changes induced by border effects). _strided convolutions are rarely used in practice_, although they can come in handy for some types of models; it is good to be familiar with the concept.

To downsample feature maps, instead of _strides_, we tend to use the _max-pooling_ operation, which you saw in action in the first convnet example. 

### Max-Pooling

In the convnet example, you may have noticed that the size of the feature maps is halved after every `layer_max_pooling_2d`. For instance, before the first `layer_max_` pooling_2d, the feature map is 26 x 26, but the max-pooling operation halves it to 13 x 13. __Thats the role of max pooling: to aggressively downsample feature maps__, much like strided convolutions.

Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. it is conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel), they are transformed via a hardcoded max tensor operation. A big difference from convolution is that max pooling is usually done with 2 x 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 x 3 windows and no stride (stride 1).

Why downsample feature maps this way? Why not remove the max-pooling layers and keep fairly large feature maps all the way up? Look at this option. The convolutional base of the model would then look like this:

```{r}
model_no_max_pool <- keras_model_sequential() %>% 
     layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", 
                   input_shape = c(28, 28, 1)) %>%
     layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
     layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")
model_no_max_pool
```

Whats wrong with this setup? Two things:

1. It is not conducive to learning a spatial hierarchy of features. The high-level patterns learned by the convnet will be very small with regard to the initial input, which may not be enough to learn to classify digits (try recognizing a digit by only looking at it through windows that are 7 x 7 pixels!). We need the features from the last convolution layer to contain information about the totality of the input.
2. The final feature map has 22 x 22 x 64 = 30,976 total coefficients per sample. This is huge. If you were to flatten it to stick a dense layer of size 512 on top, that layer would have 15.8 million parameters. This is far too large for such a small model and would result in intense overfitting.

In short, __the reason to use down sampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows__ (in terms of the fraction of the original input they cover).

Note that max pooling is not the only way you can achieve such down sampling. As you already know, you can also use strides in the prior convolution layer. And you can use average pooling instead of max pooling, where each local input patch is transformed by taking the average value of each channel over the patch, rather than the max. But max pooling tends to work better than these alternative solutions. In a nutshell, the reason is __features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map (hence, the term feature map), and it is more informative to look at the maximal presence of different features than at their average presence__. So the most reasonable sub-sampling strategy is to first produce dense maps of features (via unstrided convolutions) and then look at the maximal activation of the features over small patches, rather than looking at sparser windows of the inputs (via strided convolutions) or averaging input patches, which could cause you to miss or dilute feature-presence information.

## Training a Convnet from Scratch

```{r echo=FALSE}
rm(list= ls())
```

Having to train an image-classification model using very little data is a common situation, which you will likely encounter in practice if you ever do computer vision in a professional context. A _few_ samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, We will focus on classifying images as dogs or cats, in a dataset containing 4,000 pictures of cats and dogs (2,000 cats, 2,000 dogs). We will use 2,000 pictures for training, 1,000 for validation and 1,000 for testing.

Review one basic strategy to tackle this problem: training a new model from scratch using what little data you have. you will start by naively training a small convnet on the 2,000 training samples, without any regularization, to set a baseline for what can be achieved. This will get you to a classification accuracy of 71%. At that point, the main issue will be overfitting. Then We will introduce __data augmentation__, a powerful technique for mitigating overfitting in computer vision. By using data augmentation, you will improve the network to reach an accuracy of 82%.

Review two more essential techniques for applying deep learning to small datasets: 

- feature extraction with a pretrained network (which will get you to an accuracy of 90% - 96%) 
- fine-tuning a pretrained network (this will get you to a final accuracy of 97%). 

Together, these three strategies:

1. training a small model from scratch
2. feature extraction using a pretrained model
3. fine-tuning a pretrained model

will constitute your future toolbox for tackling the problem of performing image classification with small datasets.
 
### Relevance of Deep Learning for Small-data Problems

you will sometimes hear that deep learning only works when lots of data is available. This is valid in part: one fundamental characteristic of deep learning is that it can find interesting features in the training data on its own, without any need for manual feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where the input samples are very high dimensional, like images.

But what constitutes lots of samples is relative to the size and depth of the network you are trying to train. It is not possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundred can potentially suffice if the model is small and well regularized and the task is simple. Because convnets learn local, translation-invariant features, they are highly data efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results despite a relative lack of data, without the need for any custom feature engineering. 

Deep-learning models are by nature highly repurposable: you can take, say, an image-classification or speech-to-text model trained on a large-scale dataset and reuse it on a significantly different problem with only minor changes. Specifically, in the case of computer vision, many pretrained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data. 

### Downloading the data

The Dogs vs. Cats dataset is made available by Kaggle as part of a computer-vision competition in late 2013, back when convnets were not mainstream. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data.

The pictures are medium-resolution color JPEGs. Unsurprisingly, the dogs-versus-cats Kaggle competition in 2013 was won by entrants who used convnets. The best entries achieved up to 95% accuracy. In this example, you will get fairly close to this accuracy (in the next section), even though you will train your models on less than 10% of the data that was available to the competitors.

This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you will create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.  This was done offline from this transcript.

As a sanity check, count how many pictures are in each training split (train/validation/test):

```{r}
cat("total training cat images:", length(list.files("../../LargeDataFiles/CatsDogs/sample/train/cat/")), "\n")
cat("total training dog images:", length(list.files("../../LargeDataFiles/CatsDogs/sample/train/dog/")), "\n")
```

```{r}
cat("total validation cat images:", length(list.files("../../LargeDataFiles/CatsDogs/sample/validation/cat/")), "\n")
cat("total validation dog images:", length(list.files("../../LargeDataFiles/CatsDogs/sample/validation/dog/")), "\n")
```

```{r}
cat("total test cat images:", length(list.files("../../LargeDataFiles/CatsDogs/sample/test/cat/")), "\n")
cat("total test dog images:", length(list.files("../../LargeDataFiles/CatsDogs/sample/test/dog/")), "\n")
```

### Building the Network

Reuse the same general structure as before: the convnet will be a stack of alternated `layer_conv_2d` (with relu activation) and `layer_max_pooling_2d` stages.

Because you are dealing with bigger images and a more complex problem, make your network larger, accordingly: it will have one more `layer_conv_2d` + `layer_max_pooling_2d` stage. This serves both to augment the capacity of the network and to further reduce the size of the feature maps so they are not overly large when you reach `layer_flatten.` 

Because you start from inputs of size 150 x 150 (an arbitrary choice), you end up with feature maps of size 7 x 7 just before `layer_flatten.`

> The depth of the feature maps progressively increases in the network (from 32 to 128), whereas the size of the feature maps decreases (from 148 x 148 to 7 x 7). This is a pattern you will see in almost all convnets.

Because you are attacking a binary-classification problem, you will end the network with a single unit (a layer_dense of size 1) and a sigmoid activation. This unit will encode the probability that the network is looking at one class or the other.

```{r}
model <- keras_model_sequential() %>%
     layer_conv_2d(filters = 32, kernel_size = c(3, 3), 
                   activation = "relu", input_shape = c(150, 150, 3)) %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
     layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
     layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
     layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
     layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
     layer_flatten() %>% 
     layer_dense(units = 512, activation = "relu") %>% 
     layer_dense(units = 1, activation = "sigmoid")
summary(model)
```

For the compilation step, use `RMSprop` optimizer as usual. Because you ended the network with a single sigmoid unit, you will use `binary crossentropy` as the loss.

```{r}
model %>% compile(loss = "binary_crossentropy", optimizer = optimizer_rmsprop(lr = 1e-4),
                  metrics = c("acc") )
```

### Data preprocessing

As you know, data should be formatted into appropriately preprocessed floating point tensors before being fed into the network. Currently, the data are JPEG files, so the steps for getting it into the network are roughly as follows:

1. Read the picture files.
2. Decode the JPEG content to RGB grids of pixels.
3. Convert these into floating-point tensors.
4. Rescale the pixel values (between 0 and 255) to the [0, 1] interval.

It may seem a bit daunting, but thankfully Keras has utilities to take care of these steps automatically. Keras includes a number of image-processing helper tools. In particular, it includes the `image_data_generator()` function, which can automatically turn image files on disk into batches of preprocessed tensors. 

```{r}
train_datagen <- image_data_generator(rescale = 1/255)	
validation_datagen <- image_data_generator(rescale = 1/255) 

train_generator <- flow_images_from_directory(
     "../../LargeDataFiles/CatsDogs/sample/train/", train_datagen, 
     target_size = c(150, 150), batch_size = 20, class_mode = "binary")

validation_generator <- flow_images_from_directory(
     "../../LargeDataFiles/CatsDogs/sample/validation/", 
     validation_datagen, target_size = c(150, 150), batch_size = 20, 
     class_mode = "binary")
```

Look at the output of one of these generators: it yields batches of 150 x 150 RGB images (shape (20, 150, 150, 3)) and binary labels (shape (20)). There are 20 samples in each batch (the batch size). Note that the generator yields these batches indefinitely - it loops endlessly over the images in the target folder:

```{r}
batch <- generator_next(train_generator)
str(batch)
```

Fit the model to the data using the generator. You do so using the `fit_generator`, the equivalent of fit for data generators like this one. It expects as its first argument a generator that will yield batches of inputs and targets indefinitely, like this one does. Because the data is being generated endlessly, the fitting process needs to know how many samples to draw from the generator before declaring an epoch over.

This is the role of the `steps_per_epoch` argument: after having drawn `steps_per_epoch` batches from the generator after having run for `steps_per_epoch` gradient descent steps the fitting process will go to the next epoch. In this case, batches are 20 samples, so it will take 100 batches until you see your target of 2,000 samples.

When using `fit_generator`, you can pass a `validation_data` argument, much as with the `fit` function. it is important to note that this argument is allowed to be a data generator, but it could also be a list of arrays. If you pass a generator as `validation_data`, then this generator is expected to yield batches of validation data endlessly; thus you should also specify the `validation_steps` argument, which tells the process how many batches to draw from the validation generator for evaluation.

```{r eval=FALSE}
history <- model %>% fit_generator(train_generator, 
                                   steps_per_epoch = 100, 
                                   epochs = 30, 
                                   validation_data = validation_generator,
                                   validation_steps = 50)

model %>% save_model_hdf5("./Deep_Learning_with_R/models/cats_and_dogs_small_1.h5")
save.image("./RData/5-8-ConvnetScratch.RData")
plot(history)
```

```{r echo=FALSE}
load("./RData/5-8-ConvnetScratch.RData")
plot(history)
```

Plot the loss and accuracy of the model over the training and validation data during training. These plots are characteristic of overfitting. The training accuracy increases linearly over time, until it reaches nearly 100%, whereas the validation accuracy stalls at 71% - 75%. The validation loss reaches its minimum after only five epochs and then stalls, whereas the training loss keeps decreasing linearly until it reaches nearly 0.

Because you have relatively few training samples (2,000), overfitting will be your number-one concern. *You already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). Now going to work with a new one, specific to computer vision and used almost universally when processing images with deep-learning models: __data augmentation__*. 

### Using Data Augmentation

Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. *Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images*. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.

In Keras, this can be done by configuring a number of random transformations to be performed on the images read by an `image_data_generator.`

```{r}
datagen <- image_data_generator(
     rescale = 1/255, rotation_range = 40, width_shift_range = 0.2, 
     height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, 
     horizontal_flip = TRUE, fill_mode = "nearest")
```

These are just a few of the options available (for more, see the Keras documentation). 

- _rotation_range_ is a value in degrees (0-180), a range within which to randomly rotate pictures.
- _width_shift_ and _height_shift_ are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.
- _shear_range_ is for randomly applying shearing transformations.
- _zoom_range_ is for randomly zooming inside pictures.
- _horizontal_flip_ is for randomly flipping half the images horizontally are relevant when there are no assumptions of horizontal asymmetry (for example, real-world pictures).
- _fill_mode_ is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.

```{r}
fnames <- list.files("../../LargeDataFiles/CatsDogs/sample/train/dog/", full.names = TRUE)
img_path <- fnames[[7]]
img <- image_load(img_path, target_size = c(150, 150))	
img_array <- image_to_array(img)
img_array <- array_reshape(img_array, c(1, 150, 150, 3))

augmentation_generator <- flow_images_from_data(img_array, generator = datagen, 
                                                batch_size = 1)

op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0)) 

for (i in 1:4) {
batch <- generator_next(augmentation_generator) 
plot(as.raster(batch[1,,,]))} 

par(op)
```

If you train a new network using this data-augmentation configuration, the network will never see the same input twice. But the inputs it sees are still heavily intercorrelated, because they come from a small number of original images you cannot produce new information, you can only remix existing information. As such, this may not be enough to completely get rid of overfitting. To further fight overfitting, you will also add a dropout layer to your model, right before the densely connected classifier.

```{r}
model <- keras_model_sequential() %>%
     layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", 
                   input_shape = c(150, 150, 3)) %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>%
     layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
     layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
     layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
     layer_flatten() %>% 
     layer_dropout(rate = 0.5) %>% 
     layer_dense(units = 512, activation = "relu") %>% 
     layer_dense(units = 1, activation = "sigmoid")

model %>% compile(loss = "binary_crossentropy", 
                  optimizer = optimizer_rmsprop(lr = 1e-4), metrics = c("acc"))
```

Train the network using data augmentation and dropout.

```{r eval=FALSE}
datagen <- image_data_generator(
     rescale = 1/255, rotation_range = 40, width_shift_range = 0.2, 
     height_shift_range = 0.2, shear_range = 0.2,
     zoom_range = 0.2, horizontal_flip = TRUE)

test_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
     "../../LargeDataFiles/CatsDogs/sample/train/", datagen, 
     target_size = c(150, 150), batch_size = 32, class_mode = "binary")
 
validation_generator <- flow_images_from_directory(
     "../../LargeDataFiles/CatsDogs/sample/validation/",
     test_datagen,
     target_size = c(150, 150), 
     batch_size = 32, 
     class_mode = "binary")

history <- model %>% fit_generator(
     train_generator, 
     steps_per_epoch = 100, 
     epochs = 100, 
     validation_data = validation_generator, 
     validation_steps = 50)

model %>% save_model_hdf5("./models/cats_and_dogs_small_2.h5")
save.image("./RData/convnet53.RData")
```

```{r echo=FALSE}
rm(list=ls())
#load_model_hdf5("Deep_Learning_with_R/models/cats_and_dogs_small_2.h5")
load("./RData/convnet53.RData")
```

Thanks to data augmentation and dropout, you are no longer overfitting: the training curves are closely tracking the validation curves. You can now reach an accuracy of 82%, a 15% relative improvement over the non-regularized model.

```{r}
plot(history)
```

By using regularization techniques even further, and by tuning the networks parameters (such as the number of filters per convolution layer, or the number of layers in the network), you may be able to get an even better accuracy, likely up to 86% or 87%. But it would prove difficult to go any higher just by training your own convnet from scratch, because you have so little data to work with. As a next step to improve your accuracy on this problem, you will have to use a pretrained model, which is the focus of the next two sections. 

## Pretrained Convnet

A common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computervision problems, even though these new problems may involve completely different classes than those of the original task. For instance, you might train a network on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained network for something as remote as identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning compared to many older, shallow-learning approaches, and it makes deep learning very effective for small-data problems.

Consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs, and you can thus expect to perform well on the dogs-versus-cats classification problem.

you will use the `VGG16` architecture; it is a simple and widely used convnet architecture for ImageNet. Although it is an older model, far from the current state of the art and somewhat heavier than many other recent models, it is selected because its architecture is similar to what you are already familiar with and is easy to understand without introducing any new concepts. This may be your first encounter with one of these model names -`VGG`, `ResNet`, `Inception`, `Inception-ResNet`, `Xception`, and so on; you will get used to them, because they will come up frequently if you keep doing deep learning for computer vision.

There are two ways to use a pretrained network: 

- feature extraction
- fine-tuning

### Feature Extraction

```{r echo=FALSE}
rm(list=ls())
```

Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier which is trained from scratch.

As you saw previously, convnets used for image classification comprise two parts: 

- start with a series of pooling and convolution layers
- end with a densely connected classifier

The first part is called the `convolutional base of the model`. In the case of convnets, feature extraction consists of taking the convolutional base of a previously trained network, running the new data through it, and training a new classifier on top of the output.

Why only reuse the convolutional base? Could you reuse the densely connected classifier as well? In general, doing so should be avoided. The reason is  the representations learned by the convolutional base are likely to be more generic and therefore more reusable: the feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer-vision problem at hand. 

- The representations learned by the classifier will necessarily be specific to the set of classes on which the model was trained they will only contain information about the presence probability of this or that class in the entire picture. 
- The representations found in densely connected layers no longer contain any information about where objects are located in the input image: these layers get rid of the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely connected features are largely useless.

Note that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as a cat ear or a dog eye). So if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.

In this case, because the ImageNet class set contains multiple dog and cat classes, it is likely to be beneficial to reuse the information contained in the densely connected layers of the original model. But We will choose not to, in order to cover the more general case where the class set of the new problem does not overlap the class set of the original model. Put this in practice by using the convolutional base of the `VGG16` network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features.

The `VGG16` model, among others, comes prepackaged with Keras. Heres the list of image-classification models (all pretrained on the ImageNet dataset) that are available as part of Keras:

- Xception
- Inception V3
- ResNet50
- VGG16
- VGG19
- MobileNet

Instantiate the VGG16 model.

```{r message=FALSE}
conv_base <- application_vgg16(weights = "imagenet", 
                               include_top = FALSE, 
                               input_shape = c(150, 150, 3))
```

You pass three arguments to the function:

1. `weights` specifies the weight checkpoint from which to initialize the model.
2. `include_top` refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because you intend to use your own densely connected classifier (with only two classes: cat and dog), you do not need to include it.
3. `input_shape` is the shape of the image tensors that you will feed to the network. This argument is purely optional: if you do not pass it, the network will be able to process inputs of any size.

Here is the detail of the architecture of the VGG16 convolutional base. it is similar to the simple convnets you are already familiar with:

```{r}
conv_base
```

The final feature map has shape (4, 4, 512). That is the feature on top of which you will stick a densely connected classifier.

There are two ways you could proceed:

1. Running the convolutional base over your dataset, recording its output to an array on disk, and then using this data as input to a standalone, densely connected classifier similar to those you saw in part 1 of this book. This solution is fast and cheap to run because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique will not allow you to use data augmentation.
2. Extending the model you have (conv_base) by adding dense layers on top, and running the whole thing end to end on the input data. This will allow you to use data augmentation, because every input image goes through the convolutional base every time it is seen by the model. But for the same reason, this technique is far more expensive than the first.

We will cover both techniques. Walk through the code required to set up the first one: __recording the output of conv_base__ on your data and using these outputs as inputs to a new model.

#### Fast Feature Extraction w/o Data Augmentation

Start by running instances of the previously introduced `image_data_generator` to extract images as arrays as well as their labels. you will extract features from these images by calling the predict method on the model.

```{r eval=FALSE}
base_dir <- "../../LargeDataFiles/CatsDogs/sample" 
train_dir <- file.path(base_dir, "train") 
validation_dir <- file.path(base_dir, "validation") 
test_dir <- file.path(base_dir, "test")

datagenFE <- image_data_generator(rescale = 1/255) 
batch_size <- 20 

extract_features <- function(directory, sample_count) {
     features <- array(0, dim = c(sample_count, 4, 4, 512)) 
     labels <- array(0, dim = c(sample_count))
     
generatorFE <- flow_images_from_directory(
     directory = directory, 
     generator = datagenFE, 
     target_size = c(150, 150), 
     batch_size = batch_size, 
     class_mode = "binary")

i <- 0
while(TRUE) {
     batch <- generator_next(generatorFE) 
     inputs_batch <- batch[[1]] 
     labels_batch <- batch[[2]] 
     features_batch <- conv_base %>% predict(inputs_batch)
     
     index_range <- ((i * batch_size)+1):((i + 1) * batch_size) 
     features[index_range,,,] <- features_batch 
     labels[index_range] <- labels_batch
     
     i <- i + 1
     if (i * batch_size >= sample_count) 
          break 
     #Note that because generators yield data indefinitely in a loop, 
     #you must break after every image has been seen once.
     }
 
     list(features = features, labels = labels) 
}

train <- extract_features(train_dir, 2000) 
validation <- extract_features(validation_dir, 1000) 
test <- extract_features(test_dir, 1000)

#The extracted features are currently of shape (samples, 4, 4, 512). you will feed them to a densely connected classifier, so first you must flatten them to (samples, 8192):

reshape_features <- function(features) {
     array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))} 

train$features <- reshape_features(train$features) 
validation$features <- reshape_features(validation$features) 
test$features <- reshape_features(test$features)

#At this point, you can define your densely connected classifier (note the use of dropout for regularization) and train it on the data and labels that you just recorded.

modelFE <- keras_model_sequential() %>%
     layer_dense(units = 256, activation = "relu", input_shape = 4 * 4 * 512) %>%
     layer_dropout(rate = 0.5) %>% 
     layer_dense(units = 1, activation = "sigmoid")


modelFE %>% compile(
     optimizer = optimizer_rmsprop(lr = 2e-5), 
     loss = "binary_crossentropy", 
     metrics = c("accuracy"))

historyFE <- modelFE %>% fit(
     train$features, 
     train$labels, 
     epochs = 30, 
     batch_size = 20, 
     validation_data = list(validation$features, validation$labels))
plot(historyFE)

save.image("./ConvnetFE.RData")
```

```{r echo=FALSE}
load("./RData/ConvnetFE.RData")
plot(historyFE)
```

Training is very fast, because you only have to deal with two dense layers and epoch takes less than one second even on CPU. 
You reach a validation accuracy of about 90%  much better than you achieved in the previous section with the small model trained from scratch. But the plots also indicate that you are overfitting almost from the start  despite using dropout with a fairly large rate. That is because this technique does not use data augmentation, which is essential for preventing overfitting with small image datasets. 

#### Feature Extraction with Data Augmentation

```{r echo=FALSE}
rm(list=ls())
conv_base <- application_vgg16(weights = "imagenet", 
                               include_top = FALSE, 
                               input_shape = c(150, 150, 3))
```

The second technique is much slower and more expensive, but which allows you to use data augmentation during training: extending the `conv_base` model and running it end to end on the inputs.

> NOTE This technique is so expensive that you should only attempt it if you have access to a GPU it is absolutely intractable on a CPU. If you cannot run your code on a GPU, then the previous technique is the way to go.

Because models behave just like layers, you can add a model (like `conv_base`) to a sequential model just like you would add a layer.

```{r}
modelFE2 <- keras_model_sequential() %>%
     conv_base %>% 
     layer_flatten() %>% 
     layer_dense(units = 256, activation = "relu") %>% 
     layer_dense(units = 1, activation = "sigmoid")
modelFE2
```

As you can see, the convolutional base of `VGG16` has 14,714,688 parameters, which is very large. The classifier you are adding on top has 2 million parameters.

Before you compile and train the model, it is very important to freeze the convolutional base. Freezing a layer or set of layers means preventing their weights from being updated during training. If you do not do this, then the representations that were previously learned by the convolutional base will be modified during training. Because the dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.

In Keras, you freeze a network using the `freeze_weights()`:

```{r}
cat("This is the number of trainable weights before freezing",
    "the conv base:", length(modelFE2$trainable_weights), "\n")
#This is the number of trainable weights before freezing the conv base: 30

freeze_weights(conv_base)
cat("This is the number of trainable weights after freezing",
    "the conv base:", length(modelFE2$trainable_weights), "\n") 
#This is the number of trainable weights before freezing the conv base: 4
```

With this setup, only the weights from the two dense layers that you added will be trained. That is a total of four weight tensors: two per layer (the main weight matrix and the bias vector). Note that in order for these changes to take effect, you must first compile the model. If you ever modify weight trainability after compilation, you should then recompile the model, or these changes will be ignored.

Now you can start training your model, with the same data-augmentation configuration that you used in the previous example.

```{r eval=FALSE}
train_datagenFE3 = image_data_generator(
     rescale = 1/255, 
     rotation_range = 40, 
     width_shift_range = 0.2, 
     height_shift_range = 0.2, 
     shear_range = 0.2, 
     zoom_range = 0.2, 
     horizontal_flip = TRUE, 
     fill_mode = "nearest") 

test_datagenFE3 <- image_data_generator(rescale = 1/255)

train_generatorFE3 <- flow_images_from_directory(
     "../LargeDataFiles/CatsDogs/train/",
     train_datagenFE3,	
     target_size = c(150, 150),
     batch_size = 20,
     class_mode = "binary"     )

validation_generatorFE3 <- flow_images_from_directory(
     "../LargeDataFiles/CatsDogs/validation/", 
     test_datagenFE3, 
     target_size = c(150, 150), 
     batch_size = 20, 
     class_mode = "binary")

modelFE2 %>% compile(
     loss = "binary_crossentropy", 
     optimizer = optimizer_rmsprop(lr = 2e-5), 
     metrics = c("accuracy"))

historyFE3 <- modelFE2 %>% fit_generator(
     train_generatorFE3, 
     steps_per_epoch = 100, 
     epochs = 30, 
     validation_data = validation_generatorFE3, 
     validation_steps = 50)
```

A validation accuracy of about 90%. This is much better than you achieved with the small convnet trained from scratch.

```{r eval=FALSE}
plot(historyFE3)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/modelFE3.jpg")
```

#### Fine-tuning

Another widely used technique for model reuse, complementary to feature extraction, is fine-tuning. Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and these top layers. This is called fine-tuning because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.

It was stated earlier that it is necessary to freeze the convolution base of `VGG16` in order to be able to train a randomly initialized classifier on top. For the same reason, it is only possible to fine-tune the top layers of the convolutional base once the classifier on top has already been trained. If the classifier is not already trained, then the error signal propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed. Thus the steps for fine-tuning a network are as follows:

1. Add your custom network on top of an already-trained base network.
2. Freeze the base network.
3. Train the part you added.
4. Unfreeze some layers in the base network.
5. Jointly train both these layers and the part you added.

You already completed the first three steps when doing feature extraction. Proceed with step 4: you will unfreeze your conv_base and then freeze individual layers inside it.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./Deep_Learning_with_R/images/finetuning.jpg")
```

You fine-tune all of the layers from `block3_conv1` on. Why not fine-tune more layers? Why not fine-tune the entire convolutional base? You could. But you need to consider the following:

- Earlier layers in the convolutional base encode more-generic, reusable features, whereas layers higher up encode more-specialized features. it is more useful to fine-tune the more specialized features, because these are the ones that need to be re-purposed on your new problem. There would be fast-decreasing returns in fine-tuning lower layers.
- The more parameters you are training, the more you are at risk of overfitting. The convolutional base has 15 million parameters, so it would be risky to attempt to train it on your small dataset.

Thus, in this situation, it is a good strategy to fine-tune only some of the layers in the convolutional base. Set this up, starting from where you left off in the previous example.

```{r eval=FALSE}
unfreeze_weights(conv_base, from = "block3_conv1")
```

Now you can begin fine-tuning the network. Do this with the `RMSProp` optimizer, _using a very low learning rate_. The reason for using a low learning rate is that you want to limit the magnitude of the modifications you make to the representations of the three layers you are fine-tuning. Updates that are too large may harm these representations.

```{r eval=FALSE}
model %>% compile(loss = "binary_crossentropy", optimizer = optimizer_rmsprop(lr = 1e-5), 
                  metrics = c("accuracy"))

history <- model %>% fit_generator(
     train_generator, 
     steps_per_epoch = 100, 
     epochs = 100, 
     validation_data = validation_generator, 
     validation_steps = 50)
```

Plot the results. you are seeing a nice 6% absolute improvement in accuracy, from about 90% to greater than 96%.

Note that the loss curve does not show any real improvement (in fact, it is deteriorating). You may wonder, how can accuracy stay stable or improve if the loss is not decreasing? The answer is simple: what you display is an average of pointwise loss values; but what matters for accuracy is the distribution of the loss values, not their average, because accuracy is the result of a binary thresholding of the class probability predicted by the model. The model may still be improving even if this is not reflected in the average loss.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/fine tuning2.jpg")
```
 
You can now finally evaluate this model on the test data:

```{r eval=FALSE}
test_generator <- flow_images_from_directory(
     test_dir, 
     test_datagen, 
     target_size = c(150, 150), 
     batch_size = 20, 
     class_mode = "binary")
model %>% evaluate_generator(test_generator, steps = 50)
```

>$loss
[1] 0.2158171
$acc
[1] 0.965

Here you get a test accuracy of 96.5%. In the original Kaggle competition around this dataset, this would have been one of the top results. But using modern deep-learning techniques, you managed to reach this result using only a small fraction of the training data available (about 10%). There is a huge difference between being able to train on 20,000 samples compared to 2,000 samples!
 
### Wrapping up

Here is what you should take away regarding convnets:

- Convnets are the best type of machine-learning models for computer-vision tasks. it is possible to train one from scratch even on a very small dataset, with decent results.
- On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when you are working with image data.
- it is easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets.
- As a complement to feature extraction, you can use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing model. This pushes performance a bit further.

Now you have a solid set of tools for dealing with image-classification problems in particular with small datasets. 

## Visualizing what convnets learn

it is often said that deep-learning models are a black boxes: learning representations that are difficult to extract and present in a human-readable form. Although this is partially true for certain types of deep-learning models, it is definitely not true for convnets. The representations learned by convnets are highly amenable to visualization, in large part because they are representations of visual concepts. Since 2013, a wide array of techniques have been developed for visualizing and interpreting these representations. We will not survey all of them, but We will cover three of the most accessible and useful ones:

1. *Visualizing intermediate convnet outputs (intermediate activations)*  Useful for understanding how successive convnet layers transform their input, and for getting a first idea of the meaning of individual convnet filters.
2. *Visualizing convnets filters*  Useful for understanding precisely what visual pattern or concept each filter in a convnet is receptive to.
3. *Visualizing heatmaps of class activation in an image*  Useful for understanding which parts of an image were identified as belonging to a given class, thus allowing you to localize objects in images.

For the first method  activation visualization  use the small convnet that you trained from scratch on the dogs-versus-cats classification problem earlier. For the next two methods, you will use the `VGG16` model introduced previously.

### Visualizing Intermediate Activations

Visualizing intermediate activations consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its activation, the output of the activation function). This gives a view into how an input is decomposed into the different filters learned by the network. You want to visualize feature maps with three dimensions: width, height, and depth (channels). Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image. Start by loading the model you saved earlier:

```{r echo=FALSE}
rm(list=ls())
```

```{r}
model <- load_model_hdf5("./models/cats_and_dogs_small_2.h5")
model
```

Get an input image  a picture of a cat, not part of the images the network was trained on.

```{r}
img <- image_load("../../LargeDataFiles/CatsDogs/sample/test/cat/cat.9518.jpg", target_size = c(150, 150))

img_tensor <- image_to_array(img)
img_tensor <- array_reshape(img_tensor, c(1, 150, 150, 3))
 
img_tensor <- img_tensor / 255 
dim(img_tensor)
```

```{r}
plot(as.raster(img_tensor[1,,,]))
```

In order to extract the feature maps you want to look at, create a Keras model that takes batches of images as input, and outputs the activations of all convolution and pooling layers. To do this, you will use the `keras_model` function, which takes two arguments: an `input tensor` (or list of input tensors) and an `output tensor` (or list of output tensors). The resulting class is a Keras model, just like the ones created by the `keras_sequential_model()` function mapping the specified inputs to the specified outputs. What sets this type of model apart is that it allows for models with multiple outputs (unlike `keras_sequential_model`). 

```{r}
#Extract the outputs of the top 8 layers
layer_outputs <- lapply(model$layers[1:8], function(layer) layer$output)

#Create a model that will return the outputs given the model input
activation_model <- keras_model(inputs = model$input, outputs = layer_outputs)
```

When fed an image input, this model returns the values of the layer activations in the original model. This is the first time you have encountered a multi-output model so far: until now, the models you have seen have had exactly one input and one output. In the general case, a model can have any number of inputs and outputs. This one has one input and eight outputs: one output per layer activation.

```{r}
# Returns a list of 5 arrays, one array per activation
activations <- activation_model %>% predict(img_tensor)
```

For instance, this is the activation of the first convolution layer for the cat image input:

```{r}
first_layer_activation <- activations[[1]]
dim(first_layer_activation)
```

it is a 148 x 148 feature map with 32 channels. Visualize some of them. First you define an R function that will plot a channel.

```{r plotChannel}
plot_channel <- function(channel) {
     rotate <- function(x) t(apply(x, 2, rev))
     image(rotate(channel), 
           axes = FALSE, 
           asp = 1, 
           col = terrain.colors(12))}
```

Try visualizing the second channel of the activation of the first layer of the original model. It appears to be picking up the edges.

```{r}
plot_channel(first_layer_activation[1,,,3])
```

Try the seventh channel  but note channels may vary because the specific filters learned by convolution layers are not deterministic.

```{r}
plot_channel(first_layer_activation[1,,,7])
```

Cycle through all the channels not working - puzzled:

```{r}
image_size <- 58 
images_per_row <- 16

for (i in 1:8) {
     
     layer_activation <- activations[[i]] 
     layer_name <- model$layers[[i]]$name
     
     n_features <- dim(layer_activation)[[4]] 
     n_cols <- n_features %/% images_per_row
     
     png(paste0("cat_activations_", i, "_", layer_name, ".png"), 
         width = image_size * images_per_row, 
         height = image_size * n_cols)
     op <- par(mfrow = c(n_cols, images_per_row), mai = rep_len(0.02, 4))
     
     for (col in 0:(n_cols-1)) {
          for (row in 0:(images_per_row-1)) {
               channel_image <- layer_activation[1,,,(col*images_per_row) + row + 1]
               plot_channel(channel_image)} }
par(op) 
dev.off()
}
```

The code above should output something like this"

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/catImages1.jpg")
```
```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/catImages2.jpg")
```

There are a few things to note here:

- The first layer acts as a collection of various edge detectors. At that stage, the activations retain almost all of the information present in the initial picture.
- As you go higher, the activations become increasingly abstract and less visually interpretable. They begin to encode higher-level concepts such as a cat ear and a cat eye. Higher presentations carry increasingly less information about the visual contents of the image, and increasingly more information related to the class of the image.
- The sparsity of the activations is increasing with the depth of the layer: in the first layer, all filters are activated by the input image, but in the following layers some filters are blank. This means that the pattern encoded by the filter is not found in the input image.

We have just evidenced an important universal characteristic of the representations learned by deep neural networks: *the features extracted by a layer become increasingly abstract with the depth of the layer*. The activations of higher layers carry less and less information about the specific input being seen, and more and more information about the target (in this case, the class of the image: cat or dog). *A deep neural network effectively acts as an information distillation pipeline*, with raw data going in (in this case, RGB pictures) and being repeatedly transformed so that irrelevant information is filtered out (for example, the specific visual appearance of the image), and useful information is magnified and refined (for example, the class of the image).

This is analogous to the way humans and animals perceive the world: after observing a scene for a few seconds, a human can remember which abstract objects were present in it (bicycle, tree) but cannot remember the specific appearance of these objects. In fact, if you tried to draw a generic bicycle from memory, chances are you could not get it even remotely right, even though you have seen thousands of bicycles in your lifetime. Your brain has learned to completely abstract its visual input to transform it into high-level visual concepts while filtering out irrelevant visual details making it tremendously difficult to remember how things around you look. 

### Visualizing convnet filters

Skipped this - did not find this content useful

### Visualizing Heatmaps of Class Activation

There is a 3rd visualization technique: one that is useful for understanding which parts of a given image led a convnet to its final classification decision. This is helpful for debugging the decision process of a convnet, particularly in the case of a classification mistake. It also allows you to locate specific objects in an image.

This general category of techniques is called `class activation map (CAM)` visualization and it consists of producing heatmaps of class activation over input images. A class activation heatmap is a 2D grid of scores associated with a specific output class, computed for every location in any input image, indicating how important each location is with respect to the class under consideration. For instance, given an image fed into a dogs-versus-cats convnet, CAM visualization allows you to generate a heatmap for the class cat, indicating how cat-like different parts of the image are, and also a heatmap for the class dog, indicating how dog-like parts of the image are.

The specific implementation you will use is the one described in *rad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.*  it is very simple: it consists of taking the output feature map of a convolution layer, given an input image, and weighing every channel in that feature map by the gradient of the class with respect to the channel. Intuitively, one way to understand this trick is that you are weighting a spatial map of *how intensely the input image activates different channels* by *how important each channel is with regard to the class,* resulting in a spatial map of *how intensely the input image activates the class.*

Demonstrate this technique using the pretrained `VGG16` network again.

```{r}
rm(list=ls())
model <- application_vgg16(weights = "imagenet")
```

Consider the image of two African elephants shown below (under a Creative Commons license), possibly a mother and her calf, strolling on the savanna.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./Deep_Learning_with_R/images/elephant1.jpg")
```

Convert this image into something the `VGG16` model can read: the model was trained on images of size 224 x 244, preprocessed according to a few rules that are packaged in the utility function `imagenet_preprocess_input()`. You need to load the image, resize it to 224 x 224, convert it to an array, and apply these preprocessing rules.

```{r}
img_path <- "./images/elephant1.jpg" 
img <- image_load(img_path, target_size = c(224, 224)) %>%
     image_to_array() %>%
     array_reshape(dim = c(1, 224, 224, 3)) %>% 
     imagenet_preprocess_input()
```

Now run the pretrained network on the image and decode its prediction vector back to a human-readable format:

```{r}
preds <- model %>% predict(img)
imagenet_decode_predictions(preds, top = 3)[[1]] 
```

The network has recognized the image as containing an undetermined quantity of African elephants. The entry in the prediction vector that was maximally activated is the one corresponding to the African elephant class, at index 387:

```{r}
which.max(preds[1,]) [1] 
```

To visualize which parts of the image are the most African elephant like, set up the `Grad-CAM` process.

```{r eval=FALSE}
#African elephant entry in the prediction vector
african_elephant_output <- model$output[, 387]

# Output feature map od block5_conv3 layer, the last convolution layer in VGG16
last_conv_layer <- model %>% get_layer("block5_conv3") 

#Gradient of "African Elephant" class
grads <- k_gradients(african_elephant_output, last_conv_layer$output)[[1]] 

#Vector of shape 512 where each entry is th emean intensity of the gradient over s specific feature map channel
pooled_grads <- k_mean(grads, axis = c(1, 2, 3))

#Access the vlaues of quantities just defined
iterate <- k_function(list(model$input), 
                      list(pooled_grads, last_conv_layer$output[1,,,]))

#values of the 2 quantities given the sample images of 2 elephants
c(pooled_grads_value, conv_layer_output_value) %<-% iterate(list(img))

#Multiplies each channel in the feature map array by how important each channel is  with regrad to elephant class
for (i in 1:512) {
     conv_layer_output_value[,,i] <- 
          conv_layer_output_value[,,i] * pooled_grads_value[[i]]}

#Channel-wise mean of the resulting feature map is the heatmap of the class activation
heatmap <- apply(conv_layer_output_value, c(1,2), mean)
```

For visualization purposes, normalize the heatmap between 0 and 1.

```{r eval=FALSE}
heatmap <- pmax(heatmap, 0)
heatmap <- heatmap/max(heatmap)

# Function to write a heatmap to PNG

write_heatmap <- function(heatmap, filename, width = 224, height = 224, 
                          bg = "white", col = terrain.colors(12)) {
     png(filename, width = width, height = height, bg = bg) 
     op = par(mar = c(0,0,0,0)) 
     on.exit({par(op); dev.off()}, add = TRUE) 
     rotate <- function(x) t(apply(x, 2, rev)) 
     image(rotate(heatmap), axes = FALSE, asp = 1, col = col)}

write_heatmap(heatmap, "elephant_heatmap.png")
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/elephant_heatmap.png")
```

Use `magick` package to generate an image that superimposes the original image with the heatmap above.

```{r elephantOverlay}
library(magick)
library(viridis)

image <- image_read(img_path)
info <- image_info(image)
	
geometry <- sprintf("%dx%d!", info$width, info$height)

pal <- col2rgb(viridis(20), alpha = TRUE)
alpha <- floor(seq(0, 255, length = ncol(pal))) 
pal_col <- rgb(t(pal), alpha = alpha, maxColorValue = 255) 
write_heatmap(heatmap, "elephant_overlay.png", width = 14, height = 14, 
              bg = NA, col = pal_col)
 
image_read("elephant_overlay.png") %>%
     image_resize(geometry, filter = "quadratic") %>% 
     image_composite(image, operator = "blend", compose_args = "20") %>% 
     plot()
```

This visualization technique answers two important questions:

- Why did the network think this image contained an African elephant? 
- Where is the African elephant located in the picture?

it is interesting to note that the ears of the elephant calf are strongly activated: this is probably how the network can tell the difference between African and Indian elephants. 

##	Summary

- Convnets are the best tool for attacking visual-classification problems.
- Convnets work by learning a hierarchy of modular patterns and concepts to represent the visual world.
- The representations they learn are easy to inspect convnets are the opposite of black boxes!
- You now capable of training your own convnet from scratch to solve an image-classification problem.
- You understand how to use visual data augmentation to fight overfitting.
- You know how to use a pretrained convnet to do feature extraction and fine-tuning.
- You can generate visualizations of the filters learned by your convnets, as well as heatmaps of class activity.

# Deep Learning for Text & Sequences
```{r echo=FALSE, message=FALSE}
rm(list = ls())

if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "keras", "gt", "here", prompt = TRUE)
```
```{r echo=FALSE}
#setwd("~/DeepLearning/Deep_Learning_with_R")
```

Now explore deep-learning models that can process text (understood as sequences of words or sequences of characters), timeseries, and sequence data in general. The two fundamental deep-learning algorithms for sequence processing are recurrent neural networks and 1D convnets, the one-dimensional version of the 2D convnets covered previously. 
 
Applications of these algorithms include the following:

- Document classification and timeseries classification, such as identifying the topic of an article or the author of a book
- Timeseries comparisons, such as estimating how closely related two documents or two stock tickers are
- Sequence-to-sequence learning, such as decoding an English sentence into French
- Sentiment analysis, such as classifying the sentiment of tweets or movie reviews as positive or negative
- Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data

Examples focus on two narrow tasks: sentiment analysis on the IMDB dataset, and temperature forecasting.  The techniques demonstrated for these two tasks are relevant to all the applications just listed, and many more.

## Text Data

Text is one of the most widespread forms of sequence data. It can be understood as either a sequence of characters or a sequence of words, but it is most common to work at the level of words. The deep-learning sequence-processing models can use text to produce a basic form of natural-language understanding, sufficient for applications including document classification, sentiment analysis, author identification, and even question answering (QA) in a constrained context. None of these deep-learning models truly understand text in a human sense; rather, these models can map the statistical structure of written language, which is sufficient to solve many simple textual tasks. Deep learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels.

Like all other neural networks, deep-learning models do not take as input raw text: they only work with numeric tensors. Vectorizing text is the process of transforming text into numeric tensors. This can be done in multiple ways:

- Segment text into words and transform each word into a vector.
- Segment text into characters and transform each character into a vector.
- Extract n-grams of words or characters and transform each n-gram into a vector.
 
Collectively, the different units into which you can break down text (words, characters, or n-grams) are called tokens and breaking text into such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens. These vectors, packed into sequence tensors, are fed into deep neural networks. There are multiple ways to associate a vector with a token: one-hot encoding of tokens and token embedding (typically used exclusively for words called _word embedding_). 

----

**Understanding n-grams and bag-of-words**

Word n-grams are groups of N (or fewer) consecutive words that you can extract from a sentence. The same concept may also be applied to characters instead of words.

Here is a simple example. Consider the sentence _The cat sat on the mat_. It may be decomposed into the following set of 2-grams:

{"The", "The cat", "cat", "cat sat", "sat",
"sat on", "on", "on the", "the", "the mat", "mat"}

It may also be decomposed into the following set of 3-grams:

{"The", "The cat", "cat", "cat sat", "The cat sat",
"sat", "sat on", "on", "cat sat on", "on the", "the",
"sat on the", "the mat", "mat", "on the mat"}

Such a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term bag here refers to the fact that you are dealing with a set of tokens rather than a list or sequence: the tokens have no specific order. This family of tokenization methods is called bag-of-words.

Because bag-of-words is not an order-preserving tokenization method (the tokens generated are understood as a set, not a sequence, and the general structure of the sentences is lost), _it tends to be used in shallow language-processing models rather than in deep-learning models_.

_Extracting n-grams is a form of feature engineering and deep learning does away with this kind of rigid, brittle approach, replacing it with hierarchical feature learning_. One-dimensional convnets and recurrent neural networks are capable of learning representations for groups of words and characters without being explicitly told about the existence of such groups, by looking at continuous word or character sequences. For this reason, _n-grams are not covered herein. Keep in mind that they are a powerful, unavoidable feature-engineering tool when using lightweight, shallow, text-processing models such as logistic regression and random forests_.

----

### One-hot Encoding

One-hot encoding is the most common, most basic way to turn a token into a vector. It consists of associating a unique integer index with every word and then turning this integer index `i` into a binary vector of size N (the size of the vocabulary); the vector is all zeros except for the `i` th entry, which is 1.

Keras has built-in utilities for doing one-hot encoding of text at the word level or character level, starting from raw text data. You should use these utilities because they perform a number of important features such as stripping special characters from strings and only taking into account the N most common words in your dataset (a common restriction, to avoid dealing with very large input vector spaces).

```{r}
samples <- c("The cat sat on the mat.", "The dog ate my homework.")

tokenizer <- text_tokenizer(num_words = 1000) %>% #Creates tokenizer for 1000 most common words
     fit_text_tokenizer(samples) #Builds the word index

sequences <- texts_to_sequences(tokenizer, samples) 
# Strings into lists of integer indicies

one_hot_results <- texts_to_matrix(tokenizer, samples, mode = "binary") 
#Could also get directly get one hot binary represemntations
#Vectorization models other than one hot are supported

#FYI - subset matrix 
#one_hot_results[one_hot_results[,] ==1]

word_index <- tokenizer$word_index 
# How you can recover the word from the indicies

cat("Found", length(word_index), "unique tokens.\n")
```

A variant of one-hot encoding is the `one-hot hashing trick`, which you use when the number of unique tokens in your vocabulary is too large to handle explicitly. Instead of explicitly assigning an index to each word and keeping a reference of these indices in a dictionary, you can hash words into vectors of fixed size. This is typically done with a very lightweight hashing function. *The main advantage of this method is that it does away with maintaining an explicit word index, which saves memory and allows online encoding of the data* (you can generate token vectors right away, before you have seen all of the available data). The one drawback of this approach is that *it is susceptible to hash collisions*: two different words may end up with the same hash, and subsequently any machine-learning model looking at these hashes will not be able to tell the difference between these words. The likelihood of hash collisions decreases when the dimensionality of the hashing space is much larger than the total number of unique tokens being hashed.

```{r}
library(hashFunction)

samples <- c("The cat sat on the mat.", "The dog ate my homework.")

dimensionality <- 1000
#Stores the words as vectors of size 1,000. If you have close to 1,000 words (or more), you will see many hash collisions, which will decrease the accuracy of this encoding method.

max_length <- 10 

results <- array(0, dim = c(length(samples), max_length, dimensionality))

for (i in 1:length(samples)) {
     sample <- samples[[i]] 
     words <- head(strsplit(sample, " ")[[1]], n = max_length) 
     for (j in 1:length(words)) { 
          index <- abs(spooky.32(words[[i]])) %% dimensionality 
          #hashes word into a random integer index between 0 and 1000
          results[[i, j, index]] <- 1}
	}
```

### Using word embeddings

Another popular and powerful way to associate a vector with a word is the use of dense word vectors, also called `word embeddings`. Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary), word embeddings are low dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors). Unlike the word vectors obtained via one-hot encoding, word embeddings are learned from data. it is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional, when dealing with very large vocabularies. One-hot encoding words generally leads to vectors that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information into far fewer dimensions.

There are two ways to obtain word embeddings:

- Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.
- Load into your model word embeddings that were precomputed using a different machine-learning task than the one you are trying to solve. These are called `pretrained word embeddings`.

The simplest way to associate a dense vector with a word is to choose the vector at random. The problem with this approach is that the resulting embedding space has no structure: for instance, the words `accurate` and `exact` may end up with completely different embeddings, even though they are interchangeable in most sentences. it is difficult for a deep neural network to make sense of such a noisy, unstructured embedding space.

To get a bit more abstract, the geometric relationships between word vectors should reflect the semantic relationships between these words. *Word embeddings are meant to map human language into a geometric space*. You would expect synonyms to be embedded into similar word vectors; and in general, you would expect the geometric distance (such as L2 distance) between any two word vectors to relate to the semantic distance between the associated words (words meaning different things are embedded at points far away from each other, whereas related words are closer). In addition to distance, you may want specific directions in the embedding space to be meaningful. To make this clearer, consider a example.

Imagine four words are embedded on a 2D plane: cat, dog, wolf, and tiger. With the vector representations we chose here, some semantic relationships between these words can be encoded as geometric transformations. For instance, the same vector allows us to go from cat to tiger and from dog  to wolf: this vector could be interpreted as a from pet to wild animal vector. Similarly, another vector lets us go from dog to cat and from wolf to tiger, which could be interpreted as a from canine to feline vector.

In real-world word-embedding spaces, common examples of meaningful geometric transformations are _gender_ vectors and _plural_ vectors. For instance, by adding a _female_ vector to the vector _king_, we obtain the vector _queen._ By adding a _plural_ vector, we obtain _kings._ Word-embedding spaces typically feature thousands of such interpretable and potentially useful vectors.

What makes a good word-embedding space depends heavily on your task: the perfect word-embedding space for an English-language movie-review sentiment analysis model may look different from the perfect embedding space for an English language legal-document-classification model because the importance of certain semantic relationships varies from task to task.  it is reasonable then to learn a new embedding space with every new task. Fortunately, backpropagation makes this easy, and Keras makes it even easier. it is about learning the weights of a layer using `layer_embedding.`

```{r eval=FALSE}
embedding_layer <- layer_embedding(input_dim = 1000, output_dim = 64)
```

The embedding layer takes at least two arguments: 

- the number of possible tokens (here, 1,000) 
- the dimensionality of the embeddings (here, 64)

`layer_embedding` is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors. it is effectively a dictionary lookup.

An embedding layer takes as input a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. It can embed sequences of variable lengths: for instance, you could feed into the embedding layer batches with shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15). _All sequences in a batch must have the same length, though (because you need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated_.

This layer returns a 3D floating-point tensor of shape (samples, sequence_ length, embedding_dimensionality). Such a 3D tensor can then be processed by an RNN layer or a 1D convolution layer.

When you instantiate an embedding layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure a kind of structure specialized for the specific problem for which you are training your model.

Apply this idea to the IMDB movie-review sentiment-prediction task that you are already familiar with. 

- prepare the data
- restrict the movie reviews to the top 10,000 most common words 
- cut off the reviews after 20 words
- the network will learn 8-dimensional embeddings for each of the 10,000 words
- turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor)
- flatten the tensor to 2D
- train a single dense layer on top for classification

```{r eval=FALSE}
rm(list=ls())
max_features <- 10000
maxlen <- 20

imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
# loads data as lists of integers

x_train <- pad_sequences(x_train, maxlen = maxlen)
# Turns the lists of integers into a 2D integer tensor of shape(samples, maxlen)

x_test <- pad_sequences(x_test, maxlen = maxlen)
####

model_Embedding <- keras_model_sequential() %>%
     layer_embedding(input_dim = 10000, output_dim = 8, input_length = maxlen) %>%
     layer_flatten() %>% 
     layer_dense(units = 1, activation = "sigmoid")

model_Embedding %>% compile(
     optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc")) 

#summary(model_Embedding)
```
```{r eval=FALSE}
history_Embedding <- model_Embedding %>% fit(x_train, y_train, epochs = 10, batch_size = 32, 
                                             validation_split = 0.2 )
plot(history_Embedding)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-1-2-history_embedding.jpg")
```

You get to a validation accuracy of ~76%, which is pretty good considering that you are only looking at 20 words from each review. But note that merely flattening the embedded sequences and training a single dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word relationships and sentence structure (for example, this model would likely treat both _this movie is a bomb_ and _this movie is the bomb_ as being negative reviews). it is much better to add recurrent layers or 1D convolutional layers on top of the embedded sequences *to learn features that take into account each sequence as a whole*.

#### Using Pretrained Word Embeddings

Sometimes, you have so little training data available that you cannot use your data alone to learn an appropriate task-specific embedding of your vocabulary.

Instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties  that captures generic aspects of language structure. The rationale behind using pretrained word embeddings in natural-language processing is much the same as for using pretrained convnets in image classification: you do not have enough data available to learn truly powerful features on your own, but you expect the features that you need to be fairly generic  that is, common visual features or semantic features. In this case, it makes sense to reuse features learned on a different problem.

Such word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of techniques, some involving neural networks, others not. 

There are various precomputed databases of word embeddings you can download and use in a Keras embedding layer. `Word2vec` is one of them. Another popular one is called `Global Vectors for Word Representation` (_GloVe_, https://nlp.stanford .edu/projects/glove), which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data.

Look at how you can get started using `GloVe` embeddings in a Keras model. (The same method is valid for `Word2vec` embeddings or any other word-embedding database.) 

### Raw Text to Word Embeddings

First, download the raw IMDB dataset from http://mng.bz/0tIo. Uncompress it.  Collect the individual training reviews into a list of strings, one string per review. Also collect the review labels (positive/negative) into a labels list. 

I like the code below!
```{r eval=FALSE}
rm(list=ls())
imdb_dir <- "../../LargeDataFiles/IMDB" 
train_dir <- file.path(imdb_dir, "train")
labels <- c() 
texts <- c()
for (label_type in c("neg", "pos")) {
     label <- switch(label_type, neg = 0, pos = 1) 
     dir_name <- file.path(train_dir, label_type) 
     for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), full.names = TRUE)) {
          texts <- c(texts, readChar(fname, file.info(fname)$size)) 
          labels <- c(labels, label)}
}
```

#### Tokenize the Data

Vectorize the text and prepare a training and validation split. Because pretrained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, task-specific embeddings are likely to outperform them), add the following twist: restricting the training data to the first 200 samples. So you will learn to classify movie reviews after looking at just 200 examples.

```{r eval=FALSE}
maxlen <- 100 
training_samples <- 200 
validation_samples <- 10000 
max_words <- 10000

tokenizer <- text_tokenizer(num_words = max_words) %>% fit_text_tokenizer(texts)
 
sequences <- texts_to_sequences(tokenizer, texts)

word_index = tokenizer$word_index 
cat("Found", length(word_index), "unique tokens.\n") 

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(labels) 
cat("Shape of data tensor:", dim(data), "\n") 
cat('Shape of label tensor:', dim(labels), "\n")

indices <- sample(1:nrow(data)) #shuffle data- samples are ordered (neg come first in raw data)
training_indices <- indices[1:training_samples] 
validation_indices <- indices[(training_samples + 1):
                                   (training_samples + validation_samples)]

x_train <- data[training_indices,] 
y_train <- labels[training_indices]

x_val <- data[validation_indices,] 
y_val <- labels[validation_indices]
```

`Found 88584 unique tokens.`
`Shape of data tensor: 25000 100` 
`Shape of label tensor: 25000 `

#### Download Glove Word Embeddings

Go to https://nlp.stanford.edu/projects/glove, and download the precomputed embeddings from 2014 English Wikipedia. it is an 822 MB zip file called glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or nonword tokens). Unzip it. 

#### Preprocess Embeddings

Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors).

```{r gloceEmbedding, eval=FALSE}
rm(list = ls())
glove_dir = "../../LargeDataFiles/Glove" 
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt")) 
embeddings_index <- new.env(hash = TRUE, parent = emptyenv()) 

for (i in 1:length(lines)) {
     line <- lines[[i]] 
     values <- strsplit(line, " ")[[1]] 
     word <- values[[1]] 
     embeddings_index[[word]] <- as.double(values[-1])} 

cat("Found", length(embeddings_index), "word vectors.\n")
```

`Found 400000 word vectors.`

Build an embedding matrix that you can load into an embedding layer. It must be a matrix of shape (max_words, embedding_dim), where each entry `i` contains the `embedding_dim-dimensional` vector for the word of index `i` in the reference word index (built during tokenization). Note that index 1 is a placeholder.
 
```{r eval=FALSE}
embedding_dim <- 100 
embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
     index <- word_index[[word]] 
     if (index < max_words) {
          embedding_vector <- embeddings_index[[word]]
          if (!is.null(embedding_vector))
               embedding_matrix[index+1,] <- embedding_vector}
}
```

#### Define Model

```{r eval=FALSE}
model_Glove <- keras_model_sequential() %>%
     layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                     input_length = maxlen) %>%
     layer_flatten() %>% layer_dense(units = 32, activation = "relu") %>% 
     layer_dense(units = 1, activation = "sigmoid") 
summary(model_Glove)
```

```{}
Layer (type)                            Output Shape                       Param #       
=========================================================================================
embedding_3 (Embedding)                 (None, 100, 100)                   1000000       
_________________________________________________________________________________________
flatten_2 (Flatten)                     (None, 10000)                      0             
_________________________________________________________________________________________
dense_2 (Dense)                         (None, 32)                         320032        
_________________________________________________________________________________________
dense_3 (Dense)                         (None, 1)                          33            
=========================================================================================
Total params: 1,320,065
Trainable params: 320,065
Non-trainable params: 1,000,000
_________________________________________________________________________________________
```

#### Load Glove in Model

The embedding layer has a single weight matrix: a 2D float matrix where each entry `i` is the word vector meant to be associated with index `i`. Load the GloVe matrix you prepared into the embedding layer, the first layer in the model.

```{r eval=FALSE}
get_layer(model_Glove, index = 1) %>%
     set_weights(list(embedding_matrix)) %>% freeze_weights()
```

Additionally, freeze the weights of the embedding layer, following the same rationale you are already familiar with in the context of pretrained convnet features: when parts of a model are pretrained (like your embedding layer) and parts are randomly initialized (like your classifier), the pretrained parts should not be updated during training, to avoid forgetting what they already know. The large gradient updates triggered by the randomly initialized layers would be disruptive to the already-learned features. 

#### Train & Evaluate

```{r eval=FALSE}
model_Glove %>% compile(
     optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))

history_Glove <- model_Glove %>% fit(
     x_train, 
     y_train, 
     epochs = 20, 
     batch_size = 32, 
     validation_data = list(x_val, y_val)) 

save_model_weights_hdf5(model_Glove, "./models/pre_trained_glove_model.h5") 
plot(history_Glove)
```
```{r echo=FALSE}
load_model_weights_hdf5(model_Glove, "./models/pre_trained_glove_model.h5")
plot(history_Glove)
```

The model quickly starts overfitting, which is  not surprising given the small number of training samples. Validation accuracy has high variance for the same reason, but it seems to reach the high 50s.

Note that your mileage may vary: because you have so few training samples, performance is heavily dependent on exactly which 200 samples you choose and you are choosing them at random. 

If this works poorly for you, try choosing a different random set of 200 samples, for the sake of the exercise (in real life, you do not get to choose your training data).

You can also train the same model without loading the pretrained word embeddings and without freezing the embedding layer. In that case, you will learn a task-specific embedding of the input tokens, which is generally more powerful than pretrained word embeddings when lots of data is available. But in this case, you have only 200 training samples. 

```{r eval=FALSE}
model_NoEmbed <- keras_model_sequential() %>%
     layer_embedding(input_dim = max_words, 
                     output_dim = embedding_dim, input_length = maxlen) %>%
     layer_flatten() %>% 
     layer_dense(units = 32, activation = "relu") %>% 
     layer_dense(units = 1, activation = "sigmoid") 

model_NoEmbed %>% compile(
     optimizer = "rmsprop", 
     loss = "binary_crossentropy", 
     metrics = c("acc"))

history_NoEmbed <- model_NoEmbed %>% fit(
     x_train, 
     y_train, 
     epochs = 20, 
     batch_size = 32,
     validation_data = list(x_val, y_val))

plot(history_NoEmbed)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-1-3-historyNoEmbed.jpg")
```

Validation accuracy stalls in the mid 50s. So in this case, pretrained word embeddings outperform jointly learned embeddings. If you increase the number of training samples, this will quickly stop being the case.

Finally, evaluate the model on the test data. First, tokenize the test data.
 
```{r eval=FALSE}
test_dir <- file.path(imdb_dir, "test")
labels <- c() 
texts <- c()

for (label_type in c("neg", "pos")) {
     label <- switch(label_type, neg = 0, pos = 1) 
     dir_name <- file.path(test_dir, label_type) 
     for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), full.names = TRUE)) {
          texts <- c(texts, readChar(fname, file.info(fname)$size)) 
          labels <- c(labels, label)} 
}

sequences <- texts_to_sequences(tokenizer, texts) 
x_test <- pad_sequences(sequences, maxlen = maxlen) 
y_test <- as.array(labels)

model_NoEmbed %>% 
     load_model_weights_hdf5("./models/pre_trained_glove_model.h5") %>% evaluate(x_test, y_test)
```

`25000/25000 [==============================] - 3s 105us/step`
`$loss`
`[1] 1.424911`

`$acc`
`[1] 0.57176`

You get an poor test accuracy of 58%. Working with just a handful of training samples is difficult!

### Wrapping up

Now you are able to do the following:

- Turn raw text into something a neural network can process
- Use an embedding layer in a Keras model to learn task-specific token embeddings
- Use pretrained word embeddings to get an extra boost on small natural language-processing problems 

## Understanding RNN

A characteristic of all neural networks such as densely connected networks and convnets, is that they have no memory. Each input shown to them is processed independently, with no state kept in between inputs. With such networks, in order to process a sequence or a temporal series of data points, you have to show the entire sequence to the network at once: turn it into a single data point. For instance, this is what you did in the IMDB example: an entire movie review was transformed into a single large vector and processed in one go. Such networks are called `feed forward networks`.

In contrast, as you are reading the present sentence, you are processing it word by word while keeping memories of what came before; this gives you a fluid representation of the meaning conveyed by this sentence. Biological intelligence processes information incrementally while maintaining an internal model of what it is processing, built from past information and constantly updated as new information comes in.

A recurrent neural network (RNN) adopts the same principle, albeit in an extremely simplified version: it processes sequences by iterating through the sequence elements and maintaining a state containing information relative to what it has seen so far. In effect, an RNN is a type of neural network that has an internal loop. The state of the RNN is reset between processing two different, independent sequences (such as two different IMDB reviews), so you still consider one sequence a single data point: a single input to the network. What changes is that this data point is no longer processed in a single step; rather, the network internally loops over sequence elements.

An RNN is a for loop that reuses quantities computed during the previous iteration of the loop, nothing more. Of course, there are many different RNNs fitting this definition that you could build. RNNs are characterized by their step function.

### Recurrent layer in Keras

Like all recurrent layers in Keras, `layer_simple_rnn` can be run in two different modes: 

- it can return either the full sequences of successive outputs for each timestep (a
3D tensor of shape (batch_size, timesteps, output_features)) 
- only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features))

These two modes are controlled by the `return_sequences` constructor argument. Look at an example that uses `layer_simple_rnn` and returns only the output at the last timestep:

```{r}
modelRNN <- keras_model_sequential() %>%
     layer_embedding(input_dim = 10000, output_dim = 32) %>% 
     layer_simple_rnn(units = 32)
summary(modelRNN)
```

The following example returns the full state sequence:

```{r}
modelRRN2 <- keras_model_sequential() %>%
     layer_embedding(input_dim = 10000, output_dim = 32) %>% 
     layer_simple_rnn(units = 32, return_sequences = TRUE)
summary(modelRRN2)
```

it is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. In such a setup, _you have to get all of the intermediate layers to return full sequences_:

```{r}
modelRNN3 <- keras_model_sequential() %>%
     layer_embedding(input_dim = 10000, output_dim = 32) %>% 
     layer_simple_rnn(units = 32, return_sequences = TRUE) %>% 
     layer_simple_rnn(units = 32, return_sequences = TRUE) %>% 
     layer_simple_rnn(units = 32, return_sequences = TRUE) %>% 
     layer_simple_rnn(units = 32)
 
summary(modelRNN3)
```

Use such a model on the IMDB movie-review-classification problem. First, preprocess the data.

```{r eval=FALSE}
rm(list = ls())
max_features <- 10000
maxlen <- 500
batch_size <- 32

cat("Loading data...\n") 
imdb <- dataset_imdb(num_words = max_features) 
c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb 

cat(length(input_train), "train sequences\n") 
cat(length(input_test), "test sequences")
cat("Pad sequences (samples x time)\n") 

input_train <- pad_sequences(input_train, maxlen = maxlen) 
input_test <- pad_sequences(input_test, maxlen = maxlen) 
cat("input_train shape:", dim(input_train), "\n") 
cat("input_test shape:", dim(input_test), "\n")

modelRNN4 <- keras_model_sequential() %>%
     layer_embedding(input_dim = max_features, output_dim = 32) %>% 
     layer_simple_rnn(units = 32) %>% 
     layer_dense(units = 1, activation = "sigmoid")

modelRNN4 %>% compile(
     optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))

historyRNN4 <- modelRNN4 %>% fit(input_train, y_train, epochs = 10, batch_size = 128, 
                                 validation_split = 0.2)

plot(historyRNN4)
``` 

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-2-1-historyRNN4.jpg")
```

As a reminder, in chapter 3, the first naive approach to this dataset got you to a test accuracy of 88%. Unfortunately, this small recurrent network does not perform well compared to this baseline. Part of the problem is that your inputs only consider the first 500 words, rather than full sequences hence, the RNN has access to less information than the earlier baseline model. The remainder of the problem is that `layer_simple_rnn` is not good at processing long sequences, such as text. Other types of recurrent layers perform much better. Look at some more-advanced layers. 

### LSTM and GRU Layers

Simple RNNs are not the only recurrent layers available in Keras. There are two others:

- layer_lstm 
- layer_gru

In practice, you will always use one of these, because `layer_simple_rnn` is generally too simplistic to be of real use. One major issue with `layer_simple_rnn` is that although it should theoretically be able to retain at time `t` information about inputs seen many timesteps before, in practice, such long-term dependencies are impossible to learn. This is due to the vanishing gradient problem, an effect that is similar to what is observed with non-recurrent networks (feedforward networks) that are many layers deep: as you keep adding layers to a network, the network eventually becomes untrainable. The LSTM and GRU layers are designed to solve this problem.

The Long Short-Term Memory (LSTM) is a variant of `layer_simple_rnn`; it adds a way to carry information across many timesteps. Imagine a conveyor belt running parallel to the sequence you are processing. Information from the sequence can jump onto the conveyor belt at any point, be transported to a later timestep, and jump off, intact, when you need it. This is essentially what LSTM does: it saves information for later, thus preventing older signals from gradually vanishing during processing.

### LSTM example in Keras

Set up a model using `layer_lstm` and train it on the IMDB data. The network is similar to the one with `layer_simple_rnn` that was just presented. You only specify the output dimensionality of `layer_lstm`; leave every other argument (there are many) at the Keras defaults. Keras has good defaults, and things will almost always just work without you having to spend time tuning parameters by hand.

```{r LSTM1, eval=FALSE}
model_LSTM1 <- keras_model_sequential() %>%
     layer_embedding(input_dim = max_features, output_dim = 32) %>% 
     layer_lstm(units = 32) %>%
     layer_dense(units = 1, activation = "sigmoid")

model_LSTM1 %>% compile(
     optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))

history_LSTM1 <- model_LSTM1 %>% fit(
     input_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2)
plot(history_LSTM1)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-2-3-LSTM1.jpg")
```

This time, you achieve up to 88% validation accuracy. Not bad: certainly much better than the simple RNN network That is largely because LSTM suffers much less from the vanishing-gradient problem and slightly better than the fully connected approach earlier, even though you are looking at less data than you were then.

you are truncating sequences after 500 timesteps, whereas before you were considering full sequences.

But this result is not groundbreaking for such a computationally intensive approach. Why is not LSTM performing better? One reason is that you made no effort to tune hyperparameters such as the embeddings dimensionality or the LSTM output dimensionality. Another may be lack of regularization. But honestly, the primary reason is that analyzing the global, long-term structure of the reviews (what LSTM is good at) is not helpful for a sentiment-analysis problem. Such a basic problem is well solved by looking at what words occur in each review, and at what frequency. That is what the first fully connected approach looked at. But there are far more difficult natural language-processing problems out there, where the strength of LSTM will become apparent: in particular, question answering and machine translation.

### Wrapping up LSTM

Now you understand the following:
- What RNNs are and how they work
- What LSTM is, and why it works better on long sequences than a naive RNN
- How to use Keras RNN layers to process sequence data

##	Advanced RNNs

```{r echo=FALSE}
rm(list = ls())
```

Review three advanced techniques for improving the performance and generalization power of recurrent neural networks. Demonstrate all three concepts on a temperature-forecasting problem, where you have access to a timeseries of data points coming from sensors installed on the roof of a building, such as temperature, air pressure, and humidity, which you use to predict what the temperature will be 24 hours after the last data point. This is a fairly challenging problem that exemplifies many common difficulties encountered when working with timeseries.

Cover the following techniques:

- `Recurrent dropout`  This is a specific, built-in way to use dropout to fight overfitting in recurrent layers.
- `Stacking recurrent layers`  This increases the representational power of the network (at the cost of higher computational loads).
- `Bidirectional recurrent layers`  These present the same information to a recurrent network in different ways, increasing accuracy and mitigating forgetting issues.
  
###	Temperature Forecasting Problem

14 different quantities (such air temperature, atmospheric pressure, humidity, wind direction, and so on) were recorded every 10 minutes, over several years. The original data goes back to 2003, but this example is limited to data from 2009 - 2016. This dataset is perfect for learning to work with numerical timeseries. Use it to build a model that takes as input some data from the recent past (a few days worth of data points) and predicts the air temperature 24 hours in the future.

Download and uncompress the [data](https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip):

```{r}
fname <- file.path("../../LargeDataFiles/jena_climate", "jena_climate_2009_2016.csv") 
data <- read_csv(fname)
glimpse(data)
```
```{r eval=FALSE}
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-3-1-timeseries1.jpg")
```

Look at a more narrow plot of the first 10 days of temperature data. Because the data is recorded every 10 minutes, you get 144 (6 x 24) data points per day.

```{r eval=FALSE}
ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-3-1-timeseries2.jpg")
```


On this plot, you can see daily periodicity, especially evident for the last 4 days. Also note that this 10-day period must be coming from a fairly cold winter month.

If you were trying to predict average temperature for the next month given a few months of past data, the problem would be easy, due to the reliable year-scale periodicity of the data. But looking at the data over a scale of days, the temperature looks a lot more chaotic. Is this timeseries predictable at a daily scale? 

### Preparing the data

The exact formulation of the problem will be as follows: 

_Given data going as far back as `lookback` timesteps (a timestep is 10 minutes) and sampled every `steps` timesteps, can you predict the temperature in `delay` timesteps? _

Use the following parameter values:

- `lookback` = 1440  Observations will go back 10 days
- `steps` = 6  Observations will be sampled at one data point per hour
- `delay` = 144  Targets will be 24 (6 x 24) hours in the future

To get started, do two things:

- Preprocess the data to a format a neural network can ingest. This is easy: the data is already numerical, so you do not need to do any vectorization. But each timeseries in the data is on a different scale (for example, temperature is typically between -20 and +30, but atmospheric pressure, measured in mbar, is around 1,000). Normalize each timeseries independently so that they all take small values on a similar scale.
- Write a generator function that takes the current array of float data and yields batches of data from the recent past, along with a target temperature in the future. Because the samples in the dataset are highly redundant (sample N and sample N + 1 will have most of their timesteps in common), it would be wasteful to explicitly allocate every sample. Instead, generate the samples on the fly using the original data.

----

__Understanding Generator Functions__

A generator function is a special type of function you call repeatedly to obtain a sequence of values. Often, generators need to maintain internal state, so they are typically constructed by calling another function that returns the generator function (the environment of the function that returns the generator is then used to track state).

For example, the following `sequence_generator()` function returns a generator function that yields an infinite sequence of numbers:

```{r eval=FALSE}
sequence_generator <- function(start) {
     value <- start - 1 
     function() {
          value <<- value + 1
          value}
}

gen <- sequence_generator(10)
gen()
```

[1] 10

```{r eval=FALSE}
gen()
```

[1] 11

The current state of the generator is the value variable That is defined outside of the function. Note that superassignment (`<<-`) is used to update this state from within the function.

Generator functions can signal completion by returning the value NULL. But generator functions passed to Keras training methods (such as `fit_generator()`) should always return values infinitely (the number of calls to the generator function is controlled by the epochs and steps_per_epoch parameters).

-----------------

First, convert the R data frame read earlier into a matrix of floating point values (discarding the first column, which included a text timestamp).

> `data.matrix` - Return the matrix obtained by converting all the variables in a data frame to numeric mode and then binding them together as the columns of a matrix. Factors and ordered factors are replaced by their internal codes.

```{r}
names(data)
data <- data.matrix(data[,-1])
head(data)
```

Preprocess the data by subtracting the mean of each timeseries and dividing by the standard deviation. Use the first 200,000 timesteps as training data, so compute the mean and standard deviation for normalization only on this fraction of the data.
 
```{r}
train_data <- data[1:200000,] 
mean <- apply(train_data, 2, mean) 
std <- apply(train_data, 2, sd) 
data <- scale(data, center = mean, scale = std)
```

Below is the data generator to use. It yields a list `(samples, targets)`, where `samples` is one batch of input data and `targets` is the corresponding array of target temperatures. It takes the following arguments:

- `data`  The original array of floating-point data, which was normalized above
- `lookback`  How many timesteps back the input data should go. 
     -  `delay`  How many timesteps in the future the target should be
- `min_index` and `max_ index`  Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another for testing
- `shuffle`  Whether to shuffle the samples or draw them in chronological order 
     - `batch_size`  The number of samples per batch
- `step`  The period, in timesteps, at which you sample data. you will set it to 6 in order to draw one data point every hour.

```{r generator_timeSeries}
generator <- function(
     data, lookback, delay, min_index, max_index, shuffle = FALSE, 
     batch_size = 128, step = 6) {
     
     if (is.null(max_index)) max_index <- nrow(data) - delay - 1 
     i <- min_index + lookback 
     function() {
          if (shuffle) {
               rows <- sample(c((min_index+lookback):max_index), size = batch_size)
          } else { 
               if (i + batch_size >= max_index)
                    i <<- min_index + lookback
               rows <- c(i:min(i + batch_size, max_index)) 
               i <<- i + length(rows)
          }
          samples <- array(0, dim = c(length(rows),
                                      lookback / step, 
                                      dim(data)[[-1]]))
          targets <- array(0, dim = c(length(rows)))
          
          for (j in 1:length(rows)) {
               indices <- seq(rows[[j]] - lookback, rows[[j]],
                              length.out = dim(samples)[[2]])
               samples[j,,] <- data[indices,] 
               targets[[j]] <- data[rows[[j]] + delay,2]
          }
          list(samples, targets)}
     }
```

The `i` variable contains the state that tracks the next window of data to return, so it is updated using superassignment (`i <<- i + length(rows)`).

Use the abstract generator function to instantiate three generators:

- one for training
- one for validation
- one for testing

Each will look at different temporal segments of the original data: the training generator looks at the first 200,000 timesteps, the validation generator looks at the following 100,000, and the test generator looks at the remainder.

```{r}
lookback <- 1440 
step <- 6 
delay <- 144 
batch_size <- 128

train_gen <- generator(data, lookback = lookback, delay = delay, min_index = 1, 
                       max_index = 200000, shuffle = TRUE, step = step, 
                       batch_size = batch_size)

val_gen = generator(data, lookback = lookback, delay = delay, min_index = 200001, 
                    max_index = 300000, step = step, batch_size = batch_size)

test_gen <- generator(data, lookback = lookback, delay = delay, min_index = 300001,
                      max_index = NULL, step = step, batch_size = batch_size) 

val_steps <- (300000 - 200001 - lookback) / batch_size
#How many steps to draw from valt_gen in order to see the validation test set

test_steps <- (nrow(data) - 300001 - lookback) / batch_size
#How many steps to draw from test_gen in order to see the entire test set
```

###Common-sense Baseline

Before you start using black-box deep-learning models to solve the temperature prediction problem, try a simple, common-sense approach. It will serve as a sanity check, and it will establish a baseline that you will have to beat in order to demonstrate the usefulness of more-advanced machine-learning models. Such common-sense baselines can be useful when you are approaching a new problem for which there is no known solution (yet). A classic example is that of unbalanced classification tasks, where some classes are much more common than others. If your dataset contains 90% instances of class A and 10% instances of class B, then a common-sense approach to the classification task is to always predict when presented with a new sample. Such a classifier is 90% accurate overall, and any learning-based approach should therefore beat this 90% score in order to demonstrate usefulness. Sometimes, such elementary baselines can prove surprisingly hard to beat.

In this case, the temperature timeseries can safely be assumed to be continuous (the temperatures tomorrow are likely to be close to the temperatures today) as well as periodical with a daily period. Thus a common-sense approach is to always predict that the temperature 24 hours from now will be equal to the temperature right now.

Evaluate this approach, using the mean absolute error (MAE) metric: `mean(abs(preds - targets))`. here is the evaluation loop.

```{r}
evaluate_naive_method <- function() {
     batch_maes <- c() 
     for (step in 1:val_steps) {
          c(samples, targets) %<-% val_gen() 
          preds <- samples[,dim(samples)[[2]],2] 
          mae <- mean(abs(preds - targets)) 
          batch_maes <- c(batch_maes, mae)}
     
     print(mean(batch_maes))
     } 
evaluate_naive_method()
```

This yields an MAE of 0.28. Because the temperature data has been normalized to be centered on 0 and have a standard deviation of 1, this number is not immediately interpretable. It translates to an average absolute error of 0.29 x `temperature_std` degrees Celsius: 2.57ËC.

```{r}
celsius_mae <- 0.28 * std[[2]]
celsius_mae
```

That is a fairly large average absolute error. Now the game is to use your knowledge of deep learning to do better. 

### Basic ML Approach

In the same way that it is useful to establish a common-sense baseline before trying machine-learning approaches, it is useful to try simple, cheap machine-learning models (such as small, densely connected networks) before looking into complicated and computationally expensive models such as RNNs. This is the best way to make sure any further complexity you throw at the problem is legitimate and delivers real benefits.

The following listing shows a fully connected model that starts by flattening the data and then runs it through two dense layers. _Note the lack of activation function on the last dense layer, which is typical for a regression problem_. You use MAE as the loss. Because you evaluate on the exact same data and with the exact same metric you did with the common-sense approach, the results will be directly comparable.

```{r eval=FALSE}
library(keras)
model <- keras_model_sequential() %>%
     layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
     layer_dense(units = 32, activation = "relu") %>% 
     layer_dense(units = 1)

model %>% compile(optimizer = optimizer_rmsprop(), loss = "mae")

history <- model %>% fit_generator(
     train_gen, steps_per_epoch = 500, epochs = 20, 
     validation_data = val_gen, validation_steps = val_steps)
plot(history)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-3-4-Basic_ML_Timeseries1.jpg")
```

Some of the validation losses are close to the no-learning baseline, but not reliably. This goes to show the merit of having this baseline in the first place: it turns out to be not easy to outperform. Your common sense contains a lot of valuable information that a machine-learning model does not have access to.

You may wonder, if a simple, well-performing model exists to go from the data to the targets (the common-sense baseline), why does not the model you are training find it and improve on it? Because this simple solution is not what your training setup is looking for. The space of models in which you are searching for a solution  that is, your hypothesis space  is the space of all possible two-layer networks with the configuration you defined. These networks are already fairly complicated. When you are looking for a solution with a space of complicated models, the simple, well-performing baseline may be unlearnable, even if it is technically part of the hypothesis space. _That is a pretty significant limitation of machine learning in general: unless the learning algorithm is hardcoded to look for a specific kind of simple model, parameter learning can sometimes fail to find a simple solution to a simple problem_. 
	
### A First Recurrent Baseline

The first fully connected approach did not do well, but that does not mean machine learning is not applicable to this problem. The previous approach first flattened the timeseries, which removed the notion of time from the input data. Instead look at the data as what it is: a sequence, where causality and order matter. Try a recurrent - sequence processing model  it should be the perfect fit for such sequence data, precisely because it exploits the temporal ordering of data points, unlike the first approach.

Instead of the LSTM layer introduced in the previous section, you will use the GRU layer, developed by Chung et al. in 2014.  `Gated recurrent unit (GRU)` layers work using the same principle as LSTM, but they are somewhat streamlined and thus cheaper to run (although they may not have as much representational power as LSTM). This trade-off between computational expensiveness and representational power is seen everywhere in machine learning.

```{r eval=FALSE}
model <- keras_model_sequential() %>%
     layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
     layer_dense(units = 1)

model %>% compile(optimizer = optimizer_rmsprop(), loss = "mae")

history <- model %>% fit_generator(
     train_gen, steps_per_epoch = 500, epochs = 20, 
     validation_data = val_gen, validation_steps = val_steps)

plot(history)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-3-5-GRU_Timeseries.jpg")
```

Much better! You can significantly beat the common-sense baseline, demonstrating the value of machine learning as well as the superiority of recurrent networks compared to sequence-flattening dense networks on this type of task.
 
The new validation MAE of ~0.265 (before you start significantly overfitting) translates to a mean absolute error of 2.35ËC after denormalization. That is a solid gain on the initial error of 2.57ËC, but you probably still have a bit of a margin for improvement. 

### Recurrent Dropout to Fight Overfitting

it is evident from the training and validation curves that the model is overfitting: the training and validation losses start to diverge considerably after a few epochs. you are already familiar with a classic technique for fighting this phenomenon: dropout, which randomly zeros out input units of a layer in order to break happenstance correlations in the training data that the layer is exposed to. But how to correctly apply dropout in recurrent networks is not a trivial question. It has long been known that applying dropout before a recurrent layer hinders learning rather than helping with regularization. In 2015, Yarin Gal, as part of his PhD thesis on Bayesian deep learning,  determined the proper way to use dropout with a recurrent network: the same dropout mask (the same pattern of dropped units) should be applied at every timestep, instead of a dropout mask that varies randomly from timestep to timestep.What is more, in order to regularize the representations formed by the recurrent gates of layers such as layer_gru and layer_lstm, a temporally constant dropout mask should be applied to the inner recurrent activations of the layer (a recurrent dropout mask). Using the same dropout mask at every timestep allows the network to properly propagate its learning error through time; a temporally random dropout mask would disrupt this error signal and be harmful to the learning process.
 
Yarin Gal did his research using Keras and helped build this mechanism directly into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related arguments: `dropout`, a float specifying the dropout rate for input units of the layer, and `recurrent_dropout`, specifying the dropout rate of the recurrent units. Add dropout and recurrent dropout to `layer_gru` and see how doing so impacts overfitting. Because networks being regularized with dropout always take longer to fully converge, train the network

```{r eval=FALSE}
model <- keras_model_sequential() %>%
     layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2, input_shape = list(NULL,
                                                            dim(data)[[-1]])) %>% 
     layer_dense(units = 1)

model %>% compile(optimizer = optimizer_rmsprop(), loss = "mae")

history <- model %>% fit_generator(train_gen,
steps_per_epoch = 500, epochs = 40, validation_data = val_gen, 
validation_steps = val_steps)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-3-5-Dropout_Timeseries.jpg")
```

Success! you are no longer overfitting during the first 20 epochs. But although you have more stable evaluation scores, your best scores are not much lower than they were previously. 

### Stacking Recurrent Layers

Because you are no longer overfitting but seem to have hit a performance bottleneck, you should consider increasing the capacity of the network. Recall the description of _the universal machine-learning workflow: it is generally a good idea to increase the capacity of your network until overfitting becomes the primary obstacle (assuming you have already taken basic steps to mitigate overfitting, such as using dropout). As long as you are not overfitting too badly, you are likely under capacity_.

Increasing network capacity is typically done by increasing the number of units in the layers or adding more layers. Recurrent layer stacking is a classic way to build more-powerful recurrent networks: for instance, what currently powers the Google Translate algorithm is a stack of seven large LSTM layers That is huge.
 
To stack recurrent layers on top of each other in Keras, all intermediate layers should return their full sequence of outputs (a 3D tensor) rather than their output at the last timestep. This is done by specifying `return_sequences = TRUE`.

Adding layer may improve the results a bit, though not significantly. You can draw two conclusions:

- Because you are still not overfitting too badly, you could safely increase the size of your layers in a quest for validation-loss improvement. This has a non-negligible computational cost, though.
-  Adding a layer will not help by a significant factor, so you may be seeing diminishing returns from increasing network capacity at this point.   
 
```{r eval=FALSE}
model <- keras_model_sequential() %>%
     layer_gru(units = 32, dropout = 0.1, recurrent_dropout = 0.5, 
               return_sequences = TRUE, input_shape = list(NULL, dim(data)[[-1]])) %>%
     layer_gru(units = 64, activation = "relu", dropout = 0.1, recurrent_dropout = 0.5) %>%
     layer_dense(units = 1)

model %>% compile(optimizer = optimizer_rmsprop(), loss = "mae")

history <- model %>% fit_generator(
     train_gen, steps_per_epoch = 500, epochs = 40, 
     validation_data = val_gen, validation_steps = val_steps)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-3-5-Stacking_Timeseries.jpg")
```

### Using Bdirectional RNNs

The last technique is called bidirectional RNNs. A bidirectional RNN is a common RNN variant that can offer greater performance than a regular RNN on certain tasks. it is frequently used in natural-language processing you could call it the Swiss Army knife of deep learning for natural-language processing.
 
RNNs are notably order dependent, or time dependent: they process the timesteps of their input sequences in order, and shuffling or reversing the timesteps can completely change the representations the RNN extracts from the sequence. This is precisely the reason they perform well on problems where order is meaningful, such as the temperature-forecasting problem. A bidirectional RNN exploits the order sensitivity of RNNs: it consists of using two regular RNNs, such as layer_gru and layer_lstm that you are already familiar with, each of which processes the input sequence in one direction (chronologically and anti-chronologically), and then merging their representations. By processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN.
 
Remarkably, the fact that the RNN layers in this section have processed sequences in chronological order (older timesteps first) may have been an arbitrary decision. At least, it is a decision we made no attempt to question so far. Could the RNNs have performed well enough if they processed input sequences in anti chronological order, for instance (newer timesteps first)? Try this in practice and see what happens. All you need to do is write a variant of the data generator where the input sequences are reverted along the time dimension (replace the last line with `list(samples [, ncol(samples):1,], targets))`.  Training the same one-GRU-layer network that you used in the first experiment in this section, you get the results shown below????.

The reversed-order GRU under performs even the common-sense baseline, indicating that in this case, chronological processing is important to the success of your approach. This makes perfect sense: the underlying GRU layer will typically be better at remembering the recent past than the distant past, and naturally the more recent weather data points are more predictive than older data points for the problem (That is what makes the common-sense baseline fairly strong). Thus the chronological version of the layer is bound to outperform the reversed-order version. Importantly, this is not true for many other problems, including natural language: intuitively, the importance of a word in understanding a sentence is not usually dependent on its position in the
 
Try the same trick on the LSTM IMDB example from before.

```{r eval=FALSE}
library(keras)
max_features <- 10000
maxlen <- 500

imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb

x_train <- lapply(x_train, rev)	  
x_test <- lapply(x_test, rev)

x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)

model <- keras_model_sequential() %>%
     layer_embedding(input_dim = max_features, output_dim = 128) %>% 
     layer_lstm(units = 32) %>%
     layer_dense(units = 1, activation = "sigmoid")

model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))

history <- model %>% fit(
     x_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-3-5-Reversed_Timeseries.jpg")
```

You get performance nearly identical to that of the chronological-order LSTM. Remarkably, on such a text dataset, reversed-order processing works just as well as chronological processing, confirming the hypothesis that, although word order does matter in understanding language, which order you use is not crucial. Importantly, an RNN trained on reversed sequences will learn different representations than one trained on the original sequences, much as you would have different mental models if time flowed backward in the real world if you lived a life where you died on your first day and were born on your last day. In machine learning, representations that are different yet useful are always worth exploiting, and the more they differ, the better: they offer a new angle from which to look at your data, capturing aspects of the data that were missed by other approaches, and thus they can help boost performance on a task. 

A bidirectional RNN exploits this idea to improve on the performance of chronological order RNNs. It looks at its input sequence both ways, obtaining potentially richer representations and capturing patterns that may have been missed by the chronological-order version alone.
 
To instantiate a bidirectional RNN in Keras, you use the `bidirectional()` function, which takes a recurrent layer instance as an argument. The `bidirectional()` function creates a second, separate instance of this recurrent layer and uses one instance for processing the input sequences in chronological order and the other instance for processing the input sequences in reversed order. Try it on the IMDB sentiment-analysis task.	

```{r eval=FALSE}
model <- keras_model_sequential() %>%
     layer_embedding(input_dim = max_features, output_dim = 32) %>% 
     bidirectional(layer_lstm(units = 32)) %>% 
     layer_dense(units = 1, activation = "sigmoid")

model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))

history <- model %>% fit(
     x_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2)
```

It performs slightly better than the regular LSTM you tried previously, achieving over 89% validation accuracy. It also seems to overfit more quickly, which is unsurprising because a bidirectional layer has twice as many parameters as a chronological LSTM. With some regularization, the bidirectional approach would likely be a strong performer on this task.

Now try the same approach on the temperature-prediction task.

```{r biDirectional, eval=FALSE}
model <- keras_model_sequential() %>%
     bidirectional(layer_gru(units = 32), input_shape = list(NULL, dim(data)[[-1]])) %>%
     layer_dense(units = 1)

model %>% compile(optimizer = optimizer_rmsprop(), loss = "mae")

history <- model %>% fit_generator(
     train_gen, steps_per_epoch = 500, epochs = 40, 
     validation_data = val_gen, validation_steps = val_steps)
plot(history)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-3-5-Bidirectional_Timeseries.jpg")
```

This performs about as well as the regular layer_gru. it is easy to understand why: all the predictive capacity must come from the chronological half of the network, because the anti-chronological half is known to be severely under-performing on this task (again, because the recent past matters much more than the distant past in this case). 

### Going Further

There are many other things you could try, in order to improve performance on the temperature-forecasting problem:

- Adjust the number of units in each recurrent layer in the stacked setup. The current choices are largely arbitrary and thus probably sub-optimal.
- Adjust the learning rate used by the RMSprop optimizer.
- Try using layer_lstm instead of layer_gru.
- Try using a bigger densely connected regressor on top of the recurrent layers: that is, a bigger dense layer or even a stack of dense layers.
- do not forget to eventually run the best-performing models (in terms of validation MAE) on the test set! Otherwise, you will develop architectures that are overfitting to the validation set.

As always, deep learning is more an art than a science. We can provide guidelines that suggest what is likely to work or not work on a given problem, but, ultimately, every problem is unique; you will have to evaluate different strategies empirically. There is currently no theory that will tell you in advance precisely what you should do to optimally solve a problem. You must iterate.

### Wrapping up

Here is what you should take away from this section:

- When approaching a new problem, it is good to first establish common-sense baselines for your metric of choice. If you do not have a baseline to beat, you cannot tell whether you are making real progress.
- Try simple models before expensive ones, to justify the additional expense. Sometimes a simple model will turn out to be your best option.
- When you have data where temporal ordering matters, recurrent networks are a great fit and easily outperform models that first flatten the temporal data.
- To use dropout with recurrent networks, you should use a time-constant dropout mask and recurrent dropout mask. These are built into Keras recurrent layers, so all you have to do is use the dropout and recurrent_dropout arguments of recurrent layers.
- Stacked RNNs provide more representational power than a single RNN layer. they are also much more expensive and thus not always worth it. Although they offer clear gains on complex problems (such as machine translation), they may not always be relevant to smaller, simpler problems.
- Bidirectional RNNs, which look at a sequence both ways, are useful on natural language processing problems. But t_hey are not strong performers on sequence data where the recent past is much more informative than the beginning of the sequence_.

NOTE There are two important concepts we will not cover: recurrent attention and sequence masking. Both tend to be especially relevant for natural-language processing.

## Sequence Processing with Convnets

```{r}
rm(list=ls())
```

Earlier you learned about convolutional neural networks (convnets) and how they perform particularly well on computer vision problems, due to their ability to operate convolutionally, extracting features from local input patches and allowing for representation modularity and data efficiency. The same properties that make convnets excel at computer vision also make them highly relevant to sequence processing. Time can be treated as a spatial dimension, like the height or width of a 2D image.

Such 1D convnets can be competitive with RNNs on certain sequence-processing problems, usually at a considerably cheaper computational cost. Recently, 1D convnets, typically used with dilated kernels, have been used with great success for audio generation and machine translation. In addition to these specific successes, it has long been known that _small 1D convnets can offer a fast alternative to RNNs for simple tasks such as text classification and timeseries forecasting_.

### Understanding 1D Convolution for Sequence Data

The convolution layers introduced previously were 2D convolutions, extracting 2D patches from image tensors and applying an identical transformation to every patch. In the same way, you can use 1D convolutions, extracting local 1D patches (subsequences) from sequences.
 
Such 1D convolution layers can recognize local patterns in a sequence. Because the same input transformation is performed on every patch, a pattern learned at a certain position in a sentence can later be recognized at a different position, making 1D convnets translation invariant (for temporal translations). For instance, a 1D convnet processing sequences of characters using convolution windows of size 5 should be able to learn words or word fragments of length 5 or less, and it should be able to recognize these words in any context in an input sequence. A character-level 1D convnet is thus able to learn about word morphology. 

### 1D pooling for sequence data

you are already familiar with 2D pooling operations, such as 2D average pooling and max pooling, used in convnets to spatially downsample image tensors. The 2D pooling operation has a 1D equivalent: extracting 1D patches (subsequences) from an input and outputting the maximum value (max pooling) or average value (average pooling). Just as with 2D convnets, this is used for reducing the length of 1D inputs (subsampling). 

### Implementing a 1D convnet

In Keras, you use a 1D convnet via the `layer_conv_1d` function, which has an interface similar to `layer_conv_2d.` It takes as input 3D tensors with shape (samples, time, features) and returns similarly shaped 3D tensors. The convolution window is a 1D window on the temporal axis: the second axis in the input tensor.

Build a simple two-layer 1D convnet and apply it to the IMDB sentiment classification task. As a reminder, this is the code for obtaining and preprocessing the data.

```{r}
max_features <- 10000 
max_len <- 500

imdb <- dataset_imdb(num_words = max_features) 
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb 

x_train <- pad_sequences(x_train, maxlen = max_len) 
x_test <- pad_sequences(x_test, maxlen = max_len) 
```

1D convnets are structured in the same way as their 2D counterparts: they consist of a stack of `layer_conv_1d` and `layer_max_pooling_1d`, ending in either a global pooling layer or `layer_flatten`, that turn the 3D outputs into 2D outputs, allowing you to add one or more dense layers to the model for classification or regression.

> One difference, though, is the fact that you can afford to use larger convolution windows with 1D convnets. With a 2D convolution layer, a 3 x 3 convolution window contains 3 x 3 = 9 feature vectors; but with a 1D convolution layer, a convolution window of size 3 contains only 3 feature vectors. You can thus easily afford 1D convolution windows of size 7 or 9.

This is the example 1D convnet for the IMDB dataset.

```{r}
model <- keras_model_sequential() %>%
     layer_embedding(input_dim = max_features, output_dim = 128, input_length = max_len) %>%
     layer_conv_1d(filters = 32, kernel_size = 7, activation = "relu") %>%
     layer_max_pooling_1d(pool_size = 5) %>% 
     layer_conv_1d(filters = 32, kernel_size = 7, activation = "relu") %>%
     layer_global_max_pooling_1d() %>% 
     layer_dense(units = 1) 
summary(model)
```
```{r eval=FALSE}
model %>% compile(
     optimizer = optimizer_rmsprop(lr = 1e-4), loss = "binary_crossentropy", 
     metrics = c("acc"))

history <- model %>% fit(x_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2)
plot(history())
```
```{r echo=FALSE, eval=FALSE}
setwd("~/GitHub/DeepLearning/Deep_Learning_with_R")
load("./RData/1Dconvnet.RData")
```

Validation accuracy is somewhat less than that of the LSTM, but runtime is faster on both CPU and GPU (the exact increase in speed will vary greatly depending on your exact configuration). At this point, you could retrain this model for the right number of epochs (eight) and run it on the test set. This is a convincing demonstration that a 1D convnet can offer a fast, cheap alternative to a recurrent network on a word-level sentiment-classification task.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-4-3-Convnet1.jpg")
```

### Combining CNNs and RNNs for long Sequences

Because 1D convnets process input patches independently, they are not sensitive to the order of the timesteps (beyond a local scale, the size of the convolution windows), unlike RNNs. Of course, to recognize longer-term patterns, you can stack many convolution layers and pooling layers, resulting in upper layers that will see long chunks of the original inputs but That is still a fairly weak way to induce order sensitivity. One way to evidence this weakness is to try 1D convnets on the temperature-forecasting problem, where order sensitivity is key to producing good predictions. The following example reuses these previously defined variables: `float_data`, `train_gen`, `val_gen`, and `val_steps.`

```{r eval=FALSE}
model <- keras_model_sequential() %>%
     layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu", 
                   input_shape = list(NULL, dim(data)[[-1]])) %>%
     layer_max_pooling_1d(pool_size = 3) %>% 
     layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>%
     layer_max_pooling_1d(pool_size = 3) %>% 
     layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>%
     layer_global_max_pooling_1d() %>% 
     layer_dense(units = 1)

model %>% compile(optimizer = optimizer_rmsprop(), loss = "mae"
                  )
history <- model %>% fit_generator(train_gen, steps_per_epoch = 500, epochs = 20,
                                   validation_data = val_gen, 
                                   validation_steps = val_steps)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-4-4-TNN-CNN.jpg")
```

The validation MAE stays in the 0.40s:__ you cannot even beat the common-sense baseline__ using the small convnet. Again, this is because the convnet looks for patterns anywhere in the input timeseries and has no knowledge of the temporal position of a pattern it sees (toward the beginning, toward the end, and so on). Because more recent data points should be interpreted differently from older data points in the case of this specific forecasting problem, the convnet fails at producing meaningful results. This limitation of convnets is not an issue with the IMDB data, because patterns of keywords associated with a positive or negative sentiment are informative independently of where they are found in the input sentences.

One strategy to combine the speed and lightness of convnets with the order sensitivity of RNNs is to use a 1D convnet as a preprocessing step before an RNN. This is especially beneficial when you are dealing with sequences that are so long they cannot realistically be processed with RNNs, such as sequences with thousands of steps. The convnet will turn the long input sequence into much shorter (downsampled) sequences of higher-level features. This sequence of extracted features then becomes the input to the RNN part of the network.

This technique is not seen often in research papers and practical applications, possibly because it is not well known. __it is effective and ought to be more common__. Try it on the temperature-forecasting dataset. Because this strategy allows you to manipulate much longer sequences, you can either look at data from longer ago (by increasing the lookback parameter of the data genera-tor) or look at high-resolution timeseries (by decreasing the step parameter of the generator). Here, somewhat arbitrarily, you will use a step That is half as large, resulting in a timeseries twice as long, where the temperature data is sampled at a rate of 1 point per 30 minutes. The example reuses the generator function defined earlier.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-4-4-TNN-CNN.jpg")
```

```{r}
step = 3
lookback = 720
delay = 144

train_gen <- generator( data, lookback = lookback, delay = delay, min_index = 1, 
                        max_index = 200000, shuffle = TRUE, step = step)

val_gen <- generator( data, lookback = lookback, delay = delay, min_index = 200001, 
                      max_index = 300000, step = step
                      )
test_gen <- generator( data, lookback = lookback, delay = delay, min_index = 300001, 
                       max_index = NULL, step = step)

val_steps <- (300000 - 200001 - lookback) / 128 
test_steps <- (nrow(data) - 300001 - lookback) / 128


```

Below is the model, starting with two `layer_conv_1ds` and following up with a layer_ gru. 

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/6-4-4-1D-GRU1.jpg")
```

### Wrapping up

Here is what you should take away from this section:

- In the same way that 2D convnets perform well for processing visual patterns in 2D space, 1D convnets perform well for processing temporal patterns. They offer a faster alternative to RNNs on some problems, in particular natural language processing tasks.
- Typically, 1D convnets are structured much like their 2D equivalents from the world of computer vision: they consist of stacks of `layer_conv_1ds` and layer_ `max_pooling_1ds`, ending in a global pooling operation or flattening operation.
- Because RNNs are extremely expensive for processing very long sequences, but 1D convnets are cheap, it can be a good idea to use a 1D convnet as a preprocessing step before an RNN, shortening the sequence and extracting useful representations for the RNN to process. 


## Summary

- In this chapter, you learned the following techniques, which are widely applicable to any dataset of sequence data, from text to timeseries:

     â How to tokenize text
     â What word embeddings are, and how to use them
     â What recurrent networks are, and how to use them
     â How to stack RNN layers and use bidirectional RNNs to build more-powerful sequence-processing models
     â How to use 1D convnets for sequence processing
     - How to combine 1D convnets and RNNs to process long sequences
- You can use RNNs for timeseries regression (predicting the future), timeseries classification, anomaly detection in timeseries, and sequence labeling (such as identifying names or dates in sentences).
-  Similarly, you can use 1D convnets for machine translation (sequence-to-sequence convolutional models, like SliceNet),  document classification, and spelling correction.
- If global order matters in your sequence data, then it is preferable to use a recurrent network to process it. This is typically the case for timeseries, where the recent past is likely to be more informative than the distant past.
- If global ordering is not fundamentally meaningful, then 1D convnets will turn out to work at least as well and are cheaper. This is often the case for text data, where a keyword found at the beginning of a sentence is just as meaningful as a keyword found at the end.

# Advanced Deep Learning Best Practices

## Keras functional API
Until now, all neural networks introduced have been implemented using the sequential model (_keras_ model_sequential_). The sequential model makes the assumption that the network has exactly one input and exactly one output, and that it consists of a linear stack of layers. Some networks require several independent inputs, others require multiple outputs, and some networks have internal branching between layers that makes them look like graphs of layers rather than linear stacks of layers.
 
 Some tasks, for instance, require multimodal inputs: they merge data coming from different input sources, processing each type of data using different kinds of neural layers. Imagine a deep-learning model trying to predict the most likely market price of a second-hand piece of clothing, using the following inputs: user-provided metadata (such as the items brand, age, and so on), a user-provided text description, and a picture of the item. If you had only the metadata available, you could one-hot encode it and use a densely connected network to predict the price. If you had only the text description available, you could use an RNN or a 1D convnet. If you had only the picture, you could use a 2D convnet. But how can you use all three at the same time? A naive approach would be to train three separate models and then do a weighted average of their predictions. But this may be sub-optimal, because the information extracted by the models may be redundant. A better way is to jointly learn a more accurate model of the data by using a model that can see all available input modalities simultaneously: a model with three input branches

Similarly, some tasks need to predict multiple target attributes of input data. Given the text of a novel or short story, you might want to automatically classify it by genre (such as romance or thriller) but also predict the approximate date it was written. Of course, you could train two separate models: one for the genre and one for the date. But because these attributes are not statistically independent, you could build a better model by learning to jointly predict both genre and date at the same time. Such a joint model would then have two outputs, or heads. Due to correlations between genre and date, knowing the date of a novel would help the model learn rich, accurate representations of the space of novel genres, and vice versa.

Additionally, many recently developed neural architectures require nonlinear network topology: networks structured as directed acyclic graphs. The Inception family of networks (developed by Szegedy et al. at Google),  for instance, relies on Inception modules, where the input is processed by several parallel convolutional branches whose outputs are then merged back into a single tensor:

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/7-1-Inception.jpg")
```

There is also the recent trend of adding residual connections to a model, which started with the ResNet family of networks (developed by He et al. at Microsoft).  A residual connection consists of reinjecting previous representations into the downstream flow of data by adding a past output tensor to a later output tensor, which helps prevent information loss along the data-processing flow. There are many other examples of such graph-like networks.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/7-1-ResNet.jpg")
```

These three important use cases

- multi-input models
- multi-output models
- graph-like models

are not possible when defining a model with `keras_model_sequential.` But there is another far more general and flexible way to use Keras: the functional API. This section explains in detail what it is, what it can do, and how to use it.

### Intro Functional API

In the functional API, you build your input and output layers and then pass them to the _keras_model_ function. This model can be trained just like Keras sequential models.

Start with a minimal example that shows side-by-side a simple sequential model and its equivalent in the functional API:

```{r}
seq_model <- keras_model_sequential() %>%
     layer_dense(units = 32, activation = "relu", input_shape = c(64)) %>% 
     layer_dense(units = 32, activation = "relu") %>% 
     layer_dense(units = 10, activation = "softmax")


input_tensor <- layer_input(shape = c(64))	 
output_tensor <- input_tensor %>% 
     layer_dense(units = 32, activation = "relu") %>% 
     layer_dense(units = 32, activation = "relu") %>% 
     layer_dense(units = 10, activation = "softmax")
model <- keras_model(input_tensor, output_tensor)

summary(model)
```

The only part that may seem a bit magical at this point is passing only an input tensor and an output tensor to the `keras_model` function. Behind the scenes, Keras retrieves every layer involved in going from `input_tensor` to `output_tensor`, bringing them together into a graph-like data structure a model. Of course, the reason it works is that `output_tensor` was obtained by repeatedly transforming `input_tensor.` If you tried to build a model from inputs and outputs that were not related, you would get an error.

When it comes to compiling, training, or evaluating a model built this way, the API is the same as that of sequential models:

```{r eval=FALSE}
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy")

x_train <- array(runif(1000 * 64), dim = c(1000, 64)) 
y_train <- array(runif(1000 * 10), dim = c(1000, 10)) 

model %>% fit(x_train, y_train, epochs = 10, batch_size = 128)

model %>% evaluate(x_train, y_train)
```

### Multi-Input Models

The functional API can be used to build models that have multiple inputs. Typically, such models at some point merge their different input branches using a layer that can combine several tensors: by adding them, concatenating them, and so on. This is usually done via a Keras merge operation such as `layer_add`, `layer_concatenate`, and so on. Examine at a  simple example of a multi-input model: a question-answering model.
 
A typical question-answering model has two inputs: 

1. a natural-language question 
2. text snippet (such as a news article) providing information to be used for answering the question. 

The model must then produce an answer: in the simplest possible setup, this is a one-word answer obtained via a softmax over some predefined vocabulary.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/7-1-2-QnA.jpg")
```

Following is an example of how you can build such a model with the functional API. You set up two independent branches, encoding the text input and the question input as representation vectors; then, concatenate these vectors; and finally, add a softmax classifier on top of the concatenated representations.

```{r mult_Inputs, eval=FALSE}
text_vocabulary_size <- 10000 
ques_vocabulary_size <- 10000 
answer_vocabulary_size <- 500 

text_input <- layer_input(shape = list(NULL), dtype = "int32", name = "text")
# The text input is a variable-length sequence of integers. Note that you can optionally name the inputs

encoded_text <- text_input %>%
     layer_embedding(input_dim = 64, output_dim = text_vocabulary_size) %>% layer_lstm(units = 32)
# Embeds the inputs into a sequence of vectors of size 64
# Encodes the vectors into a single vector via LSTM

question_input <- layer_input(shape = list(NULL), dtype = "int32", name = "question")

encoded_question <- question_input %>% 
     layer_embedding(input_dim = 32, output_dim = ques_vocabulary_size) %>% layer_lstm(units = 16) 

concatenated <- layer_concatenate(list(encoded_text, encoded_question))
# Concatenates the encoded question and encoded text

answer <- concatenated %>%
     layer_dense(units = answer_vocabulary_size, activation = "softmax")


model <- keras_model(list(text_input, question_input), answer)
# At model start specify the 2 inputs and the output

model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = c("acc"))
```

How do you train this two-input model? There are two possible APIs: 

1. you can feed the model a list of arrays as inputs
2. you can feed it a dictionary that maps input names to arrays. 

Naturally, the latter option is available only if you give names to your inputs.

```{r feedData}
num_samples <- 1000
max_length <- 100

random_matrix <- function(range, nrow, ncol) {
     matrix(sample(range, size = nrow * ncol, replace = TRUE), nrow = nrow, ncol = ncol)}
# Dummy Data

text <- random_matrix(1:text_vocabulary_size, num_samples, max_length) 
question <- random_matrix(1:ques_vocabulary_size, num_samples, max_length) 
answers <- random_matrix(0:1, num_samples, answer_vocabulary_size)
# Answers are one-hot encoded, not integers

model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
# Fitting using a list on inputs

### OR

model %>% fit(list(text = text, question = question), answers, epochs = 10, batch_size = 128)
# Fitting using a named list of inputs
```

### Multi-Output Models

In the same way, you can use the functional API to build models with multiple outputs (or multiple heads). A simple example is a network that attempts to simultaneously predict different properties of the data, such as a network that takes as input a series of social media posts from a single anonymous person and tries to predict attributes of that person, such as age, gender, and income level

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/7-1-3-MultiOutput.jpg")
```

```{r}

vocabulary_size <- 50000 
num_income_groups <- 10

posts_input <- layer_input(shape = list(NULL), dtype = "int32", name = "posts")

embedded_posts <- posts_input %>% layer_embedding(input_dim = 256, output_dim = vocabulary_size)

base_model <- embedded_posts %>%
     layer_conv_1d(filters = 128, kernel_size = 5, activation = "relu") %>% 
     layer_max_pooling_1d(pool_size = 5) %>% 
     layer_conv_1d(filters = 256, kernel_size = 5, activation = "relu") %>% 
     layer_conv_1d(filters = 256, kernel_size = 5, activation = "relu") %>% 
     layer_max_pooling_1d(pool_size = 5) %>% 
     layer_conv_1d(filters = 256, kernel_size = 5, activation = "relu") %>% 
     layer_conv_1d(filters = 256, kernel_size = 5, activation = "relu") %>% 
     layer_global_max_pooling_1d() %>% 
     layer_dense(units = 128, activation = "relu") 

age_prediction <- base_model %>% layer_dense(units = 1, name = "age")
#Note the ouput layers are given names.

income_prediction <- base_model %>% 
     layer_dense(num_income_groups, activation = "softmax", name = "income")

gender_prediction <- base_model %>% layer_dense(units = 1, activation = "sigmoid", name = "gender")

model <- keras_model(posts_input, list(age_prediction, income_prediction, gender_prediction))
```

__Training such a model requires the ability to specify different loss functions for different heads of the network__: for instance, age prediction is a scalar regression task, but gender prediction is a binary classification task, requiring a different training procedure. But because gradient descent requires you to minimize a scalar, you must combine these losses into a single value in order to train the model. The simplest way to combine different losses is to sum them all. In Keras, you can use either a list or a named list of losses in compile to specify different objects for different outputs; the resulting loss values are summed into a global loss, which is minimized during training.

```{r eval = FALSE}
model %>% compile(optimizer = "rmsprop",
                  loss = c("mse", "categorical_crossentropy", "binary_crossentropy") )

#This is the same as above if you give names to output layers
model %>% compile(optimizer = "rmsprop",
                  loss = list(age = "mse",
                              income = "categorical_crossentropy",
                              gender = "binary_crossentropy") )
<<<<<<< HEAD
```

> Very imbalanced loss contributions will cause the model representations to be optimized preferentially for the task with the largest individual loss, at the expense of the other tasks. 

To remedy this, assign different levels of importance to the loss values in their contribution to the final loss. This is useful in particular if the loss values use different scales. For instance, the mean squared error (MSE) loss used for the age-regression task typically takes a value around 3-5, whereas the crossentropy loss used for the gender-classification task can be as low as 0.1. In such a situation, to balance the contribution of the different losses, you can assign a weight of 10 to the crossentropy loss and a weight of 0.25 to the MSE loss.

Similar to multi-input models, you can pass data to the model for training either via a plain list of arrays or via a named list of arrays.

Below is a multi-output with loss weighting example:
```{r}
model %>% compile(optimizer = "rmsprop", 
                  loss = c("mse", "categorical_crossentropy", "binary_crossentropy"), 
                  loss_weights = c(0.25, 1, 10))

####################### OR
#If you give names to output layers:

model %>% compile(optimizer = "rmsprop", loss = list(age = "mse", income = "categorical_crossentropy",
                                                     gender = "binary_crossentropy"), 
                  loss_weights = list(age = 0.25, income = 1, gender = 10))
```

Feeding data to a multi-output model
```{r}
# age_targets, income_targets, and gender_targets are assumed to be R arrays.
model %>% fit(posts, list(age_targets, income_targets, gender_targets), epochs = 10, batch_size = 64)

# Equivalent (possible only if you give names to the output layers)
model %>% fit(posts, list(age = age_targets, income = income_targets, gender = gender_targets),
              epochs = 10, batch_size = 64)
```

### Directed Acylic Graphs of Layers

With the functional API, you can implement networks with a complex internal topology. Neural networks in Keras are allowed to be arbitrary directed acyclic graphs of layers. The qualifier acyclic is important: these graphs cannot have cycles. it is impossible for a tensor x to become the input of one of the layers that generated x. The only processing loops that are allowed (that is, recurrent connections) are those internal to recurrent layers.

Several common neural-network components are implemented as graphs. Two notable ones are `Inception` modules and `residual` connections. To better understand how the functional API can be used to build graphs of layers, take a look at how you can implement both of them in Keras.

#### Inception Modules

Inception  is a popular type of network architecture for convolutional neural networks. It consists of a stack of modules that themselves look like small independent networks, split into several parallel branches. The most basic form of an Inception module has three to four branches starting with a 1 x 1 convolution, followed by a 3 x 3 convolution, and ending with the concatenation of the resulting features. This setup helps the network separately learn spatial features and channel-wise features, which is more efficient than learning them jointly. More-complex versions of an Inception module are also possible, typically involving pooling operations, different spatial convolution sizes (for example, 5 x 5 instead of 3 x 3 on some branches), and branches without a spatial convolution (only a 1 x 1 convolution). An example of such a module is shown below.
 
```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/inception.JPG")
```

----

__The purpose of 1 x 1 convolutions__

You already know convolutions extract spatial patches around every tile in an input tensor and apply the same transformation to each patch. An edge case is when the patches extracted consist of a single tile. The convolution operation then becomes equivalent to running each tile vector through a dense layer: it will compute features that mix together information from the channels of the input tensor, but it will not mix information across space (because it is looking at one tile at a time). __1 x 1 convolutions (also called pointwise convolutions) are featured in Inception modules, where they contribute to factoring out channel-wise feature learning and spacewise feature learning a reasonable thing to do if you assume that each channel is highly auto-correlated across space, but different channels may not be highly correlated with each other.__

----

Here is how to implement the module featured in the image above using the functional API. This example assumes the existence of a 4D input tensor input:

```{r}
branch_a <- input %>%
  layer_conv_2d(filters = 128, kernel_size = 1, activation = "relu", strides = 2)
#  Every branch has the same stride value (2), which is necessary to keep all branch outputs the same size so you can concatenate them.

branch_b <- input %>%
  layer_conv_2d(filters = 128, kernel_size = 1, activation = "relu") %>% 
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu", strides = 2) 
# In this branch, the striding occurs in the spatial convolution layer.

branch_c <- input %>%
  layer_average_pooling_2d(pool_size = 3, strides = 2) %>% 
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu")
# In this branch, the striding occurs in the average pooling layer.

branch_d <- input %>%	 
  layer_conv_2d(filters = 128, kernel_size = 1, activation = "relu") %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu", strides = 2)

output <- layer_concatenate(list(branch_a, branch_b, branch_c, branch_d)
)
## Concatenates the branch outputs to obtain the module output
```

The full _inception v3 architecture_ is available in Keras as `application_inception_v3`,
including weights pretrained on the ImageNet dataset. Another closely related model
available as part of the Keras applications module is `Xception`. `Xception`, which stands for
_extreme inception_, is a convnet architecture loosely inspired by `Inception.` It takes the idea
of separating the learning of channel-wise and space-wise features to its logical extreme,
and replaces `Inception` modules with depthwise separable convolutions consisting of a
depthwise convolution (a spatial convolution where every input channel is handled separately)
followed by a pointwise convolution (a 1 x 1 convolution)  effectively, an extreme
form of an `Inception` module, where spatial features and channel-wise features are fully
separated. `Xception` has roughly the same number of parameters as `Inception V3`, but it
shows better runtime performance and higher accuracy on ImageNet as well as other
large-scale datasets, due to a more efficient use of model parameters.

#### Residual Connections

`Residual connections` are a common graph-like network component found in many post-
2015 network architectures, including `Xception.` They tackle two common problems that plague any large-scale deep-learning model:

- vanishing gradients
- representational bottlenecks.

In general, adding residual connections to any model that has more than 10 layers is likely to be beneficial. A residual connection consists of making the output of an earlier layer available as
input to a later layer, effectively creating a shortcut in a sequential network. Rather than being concatenated to the later activation, the earlier output is summed with the later activation, which assumes that both activations are the same size. If they are different sizes, you can use a linear transformation to reshape the earlier activation into the target shape (for example, a dense layer without an activation or, for convolutional feature maps, a 1 x 1 convolution without an activation).

Here is how to implement a residual connection in Keras when the feature-map sizes are the same, using identity residual connections. This example assumes the existence of a 4D input tensor input:

```{r}
# This applies a transformation to the input
output <- input %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu", padding = "same") %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu", padding = "same") %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu", padding = "same")

# This adds teh original back to output
output <- layer_add(list(output, input))
```

And the following implements a residual connection when the feature-map sizes differ,
using a linear residual connection (again, assuming the existence of a 4D input
tensor input):

```{r}
output <- input %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu", padding = "same") %>%
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu", padding = "same") %>%
  layer_max_pooling_2d(pool_size = 2, strides = 2)

residual <- input %>%
  layer_conv_2d(filters = 128, kernel_size = 1, strides = 2, padding = "same")
# Uses a 1 x 1 convolution to linearly down sample the original input tensor to the same shape as the output

output <- layer_add(list(output, residual))
# Adds the residual tensor back to the output features
```

----

__Representational Bottlenecks in Deep Learning__

In a sequential model, each successive representation layer is built on top of the previous one, which means it only has access to information contained in the activation of the previous layer. If one layer is too small (for example, it has features that are too low-dimensional), then the model will be constrained by how much information can be crammed into the activations of this layer.

You can grasp this concept with a signal-processing analogy: if you have an audio processing pipeline that consists of a series of operations, each of which takes as input the output of the previous operation, then if one operation crops your signal to a low-frequency range (for example, 0-15 kHz), the operations downstream will never be able to recover the dropped frequencies. Any loss of information is permanent. 

Residual connections, by reinjecting earlier information downstream, partially solve this issue for deep-learning models.

----

----

__Vanishing Gradients in Deep Learning__

Backpropagation, the master algorithm used to train deep neural networks, works by propagating a feedback signal from the output loss down to earlier layers. If this feedback signal has to be propagated through a deep stack of layers, the signal may become tenuous or even be lost entirely, rendering the network untrainable. 

This issue is known as vanishing gradients.

This problem occurs both with deep networks and with recurrent networks over very long sequences in both cases, a feedback signal must be propagated through a long series of operations. you are already familiar with the solution that the LSTM layer uses to address this problem in recurrent networks: it introduces a carry track that propagates information parallel to the main processing track. 

Residual connections work in a similar way in feedforward deep networks, but they are even simpler: they introduce a purely linear information carry track parallel to the main layer stack,  thus helping to propagate gradients through arbitrarily deep stacks of layers.

----

### Layer Weight Sharing

One more important feature of the functional API is the ability to r_euse a layer instance several times_. When you call a layer instance twice, instead of instantiating a new layer for each call, you reuse the same weights with every call. This allows you to build models that have shared branches several branches that all share the same knowledge and perform the same operations. They share the same representations and learn these representations simultaneously for different sets of inputs.

For example, consider a model that attempts to assess the semantic similarity between two sentences. The model has two inputs (the two sentences to compare) and outputs a score between 0 and 1, where 0 means unrelated sentences and 1 means sentences that are either identical or reformulations of each other. Such a model could be useful in many applications, including deduplicating natural-language queries in a dialog system.

In this setup, the two input sentences are interchangeable, because semantic similarity is a symmetrical relationship: the similarity of A to B is identical to the similarity of B to A. For this reason, it would not make sense to learn two independent models for processing each input sentence. Rather, you want to process both with a single LSTM layer. The representations of this LSTM layer (its weights) are learned based on both inputs simultaneously. This is what we call a __Siamese LSTM model__ or a __shared LSTM__.

```{r}
# Instantiates a single LSTM layer, once
lstm <- layer_lstm(units = 32)

# Building the left branch of the model: inputs are variable-length sequences of vectors of size 128.
left_input <- layer_input(shape = list(NULL, 128))
left_output <- left_input %>% lstm()

# Building the right branch of the model: when you call an existing layer instance, you reuse its weights.
right_input <- layer_input(shape = list(NULL, 128))
right_output <- right_input %>% lstm()

merged <- layer_concatenate(list(left_output, right_output))

# Builds the classifier on top
predictions <- merged %>% layer_dense(units = 1, activation = "sigmoid")

# Instantiating and training the model: when you train such a model, the weights of the LSTM layer are updated based on both inputs.
model <- keras_model(list(left_input, right_input), predictions)
model %>% fit(list(left_data, right_data), targets)

```

Naturally, a layer instance may be used more than once it can be called arbitrarily many times, reusing the same set of weights every time.

### Models as Layers

Importantly, in the functional API, models can be used as you would use layers  effectively, you can think of a model as a bigger layer. This is true of models created with both the `keras_model` and `keras_model_sequential` functions. This means you can call a model on an input tensor and retrieve an output tensor:

`y <- model(x)`

If the model has multiple input tensors and multiple output tensors, it should be called with a list of tensors:

`c(y1, y2) %<-% <- model(list(x1, x2))`

When you call a model instance, you are reusing the weights of the model  exactly like what happens when you call a layer instance. Calling an instance, whether it is a layer instance or a model instance, will always reuse the existing learned representations of the instance which is intuitive.

One simple practical example of what you can build by reusing a model instance is a vision model that uses a dual camera as its input: two parallel cameras, a few centimeters (one inch) apart. Such a model can perceive depth, which can be useful in many applications. You should not need two independent models to extract visual features from the left camera and the right camera before merging the two feeds. Such low-level processing can be shared across the two inputs: that is, done via layers that use the same weights and thus share the same representations. Here is how to implement a Siamese vision model (shared convolutional base) in Keras:

__
```{r}
# The base image-processing model is the Xception network (convolutional base only).
xception_base <- application_xception(weights = NULL, include_top = FALSE)

# The inputs are 250 x 250 RGB images.
left_input <- layer_input(shape = c(250, 250, 3))
right_input <- layer_input(shape = c(250, 250, 3))

# Calls the same vision model twice
left_features = left_input %>% xception_base()
right_features <- right_input %>% xception_base()

# The merged features contain information from the right visual feed and the left visual feed.
merged_features <- layer_concatenate(list(left_features, right_features))
```

### Wrapping up

Takeaways

- To step out of the sequential API whenever you need anything more than a linear stack of layers
- How to build Keras models with several inputs, several outputs, and complex internal network topology, using the Keras functional API
- How to reuse the weights of a layer or model across different processing branches, by calling the same layer or model instance several times

##Monitoring Deep Learning Models 

Review ways to gain greater access to and control over what goes on inside your model during training. Launching a training run on a large dataset for tens of epochs using `fit()` or _fit_generator()_ can be a bit like launching a paper airplane: past the initial impulse, you do not have any control over its trajectory or its landing spot. If you want to avoid bad outcomes (and thus wasted paper airplanes), it is smarter to use not a paper plane, but a drone that can sense its environment, send data back to its operator, and automatically make steering decisions based on its current state. 

The techniques presented will transform the call to` fit()` from a paper airplane into a smart, autonomous drone that can self-introspect and dynamically take action.

### Using Callbacks

When you are training a model, there are many things you cannot predict from the start. In particular, you cannot tell how many epochs will be needed to get to an optimal validation loss. The examples so far have adopted the strategy of training for enough epochs that you begin overfitting, using the first run to figure out the proper number of epochs to train for, and then finally launching a new training run from scratch using this optimal number. Of course, this approach is wasteful.

A much better way to handle this is to stop training when you measure that the validation loss in no longer improving. This can be achieved using a Keras `callback.` A `callback` is an object that is passed to the model in the call to `fit` and that is called by the model at various points during training. It has access to all the available data about the state of the model and its performance, and it can take action: interrupt training, save a model, load a different weight set, or otherwise alter the state of the model.

Here are some examples of ways you can use callbacks:

- _Model checkpointing_  Saving the current weights of the model at different points during training.
- _Early stopping_  Interrupting training when the validation loss is no longer improving (and saving the best model obtained during training).
- _Dynamically adjusting the value of certain parameters during training_  Such as the learning rate of the optimizer.
- _Logging training and validation metrics during training, or visualizing the representations learned by the model as they are updated_  The Keras progress bar that you are familiar with is a callback!

Keras includes a number of built-in callbacks (this is not an exhaustive list):

- `callback_model_checkpoint()`
- `callback_early_stopping()`
- `callback_learning_rate_scheduler()`
- `callback_reduce_lr_on_plateau()`
- `callback_csv_logger()`

Review a few of them to understand how  to use them: `callback_model_ checkpoint`, `callback_early_stopping`, and
`callback_reduce_lr_on_plateau`.

#### The model-checkpoint and early-stopping Callbacks

You can use `callback_early_stopping` to interrupt training once a target metric being monitored has stopped improving for a fixed number of epochs. For instance, this callback allows you to interrupt training as soon as you start overfitting, thus avoiding having to retrain your model for a smaller number of epochs. This callback is typically used in combination with `callback_model_checkpoint`, which lets you continually save the model during training (and, optionally, save only the current best model so far: the version of the model that achieved the best performance at the end of an epoch):

```{r eval=FALSE}
# Callbacks are passed to the model via the callbacks argument in fit, which takes a list of callbacks. You can pass any number of callbacks
callbacks_list <- list(
  callback_early_stopping(monitor = "acc", 
  # Interrupts training when improvement stops. Monitors the models validation accuracy
                          patience = 1), 
  #Interrupts training when accuracy has stopped improving for more than one epoch (that is, two epochs)
  
  callback_model_checkpoint( # Saves the current weights after every epoch
    filepath = "my_model.h5", # Path to the destination model file
    monitor = "val_loss", save_best_only = TRUE)) 
# These two arguments mean you will not overwrite the model file unless val_loss has improved, which allows you to keep the best model seen during training# 

model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))
# You monitor accuracy, so it should be part of the models metrics.

model %>% fit(x, y, epochs = 10, batch_size = 32, callbacks = callbacks_list, validation_data = list(x_val, y_val))
# Note that because the callback will monitor validation loss and accuracy, you need to pass validation_data to the call to fit.
```

#### Reduce-Learning-Rate-On-Plateau Callback

You  use this callback to reduce the learning rate when the validation loss has stopped improving. Reducing or increasing the learning rate in case of a loss plateau is is an effective strategy to get out of local minima during training. The following example uses `callback_reduce_lr_on_plateau`:

```{r eval=FALSE}
callbacks_list <- list(callback_reduce_lr_on_plateau(
  monitor = "val_loss", # Monitors the models validation loss
  factor = 0.1, # Divides the learning rate by 10 when triggered
  patience = 10)) # The callback is triggered after the validation loss has stopped improving for 10 epochs.

model %>% fit(x, y, epochs = 10, batch_size = 32, callbacks = callbacks_list, validation_data = list(x_val, y_val))
# Because the callback will monitor the validation loss, you need to pass validation_data to the call to fit.
```

Keras also provides `KerasCallback` to create custom callbacks.  This material is consciously omitted.  See p 235 in book if interested in the future.

### TensorFlow Visualization Framework

To do good research or develop good models, you need rich, frequent feedback about what is going on inside your models during your experiments. That is the point of running experiments: to get information about how well a model performs  as much information as possible. Making progress is an iterative process, or loop: you start with an idea and express it as an experiment, attempting to validate or invalidate your idea. You run this experiment and process the information it generates. This inspires your next idea. The more iterations of this loop you are able to run, the more refined and powerful your ideas become. Keras helps you go from idea to experiment in the least possible time, and fast GPUs can help you get from experiment to result as quickly as possible. But what about processing the experiment results? That is where TensorBoard comes in.

This section introduces _TensorBoard_, a browser-based visualization tool that comes packaged with TensorFlow. Note that it is only available for Keras models when you are using Keras with the TensorFlow backend. 

The key purpose of TensorBoard is to help you visually monitor everything that goes on inside your model during training. If you are monitoring more information than just the models final loss, you can develop a clearer vision of what the model does and does not do, and you can make progress more quickly. TensorBoard gives you access to several features in your browser:

- Visually monitoring metrics during training
- Visualizing your model architecture
- Visualizing histograms of activations and gradients
- Exploring embeddings in 3D

Train a 1D convnet on the IMDB sentiment-analysis task.

The model is similar to the one you saw earlier. Consider only the top 2,000 words in the IMDB vocabulary, to make visualizing word embeddings more tractable.

```{r}
max_features <- 2000
max_len <- 500

imdb <- dataset_imdb(num_words = max_features)

c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb

x_train <- pad_sequences(x_train, maxlen = max_len)
x_test = pad_sequences(x_test, maxlen = max_len)

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 128, input_length = max_len, name = "embed") %>%
  layer_conv_1d(filters = 32, kernel_size = 7, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 5) %>%
  layer_conv_1d(filters = 32, kernel_size = 7, activation = "relu") %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units = 1)

summary(model)

model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))
```

Before you start using TensorBoard, you need to create a directory where you will store the log files it generates.

```{r}
dir.create("my_log_dir")
```

Launch the training with a TensorBoard callback instance. This callback will write log events to disk at the specified location.

```{r}
tensorboard("my_log_dir")
#Launch TensorBoard and wait for output in specified directory

callbacks = list(
  callback_tensorboard(log_dir = "my_log_dir",
                       histogram_freq = 1,    # Records activation histograms every 1 epoch
                       embeddings_freq = 1,)) # Records embedding data every 1 epoch

history <- model %>% fit(x_train, y_train, epochs = 20, batch_size = 128, validation_split = 0.2, callbacks = callbacks)
```

A web browser will open, with TensorBoard monitoring the specified directory for training output (see figure 7.10). Note that metrics will not appear in TensorBoard until after the first epoch (if you do not see your training metrics when you expect to, you may need to refresh the display). In addition to live graphs of the training and validation metrics, you get access to the Histograms tab, where you can find  visualizations of histograms of activation values taken by your layers.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/tensorboard1.JPG")
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/tensorboard2.JPG")
```

The Embeddings tab gives you a way to inspect the embedding locations and spatial relationships of the 10,000 words in the input vocabulary, as learned by the initial `layer_embedding` layer. Because the embedding space is 128-dimensional, TensorBoard automatically reduces it to 2D or 3D using a dimensionality-reduction algorithm of your choice: either principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE). In the image below, in the point cloud, you can clearly see two clusters: words with a positive connotation and words with a negative connotation. The visualization makes it immediately obvious that embeddings trained jointly with a specific objective result in models that are completely specific to the underlying task — that’s __the reason using pretrained generic word embeddings is rarely a good idea__.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/tensorboard3.JPG")
```

The Graphs tab shows an interactive visualization of the graph of low-level TensorFlow operations underlying your Keras model. There’s a lot
more going on than you would expect. The model you just built may look simple when defined in Keras — a small stack of basic layers — but under the hood, you need to construct a fairly complex graph structure to make it work. A lot of it is related to the gradient-descent process. This complexity differential between what you see and what you’re manipulating is the key motivation for using Keras as your way of building models, instead of working with raw TensorFlow to define everything from scratch. Keras makes your workflow dramatically simpler.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/tensorboard4.JPG")
```

### Wrapping up

- Keras callbacks provide a simple way to monitor models during training and automatically take action based on the state of the model.
- When you’re using TensorFlow, TensorBoard is a great way to visualize model activity in your browser. You can use it in Keras models via the `callback_tensorboard()` function.

## Improving Models

Trying out architectures blindly works well enough if you just need something that works okay. In this section, we’ll go beyond “works okay” to “works great and wins machine-learning competitions” by offering you a quick guide to a set of must-know techniques for building state-of-the-art deep-learning models.

### Advanced Architecture Patterns

We covered one important design pattern in detail in the previous section: residual connections. There are two more design patterns you should know about: 

- normalization
- depthwise separable convolution 

These patterns are especially relevant when you’re__ building high-performing deep convnets__, but they’re commonly found in many other types of architectures as well.

#### Batch Normalization

Normalization is a broad category of methods that seek to make different samples seen by a machine-learning model more similar to each other, which helps the model learn and generalize well to new data. The most common form of data normalization is one you’ve seen several times  already: centering the data on 0 by subtracting the mean from the data, and giving the data a unit standard deviation by dividing the data by its standard deviation. In effect, this makes the assumption that the data follows a normal (or Gaussian) distribution and makes sure this distribution is centered and scaled to unit variance:

```{r eval = FALSE}
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)
```

Previous examples normalized data before feeding it into models. But _data normalization should be a concern after every transformation operated by the network_: even if the data entering `layer_dense` or `layer_conv_2d` has a 0 mean and unit variance, there’s no reason to expect _a priori_ that this will be the case for the data coming out.

Batch normalization is a type of layer (`layer_batch_normalization` in Keras) that can _adaptively normalize data even as the mean and variance change over time during training_. It works by internally maintaining an exponential moving average of the batch-wise mean and variance of the data seen during training. The main effect of batch normalization is that _it helps with gradient propagation_ — much like residual connections and thus allows for deeper networks. Some very deep networks can only be trained if they include multiple batch normalization layers. For instance, `layer_batch_normalization` is used liberally in many of the advanced convnet architectures that come packaged with Keras, such as
`ResNet50`,` Inception V3`, and `Xception.`

The `layer_batch_normalization` layer is typically used after a convolutional or densely connected layer:

```{r eval = F}
layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
layer_batch_normalization()
layer_dense(units = 32, activation = "relu") %>%
layer_batch_normalization()
```

The `layer_batch_normalization` layer takes an axis argument, which specifies the feature axis that should be normalized. This argument defaults to –1, the last axis in the input tensor. This is the correct value when using `layer_dense`, `layer_conv_1d`, RNN layers, and `layer_conv_2d` with `data_format` set to "channels_last". But in the niche use case of `layer_conv_2d` with data_format set to `channels_first`, the features axis is axis 1; the axis argument in `layer_batch_normalization` should accordingly be set to 1.

----

__Batch Renormalization__

A recent improvement over regular batch normalization is batch renormalization, introduced that offers clears benefits over batch normalization, at no apparent cost. At the time of writing, it’s too early to tell whether it will supplant batch normalization, but it’s likely. Even more recently, self-normalizing neural networks, b which manage to keep data normalized after going through any dense layer by using a specific activation function (`selu`) and a specific initializer (`lecun_normal`). This scheme, although interesting, is limited
to densely connected networks for now, and its usefulness hasn’t yet been broadly replicated.

----

#### Depthwise Separable Convolution

What if you were told that there’s a layer you can use as a drop-in replacement for layer_conv_2d that will make your model lighter (fewer trainable weight parameters) and faster (fewer floating-point operations) and cause it to perform a few percentage points better on its task? 

That is precisely what the depthwise separable convolution layer does (`layer_separable_conv_2d`). This layer performs a spatial convolution on each channel of its input, independently, before mixing output channels via a pointwise convolution (a 1 × 1 convolution), as shown below.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/depthwise1.JPG")
```

This is equivalent to separating the learning of spatial features and the learning of channel-wise features, which makes a lot of sense if you assume that spatial locations in the input are highly correlated, but different channels are fairly independent. It requires significantly fewer parameters and involves fewer computations, thus resulting in smaller, speedier models. And because it’s a more representationally efficient way to perform convolution, it tends to learn better representations using less data, resulting in better-performing models.

These advantages become _especially important when you’re training small models from scratch on limited data_. For instance, here’s how you can build a lightweight, depthwise separable convnet for an image-classification task (softmax categorical classification) on a small dataset:

```{r eval=FALSE}
height <- 64
width <- 64
channels <- 3
num_classes <- 10

model <- keras_model_sequential() %>% layer_separable_conv_2d(filters = 32, kernel_size = 3, activation = "relu",
                                                              input_shape = c(height, width, channels)) %>%
  layer_separable_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>% 
  layer_separable_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  layer_separable_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  layer_max_pooling_2d(pool_size = 2) %>%
  layer_separable_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  layer_separable_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  layer_global_average_pooling_2d() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = num_classes, activation = "softmax")

model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy")
```

For larger-scale models, depthwise separable convolutions are the basis of the `Xception` architecture, a high-performing convnet that comes packaged with Keras. 

### Hyperparameter Optimization

When building a deep-learning model, you have to make many seemingly arbitrary decisions: How many layers should you stack? How many units or filters should go in each layer? Should you use `relu` as activation, or a different function? Should you use `layer_batch_normalization` after a given layer? How much dropout should you use?

These architecture-level parameters are called hyperparameters to distinguish them from the parameters of a model, which are trained via backpropagation. In practice, machine-learning engineers and researchers build intuition over time as to what works and what doesn’t when it comes to these choices—they develop hyperparameter-tuning skills. But there are no formal rules. If you want to get to the very limit of what can be achieved on a given task, you can’t be content with arbitrary choices made by a fallible human. Your initial decisions are almost
always sub-optimal, even if you have good intuition. You can refine your choices by tweaking them by hand and retraining the model repeatedly—that’s what machine learning engineers and researchers spend most of their time doing. But it shouldn’t be your job as a human to fiddle with hyperparameters all day — that is better left to a machine.

Thus you need to explore the space of possible decisions automatically, systematically, in a principled way. You need to search the architecture space and find the best performing ones empirically. That’s what the field of _automatic hyperparameter optimization_ is about: it’s an entire field of research, and an important one.

The process of optimizing hyperparameters typically looks like this:

1. Choose a set of hyperparameters (automatically)
2. Build the corresponding model
3. Fit it to your training data and measure the final performance on the validation data
4. Choose the next set of hyperparameters to try (automatically)
5. Repeat
6. Eventually, measure performance on your test data

The key to this process is the algorithm that uses this history of validation performance, given various sets of hyperparameters, to choose the next set of hyperparameters to evaluate. Many different techniques are possible: Bayesian optimization, genetic algorithms, simple random search, and so on.

Training the weights of a model is relatively easy: you compute a loss function on a mini-batch of data and then use the Backpropagation algorithm to move the weights in the right direction. Updating hyperparameters, on the other hand, is extremely challenging. Consider the following:

- Computing the feedback signal (does this set of hyperparameters lead to a high-performing model on this task?) can be extremely expensive: it requires creating and training a new model from scratch on your dataset.
- The hyperparameter space is typically made of discrete decisions and thus isn’t continuous or differentiable. Hence, you typically can’t do gradient descent in hyperparameter space. Instead, you must rely on gradient-free optimization techniques, which naturally are far less efficient than gradient descent.

Because these challenges are difficult and the field is still young, we currently only have access to very limited tools to optimize models. Often, it turns out that random search (choosing hyperparameters to evaluate at random, repeatedly) is the best solution, despite being the most naive one.

The `tfruns` [package](https://tensorflow.rstudio.com/tools/tfruns) provides a set of tools that can assist with hyperparameter tuning:

- Tracking the hyperparameters, metrics, output, and source code of every training run
- Comparing hyperparmaeters and metrics across runs to find the best-performing model
- Automatically generating reports to visualize individual training runs or comparisons between runs

__NOTE__ One important issue to keep in mind when doing hyperparameter optimization at scale is validation-set overfitting. Because you’re updating hyperparameters based on a signal that is computed using your validation data, you’re effectively training them on the validation data, and thus they will __quickly overfit to the validation data__. Always keep this in mind.

Overall, hyperparameter optimization is a powerful technique that is an absolute requirement to get to state-of-the-art models on any task or to win machine-learning competitions. Think about it: once upon a time, people handcrafted the features that went into shallow machine-learning models. That was very much sub-optimal. Now, deep learning automates the task of hierarchical feature engineering—features are
learned using a feedback signal, not hand-tuned, and that’s the way it should be. In the same way, you shouldn’t handcraft your model architectures; you should optimize them in a principled way. The field of automatic hyperparameter optimization is young and immature, as deep learning was some years ago, but expect it to boom in the next few years.

### Model Ensembling

Another powerful technique for obtaining the best possible results on a task is model ensembling. Ensembling consists of pooling together the predictions of a set of different models, to produce better predictions. If you look at machine-learning competitions, in particular on Kaggle, you’ll see that the winners use very large ensembles of models that inevitably beat any single model, no matter how good.

Ensembling relies on the assumption that different good models trained independently are likely to be good for different reasons : each model looks at slightly different aspects of the data to make its predictions, getting part of the “truth” but not all of it. You may be familiar with the ancient parable of the blind men and the elephant: a group of blind men come across an elephant for the first time and try to understand what the elephant is by touching it. Each man touches a different part of the elephant’s body—just one part, such as the trunk or a leg. Then the men describe to each other what an elephant is: “It’s like a snake,” “Like a pillar or a tree,” and so on. The blind men are essentially machine-learning models trying to understand the manifold of the training data, each from its own perspective, using its own assumptions (provided by the unique architecture of the model and the unique random weight initialization). Each of them gets part of the truth of the data, but not the whole truth. By pooling their perspectives together, you can get a far more accurate description of the
data. The elephant is a combination of parts: not any single blind man gets it quite right, but, interviewed together, they can tell a fairly accurate story.

Use classification as an example. The easiest way to pool the predictions of a set of classifiers (to ensemble the classifiers) is to average their predictions at inference time:

```{r eval=FALSE}
# Use four different models to compute initial predictions.
preds_a <- model_a %>% predict(x_val)
preds_b <- model_b %>% predict(x_val)
preds_c <- model_c %>% predict(x_val)
preds_d <- model_d %>% predict(x_val)
final_preds <- 0.25 * (preds_a + preds_b + preds_c + preds_d)  
# This new prediction array should be more accurate than any of the initial ones.
```

This will work only if the classifiers are more or less equally good. If one of them is significantly worse than the others, the final predictions may not be as good as the best classifier of the group.

A smarter way to ensemble classifiers is to do a weighted average, where the weights are learned on the validation data—typically, the better classifiers are given a higher weight, and the worse classifiers are given a lower weight. To search for a good set of ensembling weights, you can use random search or a simple optimization algorithm such as Nelder-Mead:

```{r}
preds_a <- model_a %>% predict(x_val)
preds_b <- model_b %>% predict(x_val)
preds_c <- model_c %>% predict(x_val)
preds_d <- model_d %>% predict(x_val)
final_preds <- 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d
# These weights (0.5, 0.25, 0.1, 0.15) are assumed to be learned empirically.
```

There are many possible variants: you can do an average of an exponential of the predictions, for instance. In general, a simple weighted average with weights optimized on the validation data provides a very strong baseline.

The key to making ensembling work is the diversity of the set of classifiers. Diversity is strength. If all the blind men only touched the elephant’s trunk, they would agree that elephants are like snakes, and they would forever stay ignorant of the truth of the elephant. Diversity is what makes ensembling work. In machine-learning terms, if all of your models are biased in the same way, then your ensemble will retain this same bias. If your models are biased in different ways, the biases will cancel each other out, and the ensemble will be more robust and more accurate.

For this reason, you should ensemble models that are as good as possible while being as different as possible. This typically means using very different architectures or even different brands of machine-learning approaches._ One thing that is largely not worth doing is ensembling the same network trained several times independently, from different random initializations_. If the only difference between your models is their random initialization and the order in which they were exposed to the training data, then your ensemble will be low-diversity and will provide only a tiny improvement over any single model.

One thing found to work well in practice — but that doesn’t generalize to every problem domain - is the use of an ensemble of tree-based methods (such as random forests or gradient-boosted trees) and deep neural networks. In 2014, partner Andrei Kolev and I took fourth place in the Higgs Boson decay detection challenge on Kaggle (www.kaggle.com/c/higgs-boson) using an ensemble of various tree models and deep neural networks. Remarkably, one of the models in the ensemble originated from a different method than the others (it was a regularized greedy forest) and had a significantly worse score than the others. Unsurprisingly, it was assigned a small weight in the ensemble. But to our surprise, it turned out to improve the overall ensemble by a large factor, because it was so different from every other model: it provided information that the other models didn’t have access to. That’s precisely the point of ensembling. It’s not so much about how good your best
model is; it’s about the diversity of your set of candidate models.

In recent times, one style of basic ensemble that has been very successful in practice is the wide and deep category of models, blending deep learning with shallow learning. Such models consist of jointly training a deep neural network with a large linear model. The joint training of a family of diverse models is yet another option to achieve model ensembling.

### Wrapping up

- When building high-performing deep convnets, use residual connections, batch normalization, and depthwise separable convolutions. In the
future, __it’s likely that depthwise separable convolutions will completely replace regular convolutions__, whether for 1D, 2D, or 3D applications, due to their higher representational efficiency.
- Building deep networks requires making many small hyperparameter and architecture choices, which together define how good your model will be. Rather than basing these choices on intuition or random chance, it’s better to systematically search hyperparameter space to find optimal choices. At this time, the process is expensive, and the tools to do it aren’t very good (but the `tfruns` package may be able to help you manage the process more effectively). When doing hyperparameter optimization, be mindful of validation-set overfitting!
- Winning machine-learning competitions or otherwise obtaining the best possible results on a task can only be done with large ensembles of models. Ensembling via a well-optimized weighted average is usually good enough. Remember: diversity is strength. It’s largely pointless to ensemble very similar models; the best ensembles are sets of models that are as dissimilar as possible (while having as much predictive power as possible, naturally).

##Summary

– How to build models as arbitrary graphs of layers, reuse layers (layer weight sharing), and use models as R functions (model templating).
– You can use Keras callbacks to monitor your models during training and take action based on model state.
– TensorBoard allows you to visualize metrics, activation histograms, and even embedding spaces.
– What batch normalization, depthwise separable convolution, and residual connections are.
– Why you should use hyperparameter optimization and model ensembling.

With these new tools, you’re better equipped to use deep learning in the real world and start building highly competitive deep-learning models.

# Generative Deep Learning

The potential of artificial intelligence to emulate human thought processes goes beyond passive tasks such as object recognition and mostly reactive tasks such as driving a car. Machine-learning models can learn the statistical latent space of images, music, and stories, and they can then sample from this space, creating new artworks with characteristics similar to those the model has seen in its training data. It’s a mere mathematical operation: the algorithm has no grounding in human life, human emotions, or our experience of the world; instead, it learns from an experience that has little in common with ours. It’s only our interpretation, as human spectators, that will give meaning to what the model generates. 

Latent space sampling can become a brush that empowers the artist, augments our creative affordances, and expands the space of what we can imagine. What’s more, it can make artistic creation more accessible by eliminating the need for technical skill and practice — setting up a new medium of pure expression, factoring art apart from craft.

Below, explore from various angles the potential of deep learning to augment artistic creation. Review sequence data generation (which can be used to generate text or music), DeepDream, and image generation using both variational autoencoders and generative adversarial networks. 

## LSTM Text Generation

Sequence data generation has been successfully applied to speech synthesis and to dialogue generation for chatbots. The Smart Reply feature that Google released in 2016, capable of automatically generating a selection of quick replies to emails or text messages, is powered by similar techniques.

### Generate Sequence Data

The universal way to generate sequence data in deep learning is to train a network (usually an RNN or a convnet) to predict the next token or next few tokens in a
sequence using the previous tokens as input. For instance, given the input `the cat is on the ma,` the network is trained to predict the target `t`, the next character. As usual when working with text data, tokens are typically words or characters and any network that can model the probability of the next token given the previous ones is called a _language model_. A language model captures _the latent space of language: its statistical structure_.

Once you have such a trained language model, you can sample from it (generate new sequences): you feed it an initial string of text (called conditioning data), ask it to generate the next character or the next word (you can even generate several tokens at once), add the generated output back to the input data, and repeat the process many times. This loop allows you to generate sequences of arbitrary length that reflect the structure of the data on which the model was trained: sequences that look almost like human-written sentences. In the example presented below, you’ll take a LSTM layer, feed it strings of N characters extracted from a text corpus,
and train it to predict character N + 1. The output of the model will be a softmax over all possible characters: a probability distribution for the next character. This LSTM is called a _character-level neural language model_.

### Importance of the Sampling Strategy

When generating text, the way you choose the next character is crucially important. A naive approach is __greedy sampling__, consisting of always choosing the most likely next character. But such an approach results in repetitive, predictable strings that don’t look like coherent language. A more interesting approach makes slightly more surprising choices: it introduces randomness in the sampling process, by sampling from the probability distribution for the next character. This is called __stochastic sampling__ (recall that stochasticity is what we call randomness in this field). In such a setup, if `e` has a probability 0.3 of being the next character, according to the model, you’ll choose it 30% of the time. Note that greedy sampling can be also cast as sampling from a probability distribution: one where a certain character has probability 1 and all others have probability 0.

Sampling probabilistically from the softmax output of the model is neat: it allows even unlikely characters to be sampled some of the time, generating more interesting looking sentences and sometimes showing creativity by coming up with new, realistic sounding words that didn’t occur in the training data. But there’s one issue with this strategy: it doesn’t offer a way to control the amount of randomness in the sampling process.

Why would you want more or less randomness? Consider an extreme case: pure random sampling, where you draw the next character from a uniform probability distribution,
and every character is equally likely. This scheme has maximum randomness; in other words, this probability distribution has maximum entropy. Naturally, it won’t produce anything interesting. At the other extreme, greedy sampling doesn’t produce anything interesting, either, and has no randomness: the corresponding probability distribution has minimum entropy. Sampling from the “real” probability distribution — the distribution that is output by the model’s softmax function — constitutes an intermediate point between these two extremes. But there are many other intermediate points of higher or lower entropy that you may want to explore. Less entropy will give the generated sequences a more predictable structure (and thus they will potentially be more realistic looking), whereas more entropy will result in more surprising and creative sequences. When sampling from generative models, it’s always good to explore different amounts of randomness in the generation process. Because we humans are the ultimate judges of how interesting the generated data is, interestingness is highly subjective, and there’s no telling in advance where the point of optimal entropy lies.

In order to control the amount of stochasticity in the sampling process, a parameter called the `softmax temperature` will be introduced that characterizes the entropy of the probability distribution used for sampling: it characterizes how surprising or predictable the choice of the next character will be. Given a temperature value, a new probability distribution is computed from the original one (the softmax output of the model) by reweighting it in the following way.

Reweighting a probability distribution to a different temperature
```{r eval=FALSE}
# original_distribution is a vector of probability values that must sum to 1.
# temperature is a factor quantifying the entropy of the output distribution.
reweight_distribution <- function(original_distribution,
temperature = 0.5) {
     distribution <- log(original_distribution) / temperature
distribution <- exp(distribution)
distribution / sum(distribution)}
# Returns a reweighted version of the original distribution. 
# The sum of the distribution may no longer be 1, so you divide it by its sum to obtain the new distribution.
```

Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data.

### Implementing Character-level LSTM Text Generation

The first thing you need is a lot of text data that you can use to learn a language model. In this example, use some of the writings of Nietzsche. The language model you’ll learn will thus be specifically a model of Nietzsche’s writing style and topics of choice, rather than a more generic model of the English language.

PREPARING THE DATA

Start by downloading the corpus and converting it to lowercase.

```{r}
path <- get_file("nietzsche.txt", origin = "https://s3.amazonaws.com/text-datasets/nietzsche.txt")

text <- tolower(readChar(path, file.info(path)$size))
cat("Corpus character count:", nchar(text), "\n")
```

Because the text is too large for my PC to later build a 3D tensor, the text is cut 2/3.

```{r}
text <- str_sub(text, start = 1L, end = round(nchar(text)/3,0))
nchar(text)
```

Next, extract partially overlapping sequences of length `maxlen`, one-hot encode, them, and pack them in a 3D array x of shape (sequences, maxlen, unique_characters).
Simultaneously, prepare an array `y` containing the corresponding targets: the one-hot-encoded characters that come after each extracted sequence.

```{r}
maxlen <- 60 # extract sequences of 60 characters
step <- 3 # sample a new sequence every 3 characters

text_indexes <- seq(1, nchar(text) - maxlen, by = step)
sentences <- str_sub(text, text_indexes, text_indexes + maxlen - 1) # Holds the extracted sequences
next_chars <- str_sub(text, text_indexes + maxlen, text_indexes + maxlen) # Holds the targets (the follow-up characters)

cat("Number of sequences (sentences): ", length(sentences), "\n")

chars <- unique(sort(strsplit(text, "")[[1]])) # List of unique characters in the corpus
cat("Unique characters:", length(chars), "\n")

char_indices <- 1:length(chars) # Named list that maps unique characters to their index
names(char_indices) <- chars

cat("Vectorization...\n")

# One-hot encodes the characters into binary arrays
x <- array(0L, dim = c(length(sentences), maxlen, length(chars)))
dim(x)
y <- array(0L, dim = c(length(sentences), length(chars)))
dim(y)
for (i in 1:length(sentences)) {
     sentence <- strsplit(sentences[[i]], "")[[1]]
     for (t in 1:length(sentence)) {
          char <- sentence[[t]]
          x[i, t, char_indices[[char]]] <- 1}
     next_char <- next_chars[[i]]
     y[i, char_indices[[next_char]]] <- 1}
```

Building the Network

This network is a single LSTM layer followed by a dense classifier and softmax over all possible characters. Note that recurrent neural networks aren’t the only way to do sequence data generation; 1D convnets also have proven extremely successful at this task recently. (Perhaps return and compare)

```{r}
model <- keras_model_sequential() %>%
     layer_lstm(units = 128, input_shape = c(maxlen, length(chars))) %>%
     layer_dense(units = length(chars), activation = "softmax")
```

Because the targets are one-hot encoded, use categorical_crossentropy as the loss to train the model.

```{r}
model %>% compile(loss = "categorical_crossentropy", optimizer = optimizer_rmsprop(lr = 0.01))
```

Training the Language Model

Given a trained model and a seed text snippet, generate new text by doing the following repeatedly:

1. Draw from the model a probability distribution for the next character, given the generated text available so far.
2. Reweight the distribution to a certain temperature.
3. Sample the next character at random according to the reweighted distribution.
4. Add the new character at the end of the available text.

This is the code to reweight the original probability distribution coming out of the model and draw a character index from it (the _sampling function_).

```{r}
sample_next_char <- function(preds, temperature = 1.0) {
     preds <- as.numeric(preds)
     preds <- log(preds) / temperature
     exp_preds <- exp(preds)
     preds <- exp_preds / sum(exp_preds)
     which.max(t(rmultinom(1, 1, preds)))
}
```

Finally, the following loop repeatedly trains and generates text. Begins generating text using a range of different temperatures after every epoch. This allows you to see how the generated text evolves as the model begins to converge as well as the impact of temperature in the sampling strategy.

```{r}
for (epoch in 1:60) {          # 60 epochs
     cat("epoch", epoch, "\n")
     
     model %>% fit(x, y, batch_size = 128, epochs = 1)
     
     start_index <- sample(1:(nchar(text) - maxlen - 1), 1)            # Selects a text seed at random
     seed_text <- str_sub(text, start_index, start_index + maxlen - 1) 
     
     cat("--- Generating with seed:", seed_text, "\n\n")
     
     for (temperature in c(0.2, 0.5, 1.0, 1.2)) {   # Tries a range of different sampling temperatures
          
          cat("------ temperature:", temperature, "\n")
          cat(seed_text, "\n")
          
          generated_text <- seed_text
          
          for (i in 1:400) {    # Generates 400 characters, starting from the seed text
               
               # One-hot encodes the characters generated so far
               sampled <- array(0, dim = c(1, maxlen, length(chars)))
               generated_chars <- strsplit(generated_text, "")[[1]]
               
               for (t in 1:length(generated_chars)) {
                    char <- generated_chars[[t]]
                    sampled[1, t, char_indices[[char]]] <- 1}
               
               # Samples the next character
               preds <- model %>% predict(sampled, verbose = 0)
               next_index <- sample_next_char(preds[1,], temperature)
               next_char <- chars[[next_index]]
               generated_text <- paste0(generated_text, next_char)
               generated_text <- substring(generated_text, 2)
               
               cat(next_char)}
          cat("\n\n")}
}
```
Below, the random seed text was: _ers, it has applied, and applies also to the highest and usu_  

Here is what is returned at the last epoch - 60:

```{}
epoch 60 
Epoch 1/1
66747/66747 [==============================] - 93s 1ms/step - loss: 0.9762
--- Generating with seed: ers, it has applied,
and applies also to the highest and usu 

------ temperature: 0.2 
ers, it has applied,
and applies also to the highest and usu 
ally seem to the sense of the sense of the sense of the same the sense of the present to the sense of the sense of the causa for the sense of the sense of the same the same to be a discented in the sentiment of the sense of the sense of the sense of the sense of the sense of the same the same to be more really to the sense of the sense of the sense of the command--which the senses and the person a

------ temperature: 0.5 
ers, it has applied,
and applies also to the highest and usu 
ally seem to the seriousness and its
love of histent product of her sphood a higher opprination of the system of
villict, and sense of the spirit and
crimins everything in the present to he distrust, or of the sense of the fasten and descrime--what docence and effort of which itself to the sensitness and scond of histhrowils and the souls and not to see the fastent deferred to the predent tenters 

------ temperature: 1 
ers, it has applied,
and applies also to the highest and usu 
all-classing clem and far that no without serpain with regard siver, attaint to comparing, for dambles:

18-is
man what is divine of their engerall," and more
more selfes: he fou nothient in
might to which one may knon one them place of permouse younged comestants the loverfully and
sense vainte of suffer, and presented of the light wust of many
pline assume over where at once seem of climin obiiv

------ temperature: 1.2 
ers, it has applied,
and applies also to the highest and usu 
ability the cardound new precisely with rate the systemsh--seem ti
"gives for his
storts rew(refragulnes doogstopiring that course, the alrearned in either intere with me"r" fived someef by when the most feels an elitualed conseding to its 
alvess no longer cleat
the re"mater, and human modern staci himself knowledge with with, but thinks--the systems fall, will
a
violinglyge with regard to prowve
```

As you can see, a low temperature value results in extremely repetitive and predictable text but local structure is highly realistic: in particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative; it sometimes invents completely new words that sound somewhat plausible. With a high temperature, the local structure starts to break down, and most words look like semi-random strings of characters. Without a doubt, 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment with multiple sampling strategies! A clever balance between learned structure and randomness
is what makes generation interesting.

Note that by training a bigger model, longer, on more data, you can achieve generated samples that look much more coherent and realistic than this one. But, of course, don’t expect to ever generate any meaningful text, other than by random chance: all you’re doing is sampling data from a statistical model of which characters
come after which characters. Language is a communication channel, and there’s a distinction between what communications are about and the statistical structure of the
messages in which communications are encoded. 

### Wrapping up

- You can generate discrete sequence data by training a model to predict the next tokens(s), given previous tokens.
- In the case of text, such a model is called a language model. It can be based on either words or characters.
- Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.
- One way to handle this is the notion of softmax temperature. Always experiment with different temperatures to find the right one.

## DeepDream

DeepDream is an artistic image-modification technique that uses the representations learned by convolutional neural networks.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/dream1.JPG")
```

The DeepDream algorithm is almost identical to the convnet filter-visualization technique introduced previously consisting of running a convnet in reverse: doing gradient ascent on the input to the convnet in order to maximize the activation of a specific filter in an upper layer of the convnet. DeepDream uses this same idea, with a few simple differences:

- With DeepDream, you try to maximize the activation of entire layers rather than that of a specific filter, thus mixing together visualizations of large numbers of features at once.
- You start not from blank, slightly noisy input, but rather from an existing image—thus the resulting effects latch on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.
- The input images are processed at different scales (called octaves), which improves the quality of the visualizations.

### Implementing DeepDream in Keras

Start from a convnet pretrained on ImageNet. In Keras, many such convnets are available: `VGG16`, `VGG19`, `Xception`, `ResNet50`, and so on. You can implement Deep-
Dream with any of them, but your convnet of choice will naturally affect your visualizations because different convnet architectures result in different learned features. The convnet used in the original DeepDream release was an `Inception` model, and in practice `Inception` is known to produce nice-looking DeepDreams, so that is what will used here - the V3 model that comes with Keras.

```{r}
k_set_learning_phase(0) # You won’t be training the model, so this command disables all training-specific operations.

model <- application_inception_v3(weights = "imagenet", include_top = FALSE,)
# Builds the Inception V3 network, without its convolutional base. The model will be loaded with pretrained ImageNet weights.
```

Compute the loss : the quantity you’ll seek to maximize during the gradient-ascent process. Previously, for filter visualization, we maximized the value of a specific
filter in a specific layer. Here, you’ll simultaneously maximize the activation of all filters in a number of layers. Specifically, you’ll maximize a weighted sum of the L2 norm of the activations of a set of high-level layers. The exact set of layers you choose (as well as their contribution to the final loss) has a major influence on the visuals you’ll be able to produce, so you want to make these parameters easily configurable.

Lower layers result in geometric patterns, whereas higher layers result in visuals in which you can recognize some classes from ImageNet (for example, birds or dogs).
Start from a somewhat arbitrary configuration involving four layers — but you’ll definitely want to explore many different configurations later.

```{r}
layer_contributions <- list(mixed2 = 0.2, mixed3 = 3, mixed4 = 2, mixed5 = 1.5, mixed9 = 3)
# added mixed 9
# Named list mapping layer names to a coefficient quantifying how much the layer’s activation contributes to the loss you’ll seek to maximize. Note that the layer names are hardcoded in the built-in `Inception V3` application. You can list all layer names using summary(model).
```

Define a tensor that contains the loss: the weighted sum of the L2 norm of the activations of the layers

```{r}
layer_dict <- model$layers
names(layer_dict) <- lapply(layer_dict, function(layer) layer$name)
#Creates a list that maps layer names to layer instances

loss <- k_variable(0) # You’ll define the loss by adding layer contributions to this scalar variable.

for (layer_name in names(layer_contributions)) {
     coeff <- layer_contributions[[layer_name]]
     activation <- layer_dict[[layer_name]]$output                   # Retrieves thelayer’s output
     scaling <- k_prod(k_cast(k_shape(activation), "float32"))
     loss <- loss + (coeff * k_sum(k_square(activation)) / scaling)} # Adds the L2 norm of thefeatures of a layer to the loss.
```

Set up the gradient-ascent process.

```{r}
dream <- model$input # This tensor holds the generated image: the dream.

grads <- k_gradients(loss, dream)[[1]] # Computes the gradients of the dream with regard to the loss

grads <- grads / k_maximum(k_mean(k_abs(grads)), 1e-7) # Normalizes the gradients (important trick)

outputs <- list(loss, grads)
fetch_loss_and_grads <- k_function(list(dream), outputs) # Sets up a Keras function to retrieve the value of the lossand gradients, given an input image

eval_loss_and_grads <- function(x) {
     outs <- fetch_loss_and_grads(list(x))
     loss_value <- outs[[1]]
     grad_values <- outs[[2]]
     list(loss_value, grad_values)}

# This function runs gradient ascent for a number of iterations.
gradient_ascent <- function(x, iterations, step, max_loss = NULL) {
     for (i in 1:iterations) {
          c(loss_value, grad_values) %<-% 
               eval_loss_and_grads(x)
          if (!is.null(max_loss) && loss_value > max_loss)
               break
          cat("...Loss value at", i, ":", loss_value, "\n")
          x <- x + (step * grad_values)}
     x
}
```

Finally: the actual DeepDream algorithm. First, you define a list of `scales` (also called `octaves`) at which to process the images. Each successive scale is larger than the previous one by a factor of 1.4 (it’s 40% larger): you start by processing a small image and then increasingly scale it up.

For each successive scale, from the smallest to the largest, you run gradient ascent to maximize the loss you previously defined, at that scale. After each gradient ascent run, you upscale the resulting image by 40%.

To avoid losing a lot of image detail after each successive scale-up (resulting in increasingly blurry or pixelated images), you can use a simple trick: after each scaleup, you reinject the lost details back into the image, which is possible because you know what the original image should look like at the larger scale. Given a small image size `S` and a larger image size `L`, you can compute the difference between the original image resized to size `L` and the original resized to size `S` — this difference quantifies the details lost when going from `S` to `L`.

---------------
Note:  Auxiliary function need for the code that follows

```{r}
resize_img <- function(img, size) {image_array_resize(img, size[[1]], size[[2]])}

save_img <- function(img, fname) {
     img <- deprocess_image(img)
     image_array_save(img, fname)}

preprocess_image <- function(image_path) {
     image_load(image_path) %>%
          image_to_array() %>%
          array_reshape(dim = c(1, dim(.))) %>%
          inception_v3_preprocess_input()}

deprocess_image <- function(img) {
     img <- array_reshape(img, dim = c(dim(img)[[2]], dim(img)[[3]], 3))
     img <- img / 2
     img <- img + 0.5
     img <- img * 255
     dims <- dim(img)
     img <- pmax(0, pmin(img, 255))
     dim(img) <- dims
     img}
```

--------------


```{r}
step <- 0.05        # Gradient ascent step size original 0.1
num_octave <- 1     # Number of scales at which to run gradient ascent original 3
octave_scale <- 1.7 # Size ratio between scales original 1.4
iterations <- 30    # Number of ascent steps to run at each scale original 20
# Altering with these hyperparameters achieve new effects

max_loss <- 10 # If the loss grows larger than 10, unterrupt the gradient-ascent process to avoid ugly artifacts.

base_image_path <- "./images/c2.jpg" # Fill this with the path to the image you want to use.
# If image to too small, an error will occur:  
# Processsing image shape 28 18 0 
# Error in if (!is.null(max_loss) && loss_value > max_loss) break : 
#   missing value where TRUE/FALSE needed

img <- preprocess_image(base_image_path) # Loads the base image into an array

# Prepares a list of shape tuples defining the different scales at which to run gradient ascent
original_shape <- dim(img)[-1]
successive_shapes <- list(original_shape)
for (i in 1:num_octave) {
     shape <- as.integer(original_shape / (octave_scale ^ i))
     successive_shapes[[length(successive_shapes) + 1]] <- shape}

successive_shapes <- rev(successive_shapes) # Reverses the list of shapes so they’re in increasing order

original_img <- img
shrunk_original_img <- resize_img(img, successive_shapes[[1]])
# Resizes the array of the image to the smallest scale

for (shape in successive_shapes) {
     cat("Processing image shape", shape, "\n")
     img <- resize_img(img, shape)  # Scales up the dream image
     img <- gradient_ascent(img, iterations = iterations, step = step, max_loss = max_loss) # Runs gradient ascent altering the dream
     upscaled_shrunk_original_img <- resize_img(shrunk_original_img, shape)  # Scales up the smaller version of the original image: it will be pixelated.
     same_size_original <- resize_img(original_img, shape) # Computes the high-quality version of the original image at this size
     lost_detail <- same_size_original - upscaled_shrunk_original_img  # The difference between the two is the detail that was lost when scaling up.
     img <- img + lost_detail  # Reinjects lost detail into the dream
     shrunk_original_img <- resize_img(original_img, shape)
     save_img(img, fname = sprintf("dream_at_scale_%s.png", paste(shape, collapse = "x")))}
```

NOTE Because the original Inception V3 network was trained to recognize concepts in images of size 299 × 299, and given that the process involves scaling the images down by a reasonable factor, the DeepDream implementation produces much better results on images that are somewhere between 300 × 300 and 400 × 400. Regardless, you
can run the same code on images of any size and any ratio.

My Test

Explore what you can do by adjusting which layers you use in your loss. Layers that are lower in the network contain more-local, less-abstract representations and lead to dream patterns that look more geometric. Layers that are higher up lead to more-recognizable visual patterns based on the most common objects found in ImageNet, such as dog eyes, bird feathers, and so on. You can use random generation of the parameters in the layer_contributions dictionary to quickly explore many different layer combinations.

### Wrapping up

- DeepDream consists of running a convnet in reverse to generate inputs based on the representations learned by the network.
- The results produced are fun and are somewhat similar to the visual artifacts induced in humans by the disruption of the visual cortex via psychedelics.
- Note that the process isn’t specific to image models or even to convnets. It can be done for speech, music, and more.

## Neural Style Transfer

In addition to DeepDream, another major development in deep-learning-driven image modification is neural style transfer.  The neural style transfer algorithm has undergone many refinements and spawned many variations since its original introduction, and it’s made its way into many smartphone photo apps. 

Neural style transfer consists of applying the style of a reference image to a target image while conserving the content of the target image.

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/neuralStyleTransfer1.jpg")
```

In this context, style essentially means textures, colors, and visual patterns in the image, at various spatial scales; and the content is the higher-level macro-structure of the image. The key notion behind implementing style transfer is the same idea that’s central to all deep-learning algorithms: you define a loss function to specify what you want to achieve, and you minimize this loss. You know what you want to achieve: conserving the content of the original image while adopting the style of the reference image. If you were able to mathematically define content and style, then an appropriate loss function to minimize would be the following:

```{r eval=FALSE}
loss <- distance(style(reference_image) - style(generated_image)) +
     distance(content(original_image) - content(generated_image))
```

Here, distance is a norm function such as the L2 norm, content is a function that takes an image and computes a representation of its content, and style is a function that takes an image and computes a representation of its style. Minimizing this loss causes style (generated_image) to be close to style(reference_image), and content(generated_image) is close to content(generated_image), thus achieving style transfer as we defined it.

### Content Loss

As you already know, activations from earlier layers in a network contain local information about the image, whereas activations from higher layers contain increasingly global, abstract information. Formulated in a different way, the activations of the different layers of a convnet provide a decomposition of the contents of an image over different spatial scales. Therefore, you’d expect the content of an image, which is more global and abstract, to be captured by the representations of the upper layers in a convnet.

A good candidate for content loss is the L2 norm between the activations of an upper layer in a pretrained convnet, computed over the target image, and the activations of the same layer computed over the generated image. This guarantees that, as seen from the upper layer, the generated image will look similar to the original target image. Assuming that what the upper layers of a convnet see is really the content of their input images, then this works as a way to preserve image content.

### Style Loss

The content loss only uses a single upper layer, but the style loss uses multiple layers of a convnet: you try to capture the appearance of the style reference image at all spatial scales extracted by the convnet, not just a single scale. For the style loss use the Gram matrix of a layer’s activations: the inner product of the feature maps of a given layer. This inner product can be understood as representing a map of the correlations between the layer’s features. These feature correlations capture the statistics of the patterns of a particular spatial scale, which empirically correspond to the appearance of the textures found at this scale.

Hence, the style loss aims to preserve similar internal correlations within the activations of different layers, across the style-reference image and the generated image. In turn, this guarantees that the textures found at different spatial scales look similar across the style-reference image and the generated image.

In short, you can use a pretrained convnet to define a loss that will do the following:

- Preserve content by maintaining similar high-level layer activations between the target content image and the generated image. The convnet should “see” both the target image and the generated image as containing the same things.
- Preserve style by maintaining similar correlations within activations for both low level layers and high-level layers. Feature correlations capture textures : the generated image and the style-reference image should share the same textures at different spatial scales.

### Neural Style Transfer in Keras

Neural style transfer can be implemented using any pretrained convnet. Here, use the `VGG19` network. `VGG19` is a simple variant of the `VGG16` network introduced earlier, with three more convolutional layers.

This is the general process:

1. Set up a network that computes `VGG19` layer activations for the style-reference image, the target image, and the generated image at the same time.
2.  Use the layer activations computed over these three images to define the loss function described earlier, which you’ll minimize in order to achieve style transfer.
3. Set up a gradient-descent process to minimize this loss function.

Start by defining the paths to the style-reference image and the target image. To make sure that the processed images are a similar size (widely different sizes make
style transfer more difficult), you’ll later resize them all to a shared height of 400 px.

Defining initial variables:
```{r}
target_image_path <- "./images/neuralStyleTransfer/bass.jpg" # Path to the image you want to transform

style_reference_image_path <- "images/neuralStyleTransfer/stlylePics/The_Great_Wave.jpg" # Path to the style image

# Calculates the dimensions of the generated picture
img <- image_load(target_image_path)
width <- img$size[[1]]
height <- img$size[[2]]
img_nrows <- 400
img_ncols <- as.integer(width * img_nrows / height)
```

Auxiliary functions for loading, preprocessing, and post-processing the images that go in and out of the VGG19 convnet:
```{r}
preprocess_image <- function(path) {
     img <- image_load(path, target_size = c(img_nrows, img_ncols)) %>%
          image_to_array() %>%
          array_reshape(c(1, dim(.)))
     imagenet_preprocess_input(img)}

deprocess_image <- function(x) {
     # Zero-centers by removing the mean pixel value from ImageNet. This reverses a transformation done by imagenet_preprocess_input
     x <- x[1,,,]
     x[,,1] <- x[,,1] + 103.939
     x[,,2] <- x[,,2] + 116.779
     x[,,3] <- x[,,3] + 123.68
     # Converts images from ‘BGR’ to ‘RGB’. This is also part of the reversal of imagenet_preprocess_input.
     x <- x[,,c(3,2,1)]
     x[x > 255] <- 255
     x[x < 0] <- 0
     x[] <- as.integer(x)/255
     x}
```

Set up the VGG19 network. It takes as input a batch of three images: 

1. the style reference
2. image
3. the target image
4. a placeholder that will contain the generated image. 

A placeholder is a symbolic tensor, the values of which are provided externally via R arrays. The style-reference and target image are static and thus defined using
`k_constant`, whereas the values contained in the placeholder of the generated image will change over time.

Loading the pretrained VGG19 network and applying it to the three images
```{r}
target_image <- k_constant(preprocess_image(target_image_path))
style_reference_image <- k_constant(preprocess_image(style_reference_image_path))

# Placeholder that will contain the generated image
combination_image <- k_placeholder(c(1, img_nrows, img_ncols, 3))

# Combines the three images in a single batch
input_tensor <- k_concatenate(list(target_image, style_reference_image, combination_image), axis = 1)

# Builds the VGG19 network with the batch of three images as input. The model will be loaded with pretrained ImageNet weights.
model <- application_vgg19(input_tensor = input_tensor, weights = "imagenet", include_top = FALSE)
cat("Model loaded\n")
```

Define the content loss, which will make sure the top layer of the VGG19 convnet has a similar view of the target image and the generated image.
```{r}
content_loss <- function(base, combination) {k_sum(k_square(combination - base))}
```

Next is the style loss. It uses an auxiliary function to compute the Gram matrix of an input matrix: a map of the correlations found in the original feature matrix.

```{r}
gram_matrix <- function(x) {
     features <- k_batch_flatten(k_permute_dimensions(x, c(3, 1, 2)))
     gram <- k_dot(features, k_transpose(features))
     gram}

style_loss <- function(style, combination){
     S <- gram_matrix(style)
     C <- gram_matrix(combination)
     channels <- 3
     size <- img_nrows*img_ncols
     k_sum(k_square(S - C)) / (4 * channels^2 * size^2)}
```

To these two loss components, you add a third: the total variation loss, which operates on the pixels of the generated combination image. It encourages spatial continuity in the generated image, thus avoiding overly pixelated results. You can interpret it as a regularization loss.

```{r}
total_variation_loss <- function(x) {
     y_ij <- x[,1:(img_nrows - 1L), 1:(img_ncols - 1L),]
     y_i1j <- x[,2:(img_nrows), 1:(img_ncols - 1L),]
     y_ij1 <- x[,1:(img_nrows - 1L), 2:(img_ncols),]
     a <- k_square(y_ij - y_i1j)
     b <- k_square(y_ij - y_ij1)
     k_sum(k_pow(a + b, 1.25))}
```

The loss that you minimize is a weighted average of these three losses. To compute the content loss, you use only one upper layer—the `block5_conv2` layer—whereas for the style loss, you use a list of layers than spans both low-level and high-level layers. You add the total variation loss at the end.

Depending on the style-reference image and content image you’re using, you’ll likely want to tune the `content_weight` coefficient (the contribution of the content loss to the total loss). _A higher `content_weight` means the target content will be more recognizable in the generated image_.

Defining the final loss that you’ll minimize
```{r}
# Named list that maps layer names to activation tensors
outputs_dict <- lapply(model$layers, `[[`, "output")
names(outputs_dict) <- lapply(model$layers, `[[`, "name")

# Layer used for content loss
content_layer <- "block5_conv2"

# Layers used for style loss
style_layers = c("block1_conv1", "block2_conv1", "block3_conv1", "block4_conv1", "block5_conv1")

total_variation_weight <- 1e-3 # original 1e-4
style_weight <- 4.0            # Weights in the weighted average of the loss components,  original 1.0
content_weight <- 0.001 # original 0.025

# You’ll define the loss by adding all components to this scalar variable.
loss <- k_variable(0.0)
layer_features <- outputs_dict[[content_layer]]
target_image_features <- layer_features[1,,,]
combination_features <- layer_features[3,,,]
loss <- loss + content_weight * content_loss(target_image_features, combination_features)

# Adds a style loss component for each target layer
for (layer_name in style_layers) {
     layer_features <- outputs_dict[[layer_name]]
     style_reference_features <- layer_features[2,,,]
     combination_features <- layer_features[3,,,]
     sl <- style_loss(style_reference_features, combination_features)
     loss <- loss + ((style_weight / length(style_layers)) * sl)}

# Adds the total variation loss
loss <- loss + (total_variation_weight * total_variation_loss(combination_image))
```

Finally, set up the gradient-descent process. In the original Gatys et al. paper, optimization is performed using the `L-BFGS` algorithm. This is a key difference from the DeepDream example. The `L-BFGS` algorithm is available via the `optim()` function, but there are two slight limitations with the `optim()` implementation:

- It requires that you pass the value of the loss function and the value of the gradients as two separate functions.
- It can only be applied to flat vectors, whereas you have a 3D image array.

It would be inefficient to compute the value of the loss function and the value of the gradients independently, because doing so would lead to a lot of redundant computation between the two; the process would be almost twice as slow as computing them jointly. To bypass this,  set up an R6 class named `Evaluator` that computes both the loss value and the gradients value at once, returns the loss value when called the first time, and caches the gradients for the next call.

Setting up the gradient-descent process
```{r}
# Gets the gradients of the generated image with regard to the loss
grads <- k_gradients(loss, combination_image)[[1]]

fetch_loss_and_grads <- k_function(list(combination_image), list(loss, grads))

# Function to fetch the values of the current loss and the current gradients
eval_loss_and_grads <- function(image) {
     image <- array_reshape(image, c(1, img_nrows, img_ncols, 3))
     outs <- fetch_loss_and_grads(list(image))
     list(
          loss_value = outs[[1]],
          grad_values = array_reshape(outs[[2]], dim = length(outs[[2]])))}

# This class wraps fetch_loss_and_grads in a way that allows you to retrieve the losses and gradients via two separate
# method calls, which is required by the optimizer you’ll use.
library(R6)
Evaluator <- R6Class("Evaluator", public = list(loss_value = NULL, grad_values = NULL,
                                                initialize = function() {
                                                     self$loss_value <- NULL
                                                     self$grad_values <- NULL},
                                                loss = function(x) {
                                                     loss_and_grad <- eval_loss_and_grads(x)
                                                     self$loss_value <- loss_and_grad$loss_value
                                                     self$grad_values <- loss_and_grad$grad_values
                                                     self$loss_value},
                                                grads = function(x) {
                                                     grad_values <- self$grad_values
                                                     self$loss_value <- NULL
                                                     self$grad_values <- NULL
                                                     grad_values}
                                                )
                     )
evaluator <- Evaluator$new()
```

Finally, run the gradient-ascent process using the `L-BFGS` algorithm, plotting the current generated image at each iteration of the algorithm (here, a single iteration represents 20 steps of gradient ascent).

```{r}
iterations <- 20
dms <- c(1, img_nrows, img_ncols, 3)

x <- preprocess_image(target_image_path)
x <- array_reshape(x, dim = length(x))

for (i in 1:iterations) {
     opt <- optim(
          array_reshape(x, dim = length(x)),
          fn = evaluator$loss,
          gr = evaluator$grads,
          method = "L-BFGS-B",
          control = list(maxit = 15))
     
cat("Loss:", opt$value, "\n")

image <- x <- opt$par
image <- array_reshape(image, dms)

im <- deprocess_image(image)
plot(as.raster(im))}
```

Keep in mind that what this technique achieves is merely a form of image retexturing, or texture transfer. It works best with style reference images that are strongly textured and highly self-similar, and with content targets that don’t require high levels of detail in order to be recognizable. It typically can’t achieve fairly abstract feats such as transferring the style of one portrait to another. The algorithm is closer to classical signal processing than to AI, so don’t expect it to work like magic!

Additionally, note that running this style-transfer algorithm is slow. But the transformation operated by the setup is simple enough that it can be learned by a small, fast, feedforward convnet as well—as long as you have appropriate training data available.

Fast style transfer can thus be achieved by first spending a lot of compute cycles to generate input-output training examples for a fixed style-reference image, using
the method outlined here, and then training a simple convnet to learn this style specific transformation. Once that’s done, stylizing a given image is instantaneous: it’s just a forward pass of this small convnet.

### Wrapping up

- Style transfer consists of creating a new image that preserves the contents of a target image while also capturing the style of a reference image.
- Content can be captured by the high-level activations of a convnet.
- Style can be captured by the internal correlations of the activations of different layers of a convnet.
- Hence, deep learning allows style transfer to be formulated as an optimization process using a loss defined with a pretrained convnet.
- Starting from this basic idea, many variants and refinements are possible.

https://arxiv.org/abs/1508.06576

https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398

## Images with Variational Autoencoders

Sampling from a latent space of images to create entirely new images or edit existing ones is currently the most popular and successful application of creative AI. There are two main techniques in this domain:

- variational autoencoders (VAEs)
- generative adversarial networks (GANs)

The techniques presented aren’t specific to images — you could develop latent spaces of sound, music, or even text, using GANs and VAEs—but in practice, the most interesting results have been obtained with pictures.

###  Sampling from Latent Spaces of Images

The key idea of image generation is to develop a low-dimensional latent space of representations (which naturally is a vector space) where any point can be mapped to a realistic-looking image. The module capable of realizing this mapping, taking as input a latent point and outputting an image (a grid of pixels), is called a _generator_ (in the case of GANs) or a _decoder_ (in the case of VAEs). Once such a latent space has been developed, you can sample points from it, either deliberately or at random, and, by mapping them to image space, generate images that have never been seen before.

GANs and VAEs are two different strategies for learning such latent spaces of image representations, each with its own characteristics. VAEs are great for learning latent spaces that are nicely structured, where specific directions encode a meaningful axis of variation in the data. GANs generate images that can potentially be highly realistic, but the latent space they come from may not have as much structure and continuity.

### Variational Autoencoders

Variational autoencoders, are a kind of generative model that’s especially appropriate for the task of image editing via concept vectors. They’re a modern take on autoencoders—a type of network that aims to encode an input to a low-dimensional latent space and then decode it back—that mixes ideas from deep learning with Bayesian inference.

A classical image autoencoder takes an image, maps it to a latent vector space via an encoder module, and then decodes it back to an output with the same dimensions as the original image, via a decoder module. It’s then trained by using as target data the same images as the input images, meaning the autoencoder learns to reconstruct the original inputs. By imposing various constraints on the code (the output of the encoder), you can get the autoencoder to learn interesting latent representations of the data. Most commonly, you’ll constrain the code to be low-dimensional and sparse (mostly zeros), in which case the encoder acts as a way to compress the input data into fewer bits of information.

In practice, such classical autoencoders don’t lead to particularly useful or well-structured latent spaces. They’re not much good at compression, either. For these reasons, they have largely fallen out of fashion. VAEs, however, augment autoencoders with a little bit of statistical magic that forces them to learn continuous, highly structured latent spaces. They have turned out to be a powerful tool for image generation.

A VAE, instead of compressing its input image into a fixed code in the latent space, turns the image into the parameters of a statistical distribution: a mean and a variance. Essentially, this means you’re assuming the input image has been generated by a statistical process, and that the randomness of this process should be taken into account during encoding and decoding. The VAE then uses the mean and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input. The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere: every point sampled in the latent space is decoded to a valid output.

In technical terms, here’s how a VAE works:

1. An encoder module turns the input samples `input_img` into two parameters in a latent space of representations, `z_mean` and `z_log_variance.`
2. You randomly sample a point `z` from the latent normal distribution that’s assumed to generate the input image, via 
$z = z\_mean + exp(z\_log\_variance) * epsilon$, where epsilon is a random tensor of small values.
3. A decoder module maps this point in the latent space back to the original input image.

Because epsilon is random, the process ensures that every point that’s close to the latent location where you encoded `input_img` (z-mean) can be decoded to something similar to `input_img`, thus forcing the latent space to be continuously meaningful. Any two close points in the latent space will decode to highly similar images. Continuity, combined with the low dimensionality of the latent space, forces every direction in the latent space to encode a meaningful axis of variation of the data, making the latent space very structured and thus highly suitable to manipulation via concept vectors.

The parameters of a VAE are trained via two loss functions: 

1. a reconstruction loss that forces the decoded samples to match the initial inputs
2. a regularization loss that helps learn well-formed latent spaces and reduce overfitting to the training data

The following listing shows the encoder network you’ll use, mapping images to the parameters of a probability distribution over the latent space. It’s a simple convnet that maps the input image `x` to two vectors, `z_mean` and `z_log_var.`

```{r eval=FALSE}
img_shape <- c(28, 28, 1)
batch_size <- 16
latent_dim <- 2L # Dimensionality of the latent space: a 2D plane

input_img <- layer_input(shape = img_shape)

x <- input_img %>% layer_conv_2d(filters = 32, kernel_size = 3, padding = "same", activation = "relu") %>%
     layer_conv_2d(filters = 64, kernel_size = 3, padding = "same", activation = "relu", strides = c(2, 2)) %>%
     layer_conv_2d(filters = 64, kernel_size = 3, padding = "same", activation = "relu") %>%
     layer_conv_2d(filters = 64, kernel_size = 3, padding = "same", activation = "relu")

shape_before_flattening <- k_int_shape(x)

x <- x %>% layer_flatten() %>% layer_dense(units = 32, activation = "relu")

z_mean <- x %>% layer_dense(units = latent_dim)    # The input image ends upbeing encoded into these two parameters.
z_log_var <- x %>% layer_dense(units = latent_dim)
```

Next is the code for using `z_mean` and `z_log_var`, the parameters of the statistical distribution assumed to have produced `input_img`, to generate a latent space point `z`. Here, you wrap some arbitrary code (built on top of Keras backend primitives) into a `layer_lambda`, which wraps an R function into a layer. In Keras, everything needs to be a layer, so code that isn’t part of a built-in layer should be wrapped in a `layer_lambda` (or in a custom layer).

```{r eval=FALSE}
sampling <- function(args) {
     c(z_mean, z_log_var) %<-% args
     epsilon <- k_random_normal(shape = list(k_shape(z_mean)[1], latent_dim), mean = 0, stddev = 1)
     z_mean + k_exp(z_log_var) * epsilon}

z <- list(z_mean, z_log_var) %>% layer_lambda(sampling)
```

The following listing shows the decoder implementation. You reshape the vector `z` to the dimensions of an image and then use a few
convolution layers to obtain a final image output that has the same dimensions as the original `input_img.`

```{r eval=FALSE}
decoder_input <- layer_input(k_int_shape(z)[-1])

# Input where you’ll feed z
x <- decoder_input %>%
     
     # Upsamples the input
     layer_dense(units = prod(as.integer(shape_before_flattening[-1])), activation = "relu") %>%
     
     # Reshapes z into a feature map of the same shape as the feature map just before the last layer_flatten in the encoder model
     # Uses a layer_conv_2d_transpose and layer_conv_2d to decode z into a feature map the same size as the original image input
     layer_reshape(target_shape = shape_before_flattening[-1]) %>%
     layer_conv_2d_transpose(filters = 32, kernel_size = 3, padding = "same", activation = "relu", strides = c(2, 2)) %>%
     
     # You end up with a feature map the same size as the original input.
     layer_conv_2d(filters = 1, kernel_size = 3, padding = "same", activation = "sigmoid")

decoder <- keras_model(decoder_input, x) # Instantiates the decoder model,which turns “decoder_input” into the decoded image

z_decoded <- decoder(z)
```

The dual loss of a VAE doesn’t fit the traditional expectation of a sample-wise function of the form loss(input, target). Thus, you’ll set up the loss by writing a custom layer that internally uses the built-in add_loss layer method to create an arbitrary loss.

```{r eval=FALSE}
library(R6)

CustomVariationalLayer <- R6Class("CustomVariationalLayer", inherit = KerasLayer, public = list(
     vae_loss = function(x, z_decoded) {
          x <- k_flatten(x)
          z_decoded <- k_flatten(z_decoded)
          xent_loss <- metric_binary_crossentropy(x, z_decoded)
          kl_loss <- -5e-4 * k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var),axis = -1L)
          
          k_mean(xent_loss + kl_loss)},
     
     # Custom layers are implemented by writing a “call” method.
     call = function(inputs, mask = NULL) {
          x <- inputs[[1]]
          z_decoded <- inputs[[2]]
          loss <- self$vae_loss(x, z_decoded)
          self$add_loss(loss, inputs = inputs)
          x} # You don’t use this output, but the layer must return something.
     )
)

# Wraps the R6 class in a standard Keras layer function
layer_variational <- function(object) {create_layer(CustomVariationalLayer, object, list())}

# Calls the custom layer on the input and the decoded output to obtain the final model output
y <- list(input_img, z_decoded) %>% layer_variational()
```

Finally, you’re ready to instantiate and train the model. Because the loss is taken care of in the custom layer, you don’t specify an external loss at compile time (`loss = NULL`), which in turn means you won’t pass target data during training (as you can see, you only pass `x_train` to the model in `fit()`).

```{r eval=FALSE}
vae <- keras_model(input_img, y)

vae %>% compile(optimizer = "rmsprop", loss = NULL)

mnist <- dataset_mnist()
c(c(x_train, y_train), c(x_test, y_test)) %<-% mnist

x_train <- x_train / 255
x_train <- array_reshape(x_train, dim =c(dim(x_train), 1))

x_test <- x_test / 255
x_test <- array_reshape(x_test, dim =c(dim(x_test), 1))

vae %>% fit(x = x_train, y = NULL, epochs = 10, batch_size = batch_size, validation_data = list(x_test, NULL))
```

```{r eval=FALSE, echo=FALSE}
load("./RData/VAE.RData")
```

Once such a model is trained on MNIST, in this case—you can use the decoder network to turn arbitrary latent space vectors into images.

```{r eval=FALSE}
# You’ll display a grid of 15 × 15 digits (255 digits in total).
n <- 15
digit_size <- 28

# Transforms linearly spaced coordinates using the qnorm function to produce values of the latent variable z (because the prior of the latent space is Gaussian)
grid_x <- qnorm(seq(0.05, 0.95, length.out = n))
grid_y <- qnorm(seq(0.05, 0.95, length.out = n))

op <- par(mfrow = c(n, n), mar = c(0,0,0,0), bg = "black")
for (i in 1:length(grid_x)) {
     yi <- grid_x[[i]]
     for (j in 1:length(grid_y)) {
          xi <- grid_y[[j]]
          z_sample <- matrix(c(xi, yi), nrow = 1, ncol = 2)
          
          # Repeats z multiple times to form a complete batch
          z_sample <- t(replicate(batch_size, z_sample, simplify = "matrix"))
          x_decoded <- decoder %>% predict(z_sample, batch_size = batch_size)
          
          # Reshapes the first digit in the batch from 28 × 28 × 1 to 28 × 28
          digit <- array_reshape(x_decoded[1,,,], dim = c(digit_size, digit_size))
          plot(as.raster(digit))
     }
}
par(op)
```

```{r, out.width = "4000px", echo=FALSE}
knitr::include_graphics("./images/VAE.JPG")
```

The grid of sampled digits shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a path through latent space. Specific directions in this space have a meaning: for example, there’s a direction for “four-ness,” “one-ness,” and so on.

### Wrapping up

- Image generation with deep learning is done by learning latent spaces that capture statistical information about a dataset of images. By sampling and decoding points from the latent space, you can generate never-before-seen images. There are two major tools to do this: VAEs and GANs.
     - VAEs result in highly structured, continuous latent representations. For this reason, they work well for doing all sorts of image editing in latent space: face swapping, turning a frowning face into a smiling face, and so on. They also work nicely for doing latent-space-based animations, such as animating a walk along a cross section of the latent space, showing a starting image slowly morphing into different images in a continuous way.
     - GANs enable the generation of realistic single-frame images but may not induce latent spaces with solid structure and high continuity.
     
## Generative Adversarial Networks

Generative adversarial networks (GANs) are an alternative to VAEs for learning latent spaces of images. They enable the generation of fairly realistic synthetic images by forcing the generated images to be statistically almost indistinguishable from real ones.

A GAN is a forger network and an expert network, each being trained to best the other. As such, a GAN is made of two parts:

1. Generator network — Takes as input a random vector (a random point in the latent space), and decodes it into a synthetic image
2. Discriminator network (or adversary) — Takes as input an image (real or synthetic), and predicts whether the image came from the training set or was created by the generator network.

The generator network is trained to be able to fool the discriminator network, and thus it evolves toward generating increasingly realistic images as training goes on: artificial images that look indistinguishable from real ones, to the extent that it’s impossible for the discriminator network to tell the two apart. Meanwhile, the discriminator is constantly adapting to the gradually improving capabilities of the generator, setting a high bar of realism for the generated images. Once training is over, the generator is capable of turning any point in its input space into a believable image. Unlike VAEs, this latent space has fewer explicit guarantees of meaningful structure; in particular, it isn’t continuous.

Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike in any other training setup you’ve encountered herein. Normally, gradient descent consists of rolling down hills in a static loss landscape. But with a GAN, every step taken down the hill changes the entire landscape a little. It’s a dynamic system where the optimization process is seeking not a minimum, but an equilibrium between two forces. For this reason, _gans are notoriously difficult to train — getting a GAN to work requires lots of careful tuning of the model architecture and training parameters._

A good article:  https://medium.com/beyondminds/advances-in-generative-adversarial-networks-7bad57028032

### A Schematic GAN

The specific implementation is a deep convolutional GAN (DCGAN): a GAN where the generator and discriminator are deep convnets. In particular, it uses `layer_conv_2d_transpose` for image upsampling in the generator.

Train the GAN on images from `CIFAR10`, a dataset of 50,000 32 × 32 RGB images belonging to 10 classes (5,000 images per class). To make things easier, only use images belonging to the class _frog._

Schematically, the GAN looks like this:

1. A generator network maps vectors of shape (`latent_dim`) to images of shape (32, 32, 3).
2. A discriminator network maps images of shape (32, 32, 3) to a binary score estimating the probability that the image is real.
3. A GAN network chains the generator and the discriminator together: `gan(x) <- discriminator(generator(x))`. This GAN network maps latent space vectors to the discriminator’s assessment of the realism of these latent vectors as decoded by the generator.
4. Train the discriminator using examples of real and fake images along with “real”/”fake” labels, just as you train any regular image-classification model.
5. To train the generator, use the gradients of the generator’s weights with regard to the loss of the GAN model. This means, at every step, you move the weights of the generator in a direction that makes the discriminator more likely
to classify as “real” the images decoded by the generator. In other words, you train the generator to fool the discriminator.

### A bag of tricks
The process of training GANs and tuning GAN implementations is notoriously difficult. There are a number of known tricks you should keep in mind. Like most things in deep learning, it’s more alchemy than science: these tricks are heuristics, not
_theory-backed_ guidelines. They’re supported by a level of intuitive understanding of the phenomenon at hand, and they’re known to work well empirically, although not necessarily in every context.

Here are a few of the tricks used in the implementation of the GAN generator and discriminator in this section. It isn’t an exhaustive list of GAN-related tips; you’ll find many more across the GAN literature:

- Use `tanh` as the last activation in the generator, instead of `sigmoid`, which is more commonly found in other types of models.
- Sample points from the latent space using a normal distribution (Gaussian distribution), not a uniform distribution.
- Stochasticity is good to induce robustness. Because GAN training results in a dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing randomness during training helps prevent this. We introduce randomness in two ways:

     - by using dropout in the discriminator
     - by adding random noise to the labels for the discriminator
- Sparse gradients can hinder GAN training. In deep learning, sparsity is often a desirable property, but not in GANs. Two things can induce gradient sparsity:
     - max-pooling operations
     - ReLU activations. 
     Instead of max pooling, we recommend using strided convolutions for downsampling, and we recommend using `layer_activation_leaky_relu` instead of a ReLU activation. It’s similar to ReLU, but it relaxes sparsity constraints by allowing small negative activation values.
-  In generated images, it’s common to see checkerboard artifacts caused by unequal coverage of the pixel space in the generator. To fix this, use a kernel size that’s divisible by the stride size whenever we use a strided `layer_conv_2d_transpose` or `layer_conv_2d` in both the generator and the discriminator.

### The Generator

First develop a generator model that turns a vector (from the latent space during training it will be sampled at random) into a candidate image. One of the many issues that commonly arise with GANs is that the generator gets stuck with generated
images that look like noise. A possible solution is to use dropout on both the discriminator and the generator.

```{r GAN_Generator}

latent_dim <- 32
height <- 32
width <- 32
channels <- 3

generator_input <- layer_input(shape = c(latent_dim))

# Transforms the input intoa 16 × 16, 128-channel feature map
generator_output <- generator_input %>% layer_dense(units = 128 * 16 * 16) %>%
     layer_activation_leaky_relu() %>%
     layer_reshape(target_shape = c(16, 16, 128)) %>%
     layer_conv_2d(filters = 256, kernel_size = 5,padding = "same") %>%
     layer_activation_leaky_relu() %>%
     
     # Upsamples to 32 × 32
     layer_conv_2d_transpose(filters = 256, kernel_size = 4, strides = 2, padding = "same") %>%
     layer_activation_leaky_relu() %>%
     layer_conv_2d(filters = 256, kernel_size = 5, padding = "same") %>%
     layer_activation_leaky_relu() %>%
     layer_conv_2d(filters = 256, kernel_size = 5, padding = "same") %>%
     layer_activation_leaky_relu() %>%
     
     # Produces a 32 × 32, 3-channel feature map (shape of a CIFAR10 image)
     layer_conv_2d(filters = channels, kernel_size = 7, activation = "tanh", padding = "same")

# Instantiates the generator model, which maps the input of shape (latent_dim) into an image of shape (32, 32, 3)
generator <- keras_model(generator_input, generator_output)
```

### The Discriminator

Develop a discriminator model that takes as input a candidate image (real or synthetic) and classifies it into one of two classes: `generated image` or `real image that comes from the training set`.

```{r}
discriminator_input <- layer_input(shape = c(height, width, channels))

discriminator_output <- discriminator_input %>%
     layer_conv_2d(filters = 128, kernel_size = 3) %>%
     layer_activation_leaky_relu() %>%
     layer_conv_2d(filters = 128, kernel_size = 4, strides = 2) %>%
     layer_activation_leaky_relu() %>%
     layer_conv_2d(filters = 128, kernel_size = 4, strides = 2) %>%
     layer_activation_leaky_relu() %>%
     layer_conv_2d(filters = 128, kernel_size = 4, strides = 2) %>%
     layer_activation_leaky_relu() %>%
     layer_flatten() %>%
     
     # One dropout layer: an important trick!
     layer_dropout(rate = 0.4) %>%
     
     # Classification layer
     layer_dense(units = 1, activation = "sigmoid")

# Instantiates the discriminator model, which turns a (32, 32, 3) input into a binary classification decision (fake/real)
discriminator <- keras_model(discriminator_input, discriminator_output)

# Uses gradient clipping (by value) in the optimizer
# To stabilize training, uses `learning-rate` decay
discriminator_optimizer <- optimizer_rmsprop(lr = 0.0008, clipvalue = 1.0, decay = 1e-8)

discriminator %>% compile(optimizer = discriminator_optimizer, loss = "binary_crossentropy")
```

### The Adversarial Network

Set up the GAN, which chains the generator and the discriminator. When trained, this model will move the generator in a direction that improves its ability to fool the discriminator. This model turns latent-space points into a classification decision— _fake_ or _real_ and it’s meant to be trained with labels that are always t_hese are real images_. So, training GAN will update the weights of generator in a way that makes discriminator more likely to predict _real_ when looking at fake images.

It’s very important to note that you set the discriminator to be frozen during training (non-trainable): its weights won’t be updated when training GAN. If the discriminator weights could be updated during this process, then you’d be training the discriminator to always predict _real_, which is not what you want!

```{r}
freeze_weights(discriminator)

gan_input <- layer_input(shape = c(latent_dim))
gan_output <- discriminator(generator(gan_input))
gan <- keras_model(gan_input, gan_output)

gan_optimizer <- optimizer_rmsprop(lr = 0.0004, clipvalue = 1.0, decay = 1e-8)

gan %>% compile( optimizer = gan_optimizer, loss = "binary_crossentropy")
```

### Train DCGAN

Now you can begin training. This is what the training loop looks like schematically. For each epoch, you do the following:

1. Draw random points in the latent space (random noise).
2. Generate images with generator using this random noise.
3. Mix the generated images with real ones.
4. Train discriminator using these mixed images, with corresponding targets:
     - _real_ (for the real images)
     - _fake_ (for the generated images)
     
5. Draw new random points in the latent space.
6.  Train GAN using these random vectors, with targets that all say _these are real images_. This updates the weights of the generator (only, because the discriminator is frozen inside GAN) to move them toward getting the discriminator to predict _these are real images_ for generated images: this trains the generator to fool the discriminator.

```{r}

cifar10 <- dataset_cifar10() # Loads CIFAR10 data
c(c(x_train, y_train), c(x_test, y_test)) %<-% cifar10

x_train <- x_train[as.integer(y_train) == 6,,,] # Selects frog images (class 6)
x_train <- x_train / 255 # Normalizes data

iterations <- 10000
batch_size <- 20
save_dir <- "your_dir" # Specifies where you want to save generated images

start <- 1

for (step in 1:iterations) {
     
     # Decodes them to fake images
     random_latent_vectors <- matrix(rnorm(batch_size * latent_dim), nrow = batch_size, ncol = latent_dim)
     
     # Samples random points in the latent space
     generated_images <- generator %>% predict(random_latent_vectors)
     
     # Combines them with real images
     stop <- start + batch_size - 1
     real_images <- x_train[start:stop,,,]
     rows <- nrow(real_images)
     combined_images <- array(0, dim = c(rows * 2, dim(real_images)[-1]))
     combined_images[1:rows,,,] <- generated_images
     combined_images[(rows+1):(rows*2),,,] <- real_images
     
     # Assembles labels, discriminating real from fake images
     labels <- rbind(matrix(1, nrow = batch_size, ncol = 1), matrix(0, nrow = batch_size, ncol = 1))
     
     # Adds random noise to the labels—an important trick
     labels <- labels + (0.5 * array(runif(prod(dim(labels))), dim = dim(labels)))
     
     # Trains the discriminator
     d_loss <- discriminator %>% train_on_batch(combined_images, labels)
     
     # Samples random points in the latent space
     random_latent_vectors <- matrix(rnorm(batch_size * latent_dim), nrow = batch_size, ncol = latent_dim)
     
     # Assembles labels that say “these are all real images” (it’s a lie!)
     misleading_targets <- array(0, dim = c(batch_size, 1))
     
     # Trains the generator (via the GAN model, where the discriminator weights are frozen)
     a_loss <- gan %>% train_on_batch(random_latent_vectors,misleading_targets)
     
     start <- start + batch_size
     if (start > (nrow(x_train) - batch_size))
          start <- 1
     
     # Occasionallysaves images
     if (step %% 100 == 0) {
          save_model_weights_hdf5(gan, "gan.h5")   # Saves model weights
          cat("discriminator loss:", d_loss, "\n")
          cat("adversarial loss:", a_loss, "\n")
          
          # Saves one generated image
          image_array_save(generated_images[1,,,] * 255, 
                           path = file.path(save_dir, paste0("generated_frog", step, ".png")))
          
          # Saves one real image for comparison
          image_array_save(real_images[1,,,] * 255,
                           path = file.path(save_dir, paste0("real_frog", step, ".png"))
                           )
          }
}
```

> When training, you may see the adversarial loss begin to increase considerably, while the discriminative loss tends to zero—the discriminator may end up dominating the generator. If that’s the case, try reducing the discriminator learning rate, and increase the dropout rate of the discriminator.

### Wrapping up

- A GAN consists of a generator network coupled with a discriminator network. The discriminator is trained to differentiate between the output of the generator and real images from a training dataset, and the generator is trained to fool the discriminator. Remarkably, the generator never sees images from the training set directly; the information it has about the data comes from the discriminator.
- GANs are difficult to train, because training a GAN is a dynamic process rather than a simple gradient-descent process with a fixed loss landscape. Getting a GAN to train correctly requires using a number of heuristic tricks, as well as extensive tuning.
- GANs can potentially produce highly realistic images. But unlike VAEs, the latent space they learn doesn’t have a neat continuous structure and thus may not be suited for certain practical applications, such as image editing via latent space concept vectors.

## Summary

- With creative applications of deep learning, deep networks go beyond annotating existing content and start generating their own. You learned the following:
     - How to generate sequence data, one timestep at a time. This is applicable to text generation and also to note-by-note music generation or any other type of timeseries data.
     - How DeepDream works: by maximizing convnet layer activations through gradient ascent in input space.
     – How to perform style transfer, where a content image and a style image are combined to produce interesting-looking
     results.
     – What GANs and VAEs are, how they can be used to dream up new images, and how latent-space concept vectors can be used for image editing.
- These few techniques cover only the basics of this fast-expanding field. There’s a lot more to discover out there—generative deep learning is deserving of an entire book of its own.

p 284