---
title: "Deep Learnintg with R Appendix"
output:
  html_document:
    highlight: pygments
    theme: spacelab
    toc: yes
header-includes: \usepackage{graphicx} \usepackage{mathtools}
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
if(!require(easypackages)){
    install.packages("easypackages")
    library(easypackages)
}
packages("caret", "nnet", prompt = FALSE)
```

## Appendix

### Brief caret Explanation

The Caret package lets you train different models and tuning hyper-parameters using Cross Validation (Hold-Out or K-fold) or Bootstrap. 

There are two different ways to tune the hyper-parameters using Caret: Grid Search and Random Search. If you use Grid Search (Brute Force) you need to define the grid for every parameter according to your prior knowledge or you can fix some parameters and iterate on the remain ones. If you use Random Search you need to specify a tuning length (maximum number of iterations) and Caret is going to use random values for hyper-parameters until the stop criteria holds. 

No matter what method you choose Caret is going to use each combination of hyper-parameters to train the model and compute performance metrics as follows:

1. Split the initial Training samples into two different sets: Training and Validation (For bootstrap or Cross validation) and into k sets (For k-fold Cross Validation).
2. Train the model using the training set and to predict on validation set (For Cross Validation Hold-Out and Bootstrap). Or using k-1 training sets and to predict using the k-th training set (For K-fold Cross Validation).
3. On the validation set Caret computes some performance metrics as ROC, Accuracy...
4. Once the Grid Search has finished or the Tune Length is completed Caret uses the performance metrics to select the best model according to the criteria previously defined (You can use ROC, Accuracy, Sensibility, RSquared, RMSE....)
5. You can create some plot to understand the resampling profile and to pick the best model (Keep in mind performance and complexity)

### H~2~O Install

H2O is a machine learning framework. H2O is  open source and what makes it important is that works right of the box. It supports for R, Python, Scala, Java and also has a REST API and a own WebUI. So you can use it perfectly for research but also in production environments.

H2O is based on Apache Hadoop and Apache Spark which gives it enormous power with in-memory parallel processing.

Super simple to get started:

H~2~O is constantly updated.  Might want to remove and reinstall it.

```{r optional}
# The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
 
# Next, we download, install and initialize the H2O package for R.
install.packages("h2o", repos=(c("http://s3.amazonaws.com/h2o-release/h2o/rel-kahan/5/R", getOption("repos"))))
 
library(h2o)
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)

#Example functions
#h2oiris <- as.h2o(droplevels(iris[1:100, ]))
# h2o.levels(h2oiris, 5)
# h2omtcars <- h2o.importFile(path = "mtcars.csv")
# h2obin <- h2o.importFile(path = "http://www.ats.ucla.edu/stat/data/binary.csv")
```

> Default port: 54321 --> 127.0.0.1:54321


#### H~2~O Ensemble Learning Example

```{r}
data("Seatbelts")
summary(Seatbelts)
dim(Seatbelts)
trainHex <- as.h2o(Seatbelts)
x_names  <- colnames(trainHex[2:8])
myModel <- h2o.deeplearning(x = x_names, y = "DriversKilled", training_frame = trainHex)
myModel
```

```{r}
h2o.shutdown(prompt = FALSE)
```

### Plot Code Comparing In Sample and Out Sample Model Performance

```{r eval=FALSE}
digits.yhat1.pred <- predict(digits.m1, digits.train[i2, -1])
digits.yhat2.pred <- predict(digits.m2, digits.train[i2, -1])
digits.yhat3.pred <- predict(digits.m3, digits.train[i2, -1])


measures <- c("AccuracyNull", "Accuracy", "AccuracyLower", "AccuracyUpper")

n5.insample <- caret::confusionMatrix(xtabs(~digits.y + digits.yhat1))
n5.outsample <- caret::confusionMatrix(xtabs(~digits.train[i2, 1] + digits.yhat1.pred))

n10.insample <- caret::confusionMatrix(xtabs(~digits.y + digits.yhat2))
n10.outsample <- caret::confusionMatrix(xtabs(~digits.train[i2, 1] + digits.yhat2.pred))

n40.insample <- caret::confusionMatrix(xtabs(~digits.y + digits.yhat3))
n40.outsample <- caret::confusionMatrix(xtabs(~digits.train[i2, 1] + digits.yhat3.pred))

n40b.insample <- caret::confusionMatrix(xtabs(~digits.y + I(digits.yhat4 - 1)))
n40b.outsample <- caret::confusionMatrix(xtabs(~digits.train[i2, 1] +
  I(encodeClassLabels(digits.yhat4.pred) - 1)))

## results
shrinkage <- rbind(
  cbind(Size = 5, Sample = "In", as.data.frame(t(n5.insample$overall[measures]))),
  cbind(Size = 5, Sample = "Out", as.data.frame(t(n5.outsample$overall[measures]))),
  cbind(Size = 10, Sample = "In", as.data.frame(t(n10.insample$overall[measures]))),
  cbind(Size = 10, Sample = "Out", as.data.frame(t(n10.outsample$overall[measures]))),
  cbind(Size = 40, Sample = "In", as.data.frame(t(n40.insample$overall[measures]))),
  cbind(Size = 40, Sample = "Out", as.data.frame(t(n40.outsample$overall[measures]))),
  cbind(Size = 40, Sample = "In", as.data.frame(t(n40b.insample$overall[measures]))),
  cbind(Size = 40, Sample = "Out", as.data.frame(t(n40b.outsample$overall[measures])))
  )
shrinkage$Pkg <- rep(c("nnet", "RSNNS"), c(6, 2))

dodge <- position_dodge(width=0.4)

p.shrinkage <- ggplot(shrinkage, aes(interaction(Size, Pkg, sep = " : "), Accuracy,
                      ymin = AccuracyLower, ymax = AccuracyUpper,
                      shape = Sample, linetype = Sample)) +
  geom_point(size = 2.5, position = dodge) +
  geom_errorbar(width = .25, position = dodge) +
  xlab("") + ylab("Accuracy + 95% CI") +
  theme_classic() +
  theme(legend.key.size = unit(1, "cm"), legend.position = c(.8, .2))
```

#### Save H~2~0 Model

It is easy to save models in R but, when calling H2O from R, most results are not actually stored in R; instead they are stored in the H2O cluster. Thus, only saving the R object will merely save the reference to the model in the H2O cluster and, if that is shut down and lost, the full model results will not be saved. 

To avoid this and save the full model results, use `h2o.saveModel()` and specify the model to be saved (by passing the R object), the path, and whether to overwrite files if already there (using force = TRUE):

```{r exampleSaveH2O, eval=FALSE}
h2o.saveModel(object = m2, path = "c:\\Users\\jwile\\DeepLearning", force = TRUE) 
```

This will create a directory with all of the files needed to load and use the model again. Once you have saved a model, you can load it back into a new H2O cluster using `h2o.loadModel()`. Also specify the folder name for the model results to load. 

> H~2~O models can be saved as a Plain Old Java Object (POJO). Saving models as a POJO is useful as they can be embedded in other applications and used to score results. H2O models can be saved as a POJO using the h2o.download_pojo() function, with the same arguments. 

#### `h20.scoreHistory`

The score history shows the performance of the model across iterations as well as a time stamp and the duration for each epoch.  Example:

```{r}
h2o.scoreHistory(m2)
```

Although this is a useful summary, it provides less than a complete picture. Examining the model residuals can help us understand whether the model performs consistently across the range of the data and any anomalous residuals; it also helps us to generally assess performance more comprehensively.  

Calculate residuals by getting predicted values for all cases using `h2o.predict()` and  taking the difference between the observed values and the predictions. The following code extracts predictions, joins them with observed values, and plots them. A residual of zero indicates a perfect prediction, with either positive or negative residuals indicating over- or under-prediction. Since years are discrete, we can visualize the data using boxplots of the residuals for each actual year of song release.  This is borrowing the code from the 3rd walkthru.

```{r}
yhat <- as.data.frame(h2o.predict(m2, h2omsd.train))
yhat <- cbind(as.data.frame(h2omsd.train[["V1"]]), yhat)

p.resid <- ggplot(yhat, aes(factor(V1), predict - V1)) + geom_boxplot() + geom_hline(yintercept = 0) + theme_classic() +
     theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0)) + xlab("Year of Release") + 
     ylab("Residual (Predicted - Actual Year of Release)")
print(p.resid)
```

The results show a marked pattern of decreasing residuals in later years or, conversely, show extremely aberrant model predictions for the earlier years. In part, this may be due to the distribution of the data. With most cases coming from the mid 1990s to 2000s, the model will be most sensitive to accurately predicting these values, and the comparatively fewer cases before 1990 or 1980 will have less influence.

Because we used `variable_importances` argument, we can extract the relative importance of each variable for the model using `h2o.varimp()`. Although it is difficult to accurately apportion the importance of each variable, it can be helpful to provide a rough sense of which variables tend to make a larger contribution to the prediction than others. This can be a helpful way to exclude some variables that contribute very little, for example. The following code extracts the important variables, prints the top 10 (the dataset is sorted from most to least important) and makes a graph of the results to display the distribution.

```{r}
imp <- as.data.frame(h2o.varimp(m2))
imp[1:10, ]

p.imp <- ggplot(imp, aes(factor(variable, levels = variable), percentage)) + geom_point() + theme_classic() + theme(axis.text.x = element_blank()) +
     xlab("Variable Number") + ylab("Percentage of Total Importance")
print(p.imp)
```

From the description of the dataset, the first 12 variables represented various timbres of the music, with the next 78 being the unique elements of a covariance matrix from the first 12. Thus it is interesting that, in the top variables, the first three are all the timbres, not from the covariances. If, for example, the later 78 variables were costly or difficult to collect, we might consider what performance is possible using only the first 12 predictors. The following model tests that approach using a simple shallow model.

```{r}
mtest <- h2o.deeplearning(x = colnames(d)[2:13], y = "V1", training_frame= h2omsd.train, validation_frame = h2omsd.test,
                activation = "RectifierWithDropout", hidden = c(50), epochs = 100, input_dropout_ratio = 0, hidden_dropout_ratios = c(0),
                score_training_samples = 0, score_validation_samples = 0, diagnostics = TRUE, export_weights_and_biases = TRUE,
                variable_importances = TRUE)
mtest
```

