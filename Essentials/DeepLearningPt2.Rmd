---
title: "Deep Learning with R Part 2"
output:
  html_document:
    highlight: pygments
    theme: spacelab
    toc: yes
header-includes: \usepackage{graphicx} \usepackage{mathtools}
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
#Remove all packages loaded in environment
try(lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE), silent = TRUE)
#Apply packages
if(!require(easypackages)){
    install.packages("easypackages")
    library(easypackages)
}
packages("parallel", "foreach", "doSNOW", prompt = FALSE)

```

## Example Walkthrough

### The Data

Experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The  dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. 

For each record in the dataset it is provided: 

- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration. 
- Triaxial Angular velocity from the gyroscope. 
- A 561-feature vector with time and frequency domain variables. 
- Its activity label. 
- An identifier of the subject who carried out the experiment. 

```{r getDataWalkthru}
use.train.x <- read.table("../Essentials/data/X_train.txt")
use.train.y <- read.table("../Essentials/data/y_train.txt")[[1]]
use.test.x <- read.table("../Essentials/data/X_test.txt")
use.test.y <- read.table("../Essentials/data/y_test.txt")[[1]]
use.labels <- read.table("../Essentials/data/activity_labels.txt")
barplot(table(use.train.y))
```

Evaluate a variety of tuning parameters to experiment with different approaches to get the best model. Evaluate the models using different tuning parameters simultaneously using parallel processing.

Pick tuning parameters and set up a local cluster as the backend for `foreach` for parallel for loops. If you do this on a machine with fewer than five cores, change makeCluster(5) to a lower number.

```{r tuningParams, echo=FALSE}
tuning <- list(size = c(40, 20, 20, 50, 50), maxit = c(60, 100, 100, 100, 100),
               shuffle = c(FALSE, FALSE, TRUE, FALSE, FALSE),
               params = list(FALSE, FALSE, FALSE, FALSE, c(0.1, 20, 3)))
## setup cluster using 2 cores
## register as a backend for use with the foreach package
cl <- makeCluster(2)
clusterEvalQ(cl, {library(RSNNS)})
clusterExport(cl, c("tuning", "use.train.x", "use.train.y", "use.test.x", "use.test.y"))
registerDoSNOW(cl)
```

Train all the models. Use `parallel` for loop, setting some of the arguments based on the tuning parameters we previously stored in the list.

```{r}
# use.models <- foreach(i = 1:5, .combine = 'c') %dopar% {
#      if (tuning$params[[i]][1]) {
#           set.seed(9876)
#           list(Model = mlp(as.matrix(use.train.x), decodeClassLabels(use.train.y),
#                            size = tuning$size[[i]], learnFunc = "Rprop",
#                            shufflePatterns = tuning$shuffle[[i]],
#                            learnFuncParams = tuning$params[[i]],
#                            maxit = tuning$maxit[[i]]))
#           } else {
#                set.seed(9876)
#                list(Model = mlp(as.matrix(use.train.x), decodeClassLabels(use.train.y),
#                                 size = tuning$size[[i]], learnFunc = "Rprop",
#                                 shufflePatterns = tuning$shuffle[[i]],
#                                 maxit = tuning$maxit[[i]]))
#                }
# }
# save(use.models, file="../Essentials/usemodel.RData")
load("../Essentials/usemodel.RData")
```

Generating out-of-sample predictions will take time in parallel too. Export the model results to each of the workers on the cluster and calculate the predictions.

```{r}
# clusterExport(cl, "use.models")
# use.yhat <- foreach(i = 1:5, .combine = 'c') %dopar% {
#      list(list(Insample = encodeClassLabels(fitted.values(use.models[[i]])),
#                Outsample = encodeClassLabels(predict(use.models[[i]],
#                newdata = as.matrix(use.test.x)))
# ))
# }
# save(use.yhat, file="../Essentials/useyhat.RData")
load("../Essentials/useyhat.RData")
```

Merge the actual and fitted or predicted values together, calculate performance measures on each and store the results.

Additional data management is required because sometimes a model may not predict each possible response level, but this can make for non-symmetrical frequency cross tabs, unless we convert the variable to a factor and specify the levels. Also drop 0 values which indicate the model was uncertain how to classify an observation'

```{r walkthuResults}
use.insample <- cbind(Y = use.train.y, do.call(cbind.data.frame, lapply(use.yhat, `[[`, "Insample")))
colnames(use.insample) <- c("Y", paste0("Yhat", 1:5))
performance.insample <- do.call(rbind, lapply(1:5, function(i) {
     f <- substitute(~ Y + x, list(x = as.name(paste0("Yhat", i))))
     use.dat <- use.insample[use.insample[,paste0("Yhat", i)] != 0, ]
     use.dat$Y <- factor(use.dat$Y, levels = 1:6)
     use.dat[, paste0("Yhat", i)] <- factor(use.dat[, paste0("Yhat", i)], levels = 1:6)
     
     res <- caret::confusionMatrix(xtabs(f, data = use.dat))
     cbind(Size = tuning$size[[i]], Maxit = tuning$maxit[[i]], 
           Shuffle = tuning$shuffle[[i]], 
           as.data.frame(t(res$overall[c("AccuracyNull", "Accuracy", "AccuracyLower",
                                         "AccuracyUpper")])))
     }
))
```

```{r}
use.outsample <- cbind(Y = use.test.y, do.call(cbind.data.frame, 
               lapply(use.yhat, `[[`, "Outsample")))
colnames(use.outsample) <- c("Y", paste0("Yhat", 1:5))
performance.outsample <- do.call(rbind, lapply(1:5, function(i) {
     f <- substitute(~ Y + x, list(x = as.name(paste0("Yhat", i))))
     use.dat <- use.outsample[use.outsample[,paste0("Yhat", i)] != 0, ]
     use.dat$Y <- factor(use.dat$Y, levels = 1:6)
     use.dat[, paste0("Yhat", i)] <- factor(use.dat[, paste0("Yhat", i)], levels = 1:6)
  res <- caret::confusionMatrix(xtabs(f, data = use.dat))

  cbind(Size = tuning$size[[i]], Maxit = tuning$maxit[[i]],
        Shuffle = tuning$shuffle[[i]],
        as.data.frame(t(res$overall[c("AccuracyNull", "Accuracy", "AccuracyLower",
                                      "AccuracyUpper")])))
}))
```

Print the in-sample and out-of-sample performance to see how each of the models performed and the effect of varying some of the tuning parameters. The fourth column (null accuracy) is dropped as it is not as important for this comparison. 

```{r}
#In Sample
performance.insample[,-4]
# Out Sample
performance.outsample[,-4]
```

### Walkthrough Conclusion

```{r prepNewChapter2}
#Remove all packages loaded in environment
lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE)
#Apply packages
if(!require(easypackages)){
    install.packages("easypackages")
    library(easypackages)
}
packages("parallel", "foreach", "doSNOW", "glmnet", "MASS", "caret", "nnet", "deepnet", prompt = FALSE)
```

```{r}
set.seed(1234)
#mvrnorm requires MASS:  Produces one or more samples from the specified multivariate normal distribution.
X <- mvrnorm(n = 200, mu = c(0, 0, 0, 0, 0),  Sigma = matrix(c(1, .9999, .99, .99, .10, .9999, 1, .99, .99, .10, .99, .99, 1, .99, .10, 
                                                  .99, .99, .99, 1, .10, .10, .10, .10, .10, 1  ), ncol = 5))
y <- rnorm(200, 3 + X %*% matrix(c(1, 1, 1, 1, 0)), .5)
```

Not only are the in-sample performance measures biased estimates of the models' actual out-of-sample performance, they do not even provide the best way to rank order model performance to choose the best performing model. 

Despite the slightly worse out-of-sample performance, the models still do well—far better than chance alone—and, for our example use case, we could pick the best model (number 1) and be quite confident that using this will provide a good classification of a user's activities.
