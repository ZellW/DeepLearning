---
title: "Deep Learning with R Part 5-2"
output:
  html_document:
    highlight: pygments
    theme: spacelab
    toc: yes
header-includes: \usepackage{graphicx} \usepackage{mathtools}
---

### Walkthrough:  Deep Neural Network for Classification

THis walkthrough introduces new data.   There are 515,345 cases, with the first 463,715 being training cases and the last 51,630 cases used for testing. The first column of the dataset contains the year and the remaining columns are features from the timbre of songs. Download and decompress the data from here: http://archive.ics.uci.edu/ml/datasets/YearPredictionMSD. The goal is to predict the year each song was released. 

```{r echo=FALSE}
rm(list=ls())

#Remove all packages loaded in environment
try(lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE), silent = TRUE)
#Apply packages
if(!require(easypackages)){
    install.packages("easypackages")
    library(easypackages)
}
packages("data.table", "ggplot2", "parallel", "caret", "h2o", prompt = FALSE)

# download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip", 
#               destfile = "../Essentials/data/YearPredictionMSD.txt.zip")
# unzip("../Essentials/data/YearPredictionMSD.txt.zip")

## read data into R using fread() from data.table package
d <- fread("../Essentials/data/YearPredictionMSD.txt", sep = ",")

p.hist <- ggplot(d[, .(V1)], aes(V1)) +  geom_histogram(binwidth = 1) +  theme_classic() +  xlab("Year of Release") 
print(p.hist) 
```

A possible concern is the relatively extreme values may exert an undue influence on the model. Could reduce this by excluding a small amount of the more extreme cases, such as by excluding the bottom and top 0.5% (1% of data total). Checking the quantiles would include the years 1957 to 2010.

```{r}
quantile(d$V1, probs = c(.005, .995)) 
```

 Trim data and convert the training and testing datasets for H2O.
 
```{r}
d.train <- d[1:463715][V1 >= 1957 & V1 <= 2010]
d.test <- d[463716:515345][V1 >= 1957 & V1 <= 2010]

localH2O = h2o.init(max_mem_size = "4G", nthreads = 2)
h2omsd.train <- as.h2o(d.train, destination_frame = "h2omsdtrain")
h2omsd.test <- as.h2o(d.test, destination_frame = "h2omsdtest") 
```

To provide some baseline performance levels, build a linear regression model.

```{r}
summary(m0 <- lm(V1 ~ ., data = d.train))$r.squared
cor(d.test$V1,  predict(m0, newdata = d.test))^2 
```

Although not great, linear regression accounts for 24% of the variance in years in the training data and 23% in the testing data; these results provide a benchmark to beat with the feedforward neural network.

The first network is shallow with a single hidden layer. To make performance scoring occur on the full dataset, we use the special value, 0, passed to the score_training_ samples and score_validation_samples arguments. 

The results from this model show improvement over the linear regression model. The feedforward neural network, even though it only had a single layer with 50 hidden neurons, accounted for 32% of the variance in release year in the testing data, up from 23% using only linear regression. Because the model was small and had fewer hidden neurons than input variables, no dropout or other regularization was used. However, the performance discrepancy between the training and testing data (R2 = 0.37 versus R2 = 0.32, respectively) indicates that some regularization may be helpful.


```{r}
system.time(m1 <- h2o.deeplearning(x = colnames(d)[-1],  y = "V1",  training_frame= h2omsd.train,  validation_frame = h2omsd.test,  
                  activation = "RectifierWithDropout",  hidden = c(50),  epochs = 100,  input_dropout_ratio = 0,  hidden_dropout_ratios = c(0),
                  score_training_samples = 0,  score_validation_samples = 0,  diagnostics = TRUE,  export_weights_and_biases = TRUE,  
                  variable_importances = TRUE)
)
m1
```

Although the shallow neural network model was an improvement over linear regression, it still did not perform well and there is room for improvement. Try a larger, deep feedforward neural network with 3 layers of hidden neurons with 200, 300, and 400 hidden neurons, respectively. Also introduce a modest amount of dropout on the hidden (but not input) layer. 

The model shows a noticeable improvement from the small and shallow model. In the testing data, the shallow model had an R2 of 0.32 whereas the deep model has an R2 of 0.35. There is also a degree of overfitting. The difference in R2 between the training and testing data is 0.05, which is comparable to the simpler model where the difference was also 0.05. The more complex model improves performance, with little difference in overfitting, perhaps due to the dropout used.

```{r}
system.time(
     m2 <- h2o.deeplearning(x = colnames(d)[-1],  y = "V1",  training_frame= h2omsd.train,  validation_frame = h2omsd.test,  
                  activation = "RectifierWithDropout",  hidden = c(200, 300, 400),  epochs = 100,  
                  input_dropout_ratio = 0,  hidden_dropout_ratios = c(.2, .2, .2),  score_training_samples = 0,  
                  score_validation_samples = 0,  diagnostics = TRUE,  export_weights_and_biases = TRUE,  
                  variable_importances = TRUE) 
)
m2
saveRDS(m2, "../Essentials/Walkthru3.rds")
h2o.saveModel(object = m2, path = "/Essentials//data/Modelm2", force = TRUE) #See Appendix - note the path
```

To see whether the performance on the testing data can be improved further, try an additional model including substantially more hidden neurons in each layer, more training iterations (epochs) and with a higher degree of regularization. This code was not executed - it will take the best part of a day to complete.  (The performance of this model on the testing data was actually worse than either of the previous two models, though still superior to the linear regression.)

```{r eval=FALSE}
system.time(
     m3 <- h2o.deeplearning(x = colnames(d)[-1], y = "V1", training_frame= h2omsd.train, validation_frame = h2omsd.test,
                  activation = "RectifierWithDropout", hidden = c(500, 500, 1000), epochs = 500, input_dropout_ratio = 0,
                  hidden_dropout_ratios = c(.5, .5, .5), score_training_samples = 0, score_validation_samples = 0,
                  diagnostics = TRUE, export_weights_and_biases = TRUE, variable_importances = TRUE)
)
m3
```

The best model then is still the deep model, but with fewer hidden neurons per layer. One way that we can try to see if that model can be improved is to try training for additional epochs or iterations. In the model output, there is a model ID. For the best performing model, this was:

DeepLearning_model_R_1500480383778_2

This can be passed to the checkpoint argument of `h2o.deeplearning()` so  training begins using the weights from the previous model. 

As long as the general architecture—the number of hidden neurons, layers, and connections—remains the same, using the checkpoint can be a great time saver. This is not only true because the previous training iterations can be re-used, but also because it tends to take longer for earlier than later iterations. The following example shows how to run the model, changing the epochs from 500 to 1,000 (since 500 have already been done) and starting from the previous model run by specifying the model name as a character string to the checkpoint argument:

> Note that the model ID will be different every time you run the code; thus, when running it on your own computer or servers, you will need to use the model ID from your run.

The additional epochs did not improve the model performance. In fact, it became slightly worse:

```{r}
m2b <- h2o.deeplearning(x = colnames(d)[-1], y = "V1", training_frame= h2omsd.train, validation_frame = h2omsd.test,
                   activation = "RectifierWithDropout", hidden = c(200, 200, 400), checkpoint = "DeepLearning_model_R_1500480383778_2",
                   epochs = 1000, input_dropout_ratio = 0, hidden_dropout_ratios = c(.2, .2, .2), score_training_samples = 0,
                   score_validation_samples = 0, diagnostics = TRUE, export_weights_and_biases = TRUE, variable_importances = TRUE
  )
m2b
```

## Tuning & Optimizing Models

- Dealing with missing data
     - H2O includes a function to impute variables using the mean, median, or mode, and optionally to do so by some other grouping variables. 
- Solutions for models with low accuracy

### Missing Data

```{r echo=FALSE}
rm(list=ls())

#Remove all packages loaded in environment
lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE)
#Apply packages
if(!require(easypackages)){
    install.packages("easypackages")
    library(easypackages)
}
packages("data.table", "ggplot2", "parallel", "caret", "h2o", "gridExtra", "mgcv", prompt = FALSE)

localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)

## setup iris data with some missing 
d <- as.data.table(iris) 
d[Species == "setosa", c("Petal.Width", "Petal.Length") := .(NA, NA)]#data table := is left assignment operator like <-
h2o.dmiss <- as.h2o(d, destination_frame = "iris_missing") 
h2o.dmeanimp <- as.h2o(d, destination_frame = "iris_missing_imp")
```

Do a simple mean imputation. This has to be done one column at a time.

```{r}
## mean imputation 
# for loop from book does not work.  After the 1st loop h20.dmeanimp becomes numeric which then fails because the data for h20.impute must be h2oFrame
# had to chage loop with h2o.dmeanimp2
missing.cols <- colnames(h2o.dmiss)[apply(d, 2, anyNA)]
for (v in missing.cols) {
     h2o.dmeanimp2 <- h2o.impute(h2o.dmeanimp, column = v, method = "mean")
}
head(h2o.dmeanimp)
```

Instead of a simple mean imputation, could use simple prediction model. Build a random forest model to predict each missing column. All default values are used. (A glm model could also be used.)

```{r}
## random forest imputation
d.imputed <- d

## prediction model
for (v in missing.cols) {
     tmp.m <- h2o.randomForest(x = setdiff(colnames(h2o.dmiss), v), y = v, training_frame = h2o.dmiss)
     yhat <- as.data.frame(h2o.predict(tmp.m, newdata = h2o.dmiss))
     d.imputed[[v]] <- ifelse(is.na(d.imputed[[v]]), yhat$predict, d.imputed[[v]])
}
```

To compare the different methods, create a scatter plot of petal length against petal width with the color and shape of the points determined by the flower species. This graph has three panels. The top panel is the original data. The middle panel is the data using mean imputation. The bottom panel is the data using random forest imputation.

```{r}
grid.arrange(
     ggplot(iris, aes(Petal.Length, Petal.Width, color = Species, shape = Species)) + geom_point() + theme_classic() + ggtitle("Original Data"),
     ggplot(as.data.frame(h2o.dmeanimp), aes(Petal.Length, Petal.Width, color = Species, shape = Species)) + geom_point() + 
          theme_classic() + ggtitle("Mean Imputed Data"),
     ggplot(d.imputed, aes(Petal.Length, Petal.Width, color = Species, shape = Species)) + geom_point() + theme_classic() + 
          ggtitle("Random Forest Imputed Data"),
     ncol = 1
     )
```

The mean imputation creates aberrant values quite removed from reality. If needed, more advanced prediction models could be generated. In statistical inferences, multiple imputation is preferred over single imputation as the latter fails to account for uncertainty when imputing the missing values there is some degree of uncertainty as to exactly what those values are. In most use cases for deep learning, the datasets are far too large and the computational time too demanding to create multiple datasets with different imputed values, train models on each, and pool the results; thus, these simpler methods (such as mean imputation or using some other prediction model) are common. 

```{r echo=FALSE}
h2o.shutdown(prompt = FALSE)
```

### Low Accuracy Models

- Grid Searches: several values for hyperparameters are specified and all possible combinations are tried.  Use `expand`
- Random Searches

#### Grid Search

```{r eval=FALSE}
expand.grid(layers = c(1, 2, 4), epochs = c(50, 100), l1 = c(.001, .01, .05))
```

Grid searching is excellent when there are only a few values for a few parameters. However, although this is a comprehensive way of assessing different parameter values, when there are many values for some or many parameters, it quickly becomes unfeasible. For example, even with only two values for each of eight parameters, there are 28 = 256 combinations, which quickly becomes computationally impracticable. In addition, if there are no interactions between parameters and model performance, or at least the interactions are small relative to the main effects, then grid searches are an inefficient approach because many parameter values are repeated so that only a small set of values is sampled, even though many combinations are tried.

#### Random Search

Rather than prespecifying all the values to try and creating all possible combinations, randomly sample values for the parameters, fit a model, store the results, and repeat. To get a very large sample size, this too would be computationally demanding. Specify are the values to randomly sample or distributions to randomly draw from. Limits would also be set. For example, although a model could theoretically have any integer number of layers, some reasonable number (such as 1 to 10) is used rather than sampling integers from 1 to a billion.

For random sampling, write a function that takes a seed and then randomly samples a number of hyperparameters, stores the sampled parameters, runs the model, and returns the results. Do not sampling from every possible hyperparameter. Many remain fixed at values or their defaults. For some parameters, specifying how to randomly sample values requires thought. For example, when using dropout for regularization, it is common to have a relatively smaller amount of dropout for the input variables (around 20% commonly) and a higher amount for hidden neurons (around 50% commonly). Choosing the right distributions can allow us to encode this prior information into our random search. 

The following code plots the density of two beta distributions. By sampling from these distributions, it ensures the search, while random, focuses on small proportions of dropout for the input variables and in the 0 to 0.50 range for the hidden neurons with a tendency to oversample from values closer to 0.50.

```{r}
par(mfrow = c(2, 1))
plot(seq(0, .5, by = .001), dbeta(seq(0, .5, by = .001), 1, 12), type = "l", xlab = "x", ylab = "Density", main = "Density of a beta(1, 12)")

plot(seq(0, 1, by = .001)/2, dbeta(seq(0, 1, by = .001), 1.5, 1), type = "l", xlab = "x", ylab = "Density", main = "Density of a beta(1.5, 1) / 2")
```

Create a function `myRun()`. It requires is a seed, which is used to make the parameter selection reproducible. A name can be specified, although there is a default based on the seed, and there is an optional (logical) argument, run, to control whether or not the model is run. This is helpful if you want to check the hyperparameter values sampled.

- sample the depth or number of layers from 1 to 5 
- number of neurons in each layer from 20 to 200; by default each will have an equal probability (was 20 to 600)
     - `runif()` samples from a uniform distribution in the specified range
     - beta distribution using `rbeta()` 
- To automatically tune the learning rate, use ADADELTA as H~2~O does.  Two hyperparameters that need to be specified: rho and epsilon. 
     - rho parameter weights the gradients prior to the current iteration
          - 1 – rho is used to weight the gradient at the current iteration. 
          - If rho = 1 then the current gradient is not used and it is completely based on the previous gradients. 
          - If rho = 0, the previous gradients are not used and it is completely based on the current gradient. 
          - values between .9 and .999  are used
     - epsilon parameter is a small constant that is added when taking the root mean square of previous squared gradients to improve conditioning 

```{r}
myRun <- function(seed, name = paste0("m_", seed), run = TRUE) {
  set.seed(seed)

  p <- list(Name = name, seed = seed,
            depth = sample(1:5, 1),
            l1 = runif(1, 0, .01),
            l2 = runif(1, 0, .01),
            input_dropout = rbeta(1, 1, 12),
            rho = runif(1, .9, .999),
            epsilon = runif(1, 1e-10, 1e-4))

  p$neurons <- sample(20:200, p$depth, TRUE)
  p$hidden_dropout <- rbeta(p$depth, 1.5, 1)/2

  if (run) {model <- h2o.deeplearning(x = colnames(use.train.x), y = "Outcome", training_frame = h2oactivity.train,
                       #activation = "RectifierWithDropout", 
                       activation = "TanhWithDropout",
                       hidden = p$neurons, epochs = 100, loss = "CrossEntropy",
                       input_dropout_ratio = p$input_dropout, hidden_dropout_ratios = p$hidden_dropout, 
                       l1 = p$l1, l2 = p$l2, rho = p$rho, epsilon = p$epsilon, export_weights_and_biases = TRUE, model_id = p$Name)

  ## performance on training data
  p$MSE <- h2o.mse(model)
  p$R2 <- h2o.r2(model)
  p$Logloss <- h2o.logloss(model)
  p$CM <- h2o.confusionMatrix(model)

  ## performance on testing data
  perf <- h2o.performance(model, h2oactivity.test)
  p$T.MSE <- h2o.mse(perf)
  p$T.R2 <- h2o.r2(perf)
  p$T.Logloss <- h2o.logloss(perf)
  p$T.CM <- h2o.confusionMatrix(perf)

  } else {
    model <- NULL
  }

  return(list(Params = p, Model = model))
}
```

##### Test myRun

```{r getDataAgain}
rm(list=ls())

#Remove all packages loaded in environment
lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""), detach, character.only=TRUE, unload=TRUE)
#Apply packages
if(!require(easypackages)){
    install.packages("easypackages")
    library(easypackages)
}
packages("data.table", "ggplot2", "parallel", "caret", "h2o", "mgcv", prompt = FALSE)

# data and H2O setup
use.train.x <- read.table("../Essentials/data/X_train.txt")
use.train.y <- read.table("../Essentials/data/y_train.txt")[[1]]
use.test.x <- read.table("../Essentials/data/X_test.txt")
use.test.y <- read.table("../Essentials/data/y_test.txt")[[1]]

use.train <- cbind(use.train.x, Outcome = factor(use.train.y)) 
use.test <- cbind(use.test.x, Outcome = factor(use.test.y))

use.labels <- read.table("../Essentials/data/activity_labels.txt")

localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
h2oactivity.train <- as.h2o(use.train, destination_frame = "h2oactivitytrain")
h2oactivity.test <- as.h2o(use.test, destination_frame = "h2oactivitytest")
```

To make the parameters reproducible,  specify a list of random seeds which loop through to run the models.

```{r}
use.seeds <- c(1234, 2345, 3456, 4567, 5678, 6789, 78900, 1122, 2233, 3344, 4455, 5566, 6677, 7788, 8899, 9988, 8877, 7766, 6655, 5544, 4433, 3322, 2211)

model.res <- lapply(use.seeds, myRun)
saveRDS(model.res, "../Essentials/modelres.rds")
```

Once the models are done, we can create a dataset, and plot the mean squared error (MSE) against the different parameters.

```{r warning=FALSE}
model.res.dat <- do.call(rbind, lapply(model.res, function(x) 
     with(x$Params,
          data.frame(l1 = l1, l2 = l2, 
          depth = depth, 
          input_dropout = input_dropout, 
          SumNeurons = sum(neurons),
          MeanHiddenDropout = mean(hidden_dropout), 
          rho = rho, epsilon = epsilon, MSE = T.MSE))))

p.perf <- ggplot(melt(model.res.dat, id.vars = c("MSE")), aes(value, MSE)) +  geom_point() +  stat_smooth(color = "black") +  
     facet_wrap(~ variable, scales = "free_x", ncol = 2) +  theme_classic() 
print(p.perf)
```

It can be helpful to use a multivariate model to simultaneously take different parameters into account. To fit this (and allow some non-linearity), use a generalized additive model, using `gam()` from `mgcv`. Hypothesize an interaction between the model depth and total number of hidden neurons, which are captured by including both of those terms in a tensor expansion using `te()` with the remaining terms given univariate smooths, using `s()`.

```{r}
m.gam <- gam(MSE ~ s(l1, k = 4) + s(l2, k = 4) + s(input_dropout) +  s(rho, k = 4) + s(epsilon, k = 4) + 
                  s(MeanHiddenDropout, k = 4) + te(depth, SumNeurons, k = 4), data = model.res.dat) 
```

