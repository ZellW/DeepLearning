sms_dtm <- DocumentTermMatrix(corpus_clean)
DocumentTermMatrix
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
names(sms_raw)<-c("type","text")
sms_raw$type <- factor(sms_raw$type)
View(sms_raw)
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")#VERY HELPFUL
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")#VERY HELPFUL
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")#VERY HELPFUL
sms_raw$text <- iconv(sms_raw$text, to = "utf-8")#VERY HELPFUL
sms_corpus <- Corpus(VectorSource(sms_raw$text), encoding="UTF-8")
sms_corpus <- Corpus(VectorSource(sms_raw$text), encoding="UTF-8")
sms_corpus <- Corpus(VectorSource(sms_raw$text),readerControl = list(language="en_US"))
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
names(sms_raw)<-c("type","text")
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text),readerControl = list(language="en_US"))
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
names(sms_raw)<-c("type","text")
sms_raw$type <- factor(sms_raw$type)
sms_raw$text <- iconv(sms_raw$text, to = "utf-8")#VERY HELPFUL
sms_corpus <- Corpus(VectorSource(sms_raw$text),readerControl = list(language="en_US"))
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw_train <- sms_raw[1:2388, ]
sms_raw_test  <- sms_raw[2389:3184, ]
length(sms_dtm)
count(sms_dtm)
nrow(sms)dtm
nrow(smsdtm)
sms_dtm_train <- sms_dtm[1:2388, ]
sms_dtm_test  <- sms_dtm[2389:3184, ]
sms_corpus_train <- corpus_clean[1:2388]
sms_corpus_test  <- corpus_clean[2389:3184]
Dictionary <- function(x) {
if( is.character(x) ) {
return (x)
}
stop('x is not a character vector')
}
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test  <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
return(x)
}
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_test, MARGIN = 2, convert_counts)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
CrossTable(sms_test_pred, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("MASS", prompt = FALSE)
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("MASS", "dplyr", prompt = FALSE)
glimpse(fgl)
data.lda <- lda(formula = type ~ ., data = fgl)
data.lda
data.lda.pred <-predict(data.lda, newdata=fgl[,c(1:9)])$class
data.lda.pred
table(data.lda.pred,fgl[,10])
data_set <- data.frame(
id=c(1,2,3,4,5),
mood=c(0,20,20,40,50),
value=c(12.34, 32.2, 24.3, 83.1, 8.32),
outcome=c(1,1,0,0,0))
feature_name = 'mood'
data_set[,feature_name][!is.na(data_set[,feature_name])]
data_set[,feature_name][!is.na(data_set[,feature_name])]
data_set[,feature_name][!is.na(data_set[,feature_name])]
library(dplyr)
distinct(data_set, mood)
data_set[,feature_name][!is.na(data_set[,feature_name])]
data_set %>% filter(!is.na(mood))%>% distinct(data_set, mood)
data_set %>% filter(!is.na(mood)) %>% distinct(data_set, mood)
data_set %>% filter(!is.na(mood)) %>% distinct(mood)
count(data_set %>% filter(!is.na(mood)) %>% distinct(mood))
nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood))
length(data_set %>% filter(!is.na(mood)) %>% distinct(mood))
nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood))
length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 2
nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood)) > 2
Get_Free_Text_Measures <- function(data_set, minimum_unique_threshold=0.9, features_to_ignore=c()){
text_features <- c(names(data_set[sapply(data_set, is.character)]), names(data_set[sapply(data_set, is.factor)]))
for (f_name in setdiff(text_features, features_to_ignore)){
f_vector <- as.character(data_set[,f_name])
if (length(unique(as.character(f_vector))) > (nrow(data_set) * minimum_unique_threshold)){
data_set[,paste0(f_name, '_word_count')] <- sapply(strsplit(f_vector, " "), length)
data_set[,paste0(f_name, '_character_count')] <- nchar(as.character(f_vector))
data_set[,paste0(f_name, '_first_word')] <- sapply(strsplit(as.character(f_vector), " "), `[`, 1)
data_set[,f_name] <- NULL
}
}
return(data_set)
}
Titanic_dataset_temp <- Get_Free_Text_Measures(data_set = Titanic_dataset_temp, features_to_ignore = c())
Titanic_dataset_temp <- Titanic_dataset#keep the original virgin data set
Titanic_dataset_temp <- Titanic_dataset
# load the data set in case you haven't already done so
Titanic_dataset <- read.table('http://math.ucdenver.edu/RTutorial/titanic.txt', sep='\t', header=TRUE, stringsAsFactors = FALSE)
head(Titanic_dataset)
Titanic_dataset_temp <- Titanic_dataset
#Calculate the number of words in the Name field - just uses spaces - I think this is unreliable - OK for just names
#Keeps the comma.  My wordcount function below does not (of course we are just counting so this is OK)
#sapply simple gives you the number of words left by strsplit
Titanic_dataset_temp$Word_Count <- sapply(strsplit(Titanic_dataset_temp$Name, " "), length)
print(head(Titanic_dataset_temp))
Titanic_dataset_temp <- Get_Free_Text_Measures(data_set = Titanic_dataset_temp, features_to_ignore = c())
str(Titanic_dataset_temp)
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("readr", "lubridate", "dplyr", "ggplot2", "infotheo", prompt = FALSE)
Feature_Engineer_Dates <- function(data_set, remove_original_date=TRUE) {
data_set <- data.frame(data_set)
date_features <- names(data_set[sapply(data_set, is.Date)])#is.Date is from lubridate
for (feature_name in date_features) {
data_set[,paste0(feature_name,'_DateInt')] <- as.numeric(data_set[,feature_name])
data_set[,paste0(feature_name,'_Month')] <- as.integer(format(data_set[, feature_name], "%m"))
data_set[,paste0(feature_name,'_ShortYear')] <- as.integer(format(data_set[,feature_name], "%y"))
data_set[,paste0(feature_name,'_LongYear')] <- as.integer(format(data_set[,feature_name], "%Y"))
data_set[,paste0(feature_name,'_Day')] <- as.integer(format(data_set[,feature_name], "%d"))
# week day number requires first pulling the weekday label,
# creating the 7 week day levels, and casting to integer
data_set[,paste0(feature_name,'_WeekDayNumber')] <- as.factor(weekdays(data_set[, feature_name]))
levels(data_set[, paste0(feature_name, '_WeekDayNumber')]) <-
list(Monday=1, Tuesday=2, Wednesday=3, Thursday=4, Friday=5, Saturday=6, Sunday=7)
data_set[, paste0(feature_name,'_WeekDayNumber')] <-
as.integer(data_set[,paste0(feature_name,'_WeekDayNumber')])
data_set[,paste0(feature_name,'_IsWeekend')] <-
as.numeric(grepl("Saturday|Sunday", weekdays(data_set[, feature_name])))
data_set[,paste0(feature_name,'_YearDayCount')] <- yday(data_set[,feature_name])
data_set[,paste0(feature_name,'_Quarter')] <- lubridate::quarter(data_set[, feature_name], with_year = FALSE)
data_set[,paste0(feature_name,'_Quarter')] <- lubridate::quarter(data_set[, feature_name], with_year = TRUE)
if (remove_original_date)#delete the original date column only if remove_original_date=TRUE in function call
data_set[, feature_name] <- NULL
}
return(data_set)
}
mix_dataset <- data.frame(id=c(10,20,30,40,50), gender=c('male','female','female','male','female'),
some_date=c('2012-01-12','2012-01-12','2012-12-01','2012-05-30','2013-12-12'),
value=c(12.34, 32.2, 24.3, 83.1, 8.32), outcome=c(1,1,0,0,0))
write_csv(mix_dataset, './data/mix_dataset.csv')
mix_dataset <- read_csv('./data/mix_dataset.csv')
mix_dataset <- Feature_Engineer_Dates(mix_dataset)
head(mix_dataset)
Feature_Engineer_Integers <- function(data_set, features_to_ignore=c()) {
data_set <- data.frame(data_set)
for (feature_name in setdiff(names(data_set), features_to_ignore)) {
if (class(data_set[,feature_name])=='numeric' | class(data_set[,feature_name])=='integer') {
feature_vector <- data_set[,feature_name]
if (all((feature_vector - round(feature_vector)) == 0)) {
# make sure we have more than 2 values excluding NAs
if (length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 2) {
#print(feature_name)#not really needed/helpful
data_set[,paste0(feature_name,'_IsZero')] <- ifelse(data_set[,feature_name]==0,1,0)
data_set[,paste0(feature_name,'_IsPositive')] <- ifelse(data_set[,feature_name]>=0,1,0)
# separate data into two bins using infotheo
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=2)
data_set[,paste0(feature_name,'_2Bins')] <- data_discretized$X
if (length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 4) {
# try 4 bins
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=4)
data_set[,paste0(feature_name,'_4Bins')] <- data_discretized$X
}
}
}
}
}
return (data_set)
}
Feature_Engineer_Integers <- function(data_set, features_to_ignore=c()) {
data_set <- data.frame(data_set)
for (feature_name in setdiff(names(data_set), features_to_ignore)) {
if (class(data_set[,feature_name])=='numeric' | class(data_set[,feature_name])=='integer') {
feature_vector <- data_set[,feature_name]
if (all((feature_vector - round(feature_vector)) == 0)) {
# make sure we have more than 2 values excluding NAs
# if (length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 2) {
if (nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood)) > 2) {
#print(feature_name)#not really needed/helpful
data_set[,paste0(feature_name,'_IsZero')] <- ifelse(data_set[,feature_name]==0,1,0)
data_set[,paste0(feature_name,'_IsPositive')] <- ifelse(data_set[,feature_name]>=0,1,0)
# separate data into two bins using infotheo
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=2)
data_set[,paste0(feature_name,'_2Bins')] <- data_discretized$X
if (nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood)) > 4) {
# try 4 bins
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=4)
data_set[,paste0(feature_name,'_4Bins')] <- data_discretized$X
}
}
}
}
}
return (data_set)
}
mix_dataset <- read_csv('./data/mix_dataset.csv')
Feature_Engineer_Integers(mix_dataset, features_to_ignore=c('id'))
Feature_Engineer_Integers <- function(data_set, features_to_ignore=c()) {
data_set <- data.frame(data_set)
for (feature_name in setdiff(names(data_set), features_to_ignore)) {
if (class(data_set[,feature_name])=='numeric' | class(data_set[,feature_name])=='integer') {
feature_vector <- data_set[,feature_name]
if (all((feature_vector - round(feature_vector)) == 0)) {
# make sure we have more than 2 values excluding NAs
# if (length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 2) {
if (nrow(data_set %>% filter(!is.na(feature_vector)) %>% distinct(feature_vector)) > 2) {
#print(feature_name)#not really needed/helpful
data_set[,paste0(feature_name,'_IsZero')] <- ifelse(data_set[,feature_name]==0,1,0)
data_set[,paste0(feature_name,'_IsPositive')] <- ifelse(data_set[,feature_name]>=0,1,0)
# separate data into two bins using infotheo
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=2)
data_set[,paste0(feature_name,'_2Bins')] <- data_discretized$X
if (nrow(data_set %>% filter(!is.na(feature_vector)) %>% distinct(feature_vector)) > 4) {
# try 4 bins
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=4)
data_set[,paste0(feature_name,'_4Bins')] <- data_discretized$X
}
}
}
}
}
return (data_set)
}
mix_dataset <- read_csv('./data/mix_dataset.csv')
Feature_Engineer_Integers(mix_dataset, features_to_ignore=c('id'))
library(devtools)
devtools::install_github("statswithr/statsr")
library(readr)
tmpDF <- read_csv(file.choose())
View(tmpDF)
View(tmpDF)
tmpDF <- read.csv(file.choose())
tmpDF <- read.csv(file.choose(), skip =1)
View(tmpDF)
tmpDF <- tmpDF[-c(42537:42538),]
write.csv(file.choose())
write.csv(tmpDF,"C:\Users\cweaver\Desktop\LendingClubData\LoanStats3a.csv")
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats3a.csv")
tmpDF <- read.csv(file.choose(), skip =1)
View(tmpDF)
tmpDF <- tmpDF[-c(188182:188183),]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats3b.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tail(tmpDF)
View(tmpDF)
tmpDF <- tmpDF[-c(235630:235631),]
View(tmpDF)
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats3c.csv")
tmpDF <- read.csv(file.choose(), skip =1)
View(tmpDF)
tmpDF <- tmpDF[-c(421096:421097),]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats3d.csv")
tmpDF <- read.csv(file.choose(), skip =1)
View(tmpDF)
nrow(tmpDF)
nrow(tmpDF)-1
tmpDF <- tmpDF[-c(nrow(tmpDF)-1:nrow(tmpDF))]
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c(nrow(tmpDF)-1:nrow(tmpDF)), ]
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2016Q1.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2016Q2.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2016Q3.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2016Q4.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2017Q1.csv")
install.packages("plotly")
myData = read_html("https://en.wikipedia.org/wiki/List_of_U.S._states_by_life_expectancy")
## LOAD THE PACKAGES ####
library(rvest)
library(ggplot2)
library(dplyr)
library(scales)
myData = read_html("https://en.wikipedia.org/wiki/List_of_U.S._states_by_life_expectancy")
myData = myData %>% html_nodes("table") %>% .[[2]] %>% html_table(fill=T)
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
library(h2o)
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
data("Seatbelts")
summary(Seatbelts)
dim(Seatbelts)
trainHex <- as.h2o(Seatbelts)
x_names  <- colnames(trainHex[2:8])
myModel <- h2o.deeplearning(x = x_names, y = "DriversKilled", training_frame = trainHex)
myModel
h2o.shutdown(prompt = FALSE)
source('~/GitHub/LT/MyLT1_Nikita.R', echo=TRUE)
install.packages("sparklyr")
library(sparklyr)
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("dplyr", "devtools" prompt = FALSE)
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("dplyr", "devtools", prompt = FALSE)
if(!require(keras)){
library(devtools)
devtools::install_github("rstudio/keras")}
if(!require(keras)){
library(devtools)
devtools::install_github("rstudio/keras")}
library(devtools)
devtools::install_github("rstudio/keras")
install.packages("processx")
source("https://install-github.me/r-lib/processx")
library(keras)
devtools::install_github("rstudio/keras")
library(processx)
install.packages("source("https://install-github.me/r-lib/processx")")
source("https://install-github.me/r-lib/processx")
devtools::install_github("rstudio/keras")
library("Rcpp", lib.loc="~/R/win-library/3.3")
devtools::install_github("rstudio/keras")
install.packages("Rcpp")
install.packages("Rcpp")
install.packages("devtools")
devtools::install_github("rstudio/reticulate")
devtools::install_github("rstudio/tensorflow")
install.packages("Rcpp")
install.packages("devtools")
install.packages("devtools")
devtools::install_github("rstudio/reticulate", force=TRUE)
devtools::install_github("r-lib/processx")
library(processx)
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/keras")
getwd()
setwd("~/GitHub/DeepLearning")
knitr::include_graphics("./images/mnist.png")
mnist_data <- dataset_mnist()
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("keras", prompt = FALSE)
install_keras()
mnist_data <- dataset_mnist()
train <- mnist_data$train$x
y_train <- mnist_data$train$y
x_test <- mnist_data$test$x
y_test <- mnist_data$test$y
head(x_train)
getwd()
head(mnist_data)
x_train <- mnist_data$train$x
y_train <- mnist_data$train$y
x_test <- mnist_data$test$x
y_test <- mnist_data$test$y
getwd()
head(x_train)
head(x_train, 30)
head(y_train, 30)
dim(x_train)
dim(y_train)
dim(x_train)
nrow(x_train)
dim(x_train) <- c(nrow(x_train), 784)
dim(x_train)
dim(x_test) <- c(nrow(x_test), 784)
head(x_train)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(loss = 'categorical_crossentropy', optimizer = optimizer_rmsprop(),metrics = c('accuracy'))
history <- model %>% fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
model %>% compile(loss = 'categorical_crossentropy', optimizer = optimizer_rmsprop(),metrics = c('accuracy'))
history <- model %>% fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
plot(history)
model %>%
layer_dense(units = 784, input_shape = 784) %>%
layer_dropout(rate=0.4)%>%
layer_activation(activation = 'relu') %>%
layer_dense(units = 10) %>%
layer_activation(activation = 'softmax')
model %>% compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = c('accuracy'))
model %>% fit(train_x, train_y, epochs = 100, batch_size = 128)
model %>% fit(x_train, y_train, epochs = 100, batch_size = 128)
data<-dataset_mnist()
#separating train and test file
train_x<-data$train$x
train_y<-data$train$y
test_x<-data$test$x
test_y<-data$test$y
rm(data)
# converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix
train_x <- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255
test_x <- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255
#converting the target variable to once hot encoded vectors using keras inbuilt function
train_y<-to_categorical(train_y,10)
test_y<-to_categorical(test_y,10)
#defining a keras sequential model
model <- keras_model_sequential()
#defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons]
#i.e number of digits from 0 to 9
model %>%
layer_dense(units = 784, input_shape = 784) %>%
layer_dropout(rate=0.4)%>%
layer_activation(activation = 'relu') %>%
layer_dense(units = 10) %>%
layer_activation(activation = 'softmax')
#compiling the defined model with metric = accuracy and optimiser as adam.
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
#fitting the model on the training dataset
model %>% fit(train_x, train_y, epochs = 100, batch_size = 128)
#Evaluating model on the cross validation dataset
loss_and_metrics <- model %>% evaluate(test_x, test_y, batch_size = 128)
knitr::include_graphics("./images/Keras_Resulr.png")
knitr::include_graphics("../images/Keras_Resulr.png")
getwd()
knitr::include_graphics("../images/Keras_Result.png")
knitr::include_graphics("./images/Keras_Result.png")
head(train_x, 20)
head(train_x, 5)
head(train_x, 1)
summary(model)
plot(model)
loss_and_metrics <- model %>% evaluate(test_x, test_y, batch_size = 128)
loss_and_metrics
classes <- model %>% predict_classes(test_x)
classes
head(class)
head(class)
head(classes)
head(classes, 30)
deactivate r-tensorflow
library(processx)
library(keras)
install_keras()
#loading the keras inbuilt mnist dataset
data<-dataset_mnist()
#separating train and test file
train_x<-data$train$x
train_y<-data$train$y
test_x<-data$test$x
test_y<-data$test$y
rm(data)
dim(train_x)
# converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix
train_x <- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255
test_x <- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255
train_y<-to_categorical(train_y,10)
test_y<-to_categorical(test_y,10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 784, input_shape = 784) %>%
layer_dropout(rate=0.4)%>%
layer_activation(activation = 'relu') %>%
layer_dense(units = 10) %>%
layer_activation(activation = 'softmax')
model %>% compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = c('accuracy'))
loss_and_metrics <- model %>% evaluate(test_x, test_y, batch_size = 128)
classes <- model %>% predict_classes(test_x)
loss_and_metrics
print(loss_and_metrics)
class(loss_and_metrics)
lossmetrics <- model %>% evaluate(test_x, test_y, batch_size = 128)
print(lossmetrics)
classes <- model %>% predict_classes(test_x)
head(classes, 30)
deactivate r-tensorflow
knitr::include_graphics("../images/ML2_NeuralNets1.JPG")
knitr::include_graphics("./images/ML2_NeuralNets1.JPG")
knitr::include_graphics("./images/ML2_NeuralNets2.JPG")
knitr::include_graphics("./images/ML2_NeuralNets1.JPG")
